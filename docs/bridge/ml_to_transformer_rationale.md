# ML to Transformer Conceptual Rationale

**Document Purpose:** Bridge the conceptual gap from classical ML and CS50AI survey to Transformer architecture.

## Why Sequence Modeling Differs

*(Content to be added by learner during Day 64 Bridge Sprint)*

## How Attention Addresses RNN Limitations

*(Content to be added by learner during Day 64 Bridge Sprint)*

## Role of Positional Encoding

*(Content to be added by learner during Day 64 Bridge Sprint)*

## Self-Attention as Learned Contextual Embeddings

*(Content to be added by learner during Day 64 Bridge Sprint)*

## Connections to Phase 4

*(Content to be added by learner during Day 64 Bridge Sprint)*

## Reading List for Week 11

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

---

**Target:** 800-1200 words | **Completion:** Day 64 of Phase 3B
