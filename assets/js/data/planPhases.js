// AI & Machine Learning Mastery Plan - Complete 12-Month Data Model
// 364 days across 14 phases with comprehensive task breakdown
// NOTE: Expansions include previously condensed eigen decomposition, CNN build sequence,
// GPT training passes, scaling experiments, LoRA/QLoRA steps, capstone evaluation days,
// and portfolio polish.

const PLAN = {
  phases: [
    {
      id: 'foundations',
      title: 'Phase 1: Math + Python-for-Data Foundations',
      description: 'Build strong foundations in linear algebra, calculus, probability, and Python programming for data science.',
      duration: '42 days (Weeks 1-6)',
      weeks: [1, 2, 3, 4, 5, 6],
      days: [
        // Week 1
        {
          globalDay: 1,
          week: 1,
          title: 'Environment Setup & Vectors Introduction',
          priority: 'HIGH',
          tasks: [
            { label: 'Complete DataCamp "Introduction to NumPy" Chapter 1 only', estMinutes: 90, resourceLinks: ['https://datacamp.com'], notebook: 'foundations/day01_vectors_intro.ipynb', artifact: 'day01_vectors_plot.png', successCriteria: 'Environment script passes; notebook renders plots', details: '<strong>Action:</strong> Work through <a href="https://www.datacamp.com/courses/intro-to-python-for-data-science" target="_blank" rel="noopener">DataCamp Introduction to NumPy</a> Chapter 1, focusing on array creation and basic operations. <strong>Boundaries:</strong> Stop after Chapter 1—don\'t proceed to advanced indexing yet. Take notes on np.array(), shape, dtype, and basic arithmetic. <strong>Deliverable:</strong> Create a Jupyter notebook (foundations/day01_vectors_intro.ipynb) with working examples of each concept covered. <strong>Verification:</strong> Notebook should run without errors and display output for all cells. Common pitfall: confusing Python lists with NumPy arrays—remember arrays are homogeneous and support vectorized operations. Success check: Can you create a 3×3 array and perform element-wise multiplication? Estimated time: 90 minutes including note-taking. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/user/absolute_beginners.html" target="_blank" rel="noopener">NumPy Absolute Beginners Guide</a>' },
            { label: 'Watch 3Blue1Brown Essence of Linear Algebra Ep. 1 "Vectors" (full episode)', estMinutes: 12, resourceLinks: ['https://www.youtube.com/watch?v=fNk_zzaMoSs'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs" target="_blank" rel="noopener">3Blue1Brown\'s "Vectors" episode</a> (12 minutes) focusing on the geometric intuition of vectors as arrows in space. <strong>Boundaries:</strong> Don\'t worry about formal mathematical notation yet—focus on the visual understanding of vectors as directed quantities with magnitude and direction. <strong>Deliverable:</strong> Mental model of vectors as geometric objects that can be added tip-to-tail and scaled. <strong>Verification:</strong> After watching, can you explain why vectors are more than just lists of numbers? Common pitfall: treating vectors purely as algebraic objects without geometric intuition. Success check: Can you visualize vector addition geometrically? This video is foundational—the geometric perspective will help throughout ML, especially when understanding gradients and optimization. Pause and replay sections as needed. <strong>Resources:</strong> <a href="https://www.3blue1brown.com/topics/linear-algebra" target="_blank" rel="noopener">3Blue1Brown Linear Algebra Series</a>' },
            { label: 'Run scripts/test_env_setup.py to verify Python environment', estMinutes: 15, details: '<strong>Action:</strong> Execute the environment verification script to ensure Python 3.8+, NumPy, Matplotlib, and Jupyter are properly installed. <strong>Boundaries:</strong> If tests fail, debug one dependency at a time—don\'t reinstall everything at once. Check Python version with <code>python --version</code>, verify pip with <code>pip --version</code>, then test imports individually in a Python shell. <strong>Deliverable:</strong> All environment tests passing, confirming a working setup for the entire plan. <strong>Verification:</strong> Script should output "All checks passed" or similar success message. Common pitfall: mixing system Python with virtual environment—use <code>python -m venv env</code> to create an isolated environment. Success check: Can you import numpy, matplotlib.pyplot, and jupyter without errors? Estimated time: 15 minutes for clean install, up to 45 minutes if troubleshooting is needed. <strong>Resources:</strong> <a href="https://docs.python.org/3/tutorial/venv.html" target="_blank" rel="noopener">Python Virtual Environments</a>, <a href="https://numpy.org/install/" target="_blank" rel="noopener">NumPy Installation Guide</a>' },
            { label: 'Create notebooks/foundations/day01_vectors_intro.ipynb with vector visualization', estMinutes: 60, details: '<strong>Action:</strong> Create a new Jupyter notebook with sections for: (1) Creating NumPy vectors, (2) Vector addition visualization, (3) Scalar multiplication demonstration, (4) 2D and 3D vector plots. Use <code>matplotlib.pyplot</code> with <code>quiver()</code> for 2D arrows and <code>mpl_toolkits.mplot3d</code> for 3D visualization. <strong>Boundaries:</strong> Keep vectors simple (2D and 3D only), focus on clarity over complexity. Include markdown cells explaining each code section. <strong>Deliverable:</strong> A well-commented notebook showing at least 3 vector visualizations with clear labels and axes. <strong>Verification:</strong> Notebook renders properly in Jupyter, plots are readable, code cells run in sequence without errors. Common pitfall: forgetting to set equal aspect ratios in plots, making unit vectors appear non-unit. Use <code>plt.axis(\'equal\')</code>. Success check: Can a beginner follow your notebook and understand vector basics? Estimated time: 60 minutes for creation and testing. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.quiver.html" target="_blank" rel="noopener">Matplotlib quiver documentation</a>' },
            { label: 'Generate artifacts/day01_vectors_plot.png showing 2D/3D vector examples', estMinutes: 30, details: '<strong>Action:</strong> Export your best vector visualization from the notebook as a high-quality PNG image (300 DPI recommended). Create a figure showing vector addition in 2D (left subplot) and a 3D vector example (right subplot) using <code>fig.savefig(\'artifacts/day01_vectors_plot.png\', dpi=300, bbox_inches=\'tight\')</code>. <strong>Boundaries:</strong> Image should be clear and publication-ready but doesn\'t need to be artistic—focus on educational clarity. Include axis labels, grid, and a legend if multiple vectors are shown. <strong>Deliverable:</strong> A single PNG file demonstrating your understanding of vector visualization techniques. <strong>Verification:</strong> Image file exists, is viewable, and clearly shows labeled vectors with proper scaling. Common pitfall: saving plots with default low resolution—always specify dpi parameter. Success check: Could you use this image in a presentation to explain vectors to someone else? Estimated time: 30 minutes including iterations for clarity. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/gallery/subplots_axes_and_figures/index.html" target="_blank" rel="noopener">Matplotlib Subplots Gallery</a>' },
            { label: 'Write docs/notes/day01_vectors.md summarizing vectors as geometric objects', estMinutes: 30, details: '<strong>Action:</strong> Create a markdown document summarizing today\'s learning in your own words. Include: (1) Definition of vectors, (2) Geometric interpretation (arrows in space), (3) Algebraic representation (arrays of numbers), (4) Why vectors matter for ML (represent features, weights, gradients), (5) Key operations covered (addition, scaling). <strong>Boundaries:</strong> Keep it concise (300-500 words max), focus on concepts not code. Write as if teaching a peer who missed today\'s lessons. <strong>Deliverable:</strong> A clear, well-structured markdown file in docs/notes/ directory. <strong>Verification:</strong> Document renders properly in a markdown viewer, contains no spelling errors, explains concepts clearly. Common pitfall: copying definitions verbatim instead of rephrasing in your own words—true understanding comes from translation. Success check: Can you explain vectors without looking at your notes afterward? Estimated time: 30 minutes for writing and review. <strong>Resources:</strong> <a href="https://www.markdownguide.org/basic-syntax/" target="_blank" rel="noopener">Markdown Syntax Guide</a>, <a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet" target="_blank" rel="noopener">Markdown Cheatsheet</a>' }
          ],
          reflectionPrompt: 'Consider: How do vectors differ from scalars? Why are they fundamental to ML?'
        },
        {
          globalDay: 2,
          week: 1,
          title: 'Vector Operations & Community Intro',
          priority: 'HIGH',
          tasks: [
            { label: 'Complete DataCamp "Introduction to NumPy" Chapter 2', estMinutes: 90, details: '<strong>Action:</strong> Progress to <a href="https://www.datacamp.com/courses/intro-to-python-for-data-science" target="_blank" rel="noopener">DataCamp NumPy Chapter 2</a>, focusing on array indexing, slicing, and boolean indexing. <strong>Boundaries:</strong> Master 1D and 2D array access patterns before moving to higher dimensions. Practice with exercises until indexing feels intuitive. <strong>Deliverable:</strong> Complete all Chapter 2 exercises with correct solutions, understanding each indexing technique. <strong>Verification:</strong> Can you extract any row, column, or subarray from a 2D array without looking up syntax? Common pitfall: confusion between row-first vs column-first indexing—NumPy uses [row, column] ordering. Success check: Solve this: given a 5×5 array, extract the center 3×3 subarray using slicing. Estimated time: 90 minutes with practice problems. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/user/basics.indexing.html" target="_blank" rel="noopener">NumPy Indexing Guide</a>' },
            { label: 'Watch 3Blue1Brown Essence of Linear Algebra Ep. 2 "Linear combinations, span, and basis vectors"', estMinutes: 10, resourceLinks: ['https://www.youtube.com/watch?v=k7RM-ot2NWY'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=k7RM-ot2NWY" target="_blank" rel="noopener">Episode 2: Linear Combinations, Span, and Basis Vectors</a> (10 minutes). Focus on understanding how any vector can be built from basis vectors through scaling and addition. <strong>Boundaries:</strong> Concentrate on 2D examples shown in the video—3D generalization comes naturally later. Pay attention to the concept of "span" as all possible destinations reachable by scaling and adding basis vectors. <strong>Deliverable:</strong> Intuitive understanding that basis vectors are like coordinate axes and linear combinations are like recipes for reaching any point. <strong>Verification:</strong> Can you explain why two parallel vectors cannot span a 2D plane? Common pitfall: memorizing definitions without visualizing—pause the video and sketch examples on paper. Success check: Explain in your own words what "span" means and why it matters. This concept is crucial for understanding neural network representations. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces" target="_blank" rel="noopener">Khan Academy: Vectors and Spaces</a>' },
            { label: 'Create notebooks/foundations/day02_vector_ops.ipynb with addition, scaling, dot product', estMinutes: 90, details: '<strong>Action:</strong> Build a comprehensive notebook covering: (1) Vector addition with tip-to-tail visualization, (2) Scalar multiplication showing direction preservation and magnitude scaling, (3) Dot product computation and geometric interpretation (projection), (4) Interactive examples where you can change values and see results update. Use NumPy for computations and Matplotlib for visualizations. <strong>Boundaries:</strong> Include both 2D visual examples and numerical output. Add markdown explanations between code cells describing what each operation does geometrically and algebraically. <strong>Deliverable:</strong> A tutorial-style notebook that someone else could learn from, with at least 4 complete examples and visualizations. <strong>Verification:</strong> Notebook runs top-to-bottom without errors, visualizations are clear and labeled, code is commented. Common pitfall: computing dot products without understanding they measure alignment—dot product is large when vectors point same direction, zero when perpendicular. Success check: Can you predict the dot product sign before computing it? Estimated time: 90 minutes for thorough implementation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/routines.linalg.html" target="_blank" rel="noopener">NumPy Linear Algebra</a>' },
            { label: 'Join Hugging Face Discord and introduce yourself in #introductions', estMinutes: 20, details: '<strong>Action:</strong> Navigate to <a href="https://discord.gg/hugging-face-879548962464493619" target="_blank" rel="noopener">Hugging Face Discord</a>, join the server, and post a brief introduction in the #introductions channel. <strong>Boundaries:</strong> Keep introduction concise but genuine: mention your background (9th grade, Israel), your learning goal (LLM mastery over 12 months), and ask one thoughtful question to start engagement. <strong>Deliverable:</strong> An introduction post that invites helpful responses and potential study partners. <strong>Verification:</strong> Post is live, appropriately friendly, and represents you authentically. Common pitfall: being too generic or too detailed—aim for 3-5 sentences that show personality and seriousness. Success check: Did you get any welcoming responses or connections? Example: "Hi! I\'m DovJNash, 9th grade student in Israel starting a 12-month journey to master LLM systems from foundations to deployment. Currently on Day 2 learning linear algebra basics. What resource did you find most helpful when starting with transformers?" Estimated time: 20 minutes including reading server rules. <strong>Resources:</strong> <a href="https://huggingface.co/community" target="_blank" rel="noopener">Hugging Face Community</a>' },
            { label: 'Write docs/notes/day02_span.md explaining span and linear combinations', estMinutes: 30, details: '<strong>Action:</strong> Create a markdown document explaining: (1) What a linear combination is (scaling + adding vectors), (2) The concept of span (all possible points reachable via linear combinations), (3) Why linear independence matters (whether vectors add new dimensions to the span), (4) Connection to ML (feature combinations, neural network activations). <strong>Boundaries:</strong> Use 2D examples for clarity—draw simple diagrams using ASCII art or describe verbally. Keep mathematical notation minimal, focus on intuition. <strong>Deliverable:</strong> A 400-600 word document that clearly explains these interconnected concepts with examples. <strong>Verification:</strong> Someone reading your document should understand why three vectors in 2D space cannot all be linearly independent. Common pitfall: treating span as abstract math instead of practical tool—emphasize that span tells us what\'s expressible with our basis. Success check: Can you explain why a neural network layer\'s span determines what patterns it can represent? Estimated time: 30 minutes for thoughtful writing. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-combinations/v/linear-combinations-and-span" target="_blank" rel="noopener">Khan Academy: Linear Combinations</a>' }
          ],
          reflectionPrompt: 'Post in HF Discord about your learning journey start.'
        },
        {
          globalDay: 3,
          week: 1,
          title: 'Linear Combinations & Span',
          priority: 'HIGH',
          tasks: [
            { label: 'Complete DataCamp "Introduction to NumPy" Chapter 3', estMinutes: 90, details: '<strong>Action:</strong> Complete <a href="https://www.datacamp.com/courses/intro-to-python-for-data-science" target="_blank" rel="noopener">DataCamp NumPy Chapter 3</a> on advanced array operations, broadcasting, and universal functions. <strong>Boundaries:</strong> Focus on understanding broadcasting rules: how NumPy handles operations between arrays of different shapes. Practice with at least 5 different broadcasting scenarios. <strong>Deliverable:</strong> Completed exercises demonstrating mastery of broadcasting and ufuncs. <strong>Verification:</strong> Can you predict output shapes before running operations? Common pitfall: broadcasting failures due to incompatible dimensions—remember the trailing dimensions must match or be 1. Success check: Multiply a (3,1) array by a (1,4) array and predict the result shape (3,4). Estimated time: 90 minutes with experimentation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html" target="_blank" rel="noopener">NumPy Broadcasting Rules</a>' },
            { label: 'Complete Khan Academy linear algebra: vector intro and span pages', estMinutes: 60, details: '<strong>Action:</strong> Work through <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces" target="_blank" rel="noopener">Khan Academy Linear Algebra: Vectors and Spaces</a> unit, specifically the vector introduction and span sections. <strong>Boundaries:</strong> Complete all practice exercises, aim for 100% mastery. Don\'t skip the intuition-building exercises even if they seem simple. <strong>Deliverable:</strong> Green checkmarks on all vector intro and span exercises in your Khan Academy profile. <strong>Verification:</strong> Can you solve any linear combination problem presented? Common pitfall: rushing through without solving problems by hand—computational fluency comes from manual practice. Success check: Given vectors v₁=[2,1] and v₂=[1,3], can you determine if [7,10] is in their span? Calculate by hand first, verify with NumPy second. Estimated time: 60 minutes including all practice problems. <strong>Resources:</strong> <a href="https://www.youtube.com/playlist?list=PLHXZ9OQGMqxfUl0tcqPNTJsb7R6BqSLo6" target="_blank" rel="noopener">Khan Academy Linear Algebra Playlist</a>' },
            { label: 'Create notebooks/foundations/day03_linear_combinations.ipynb exploring span', estMinutes: 75, details: '<strong>Action:</strong> Build an interactive notebook demonstrating: (1) Linear combinations with different coefficients, (2) Visualizing span of two 2D vectors (plot all integer linear combinations in a grid), (3) Demonstrating linear dependence (what happens when vectors are parallel), (4) Exploring span of three 2D vectors (over-constrained case). Use color gradients or scatter plots to show reachable points. <strong>Boundaries:</strong> Keep visualizations in 2D for clarity, use interactive sliders if using plotly/ipywidgets. Include at least 3 distinct examples showing different span scenarios. <strong>Deliverable:</strong> A comprehensive notebook that visually proves key span concepts through code. <strong>Verification:</strong> Notebook demonstrates clear understanding of when vectors span a subspace vs full space. Common pitfall: plotting only a few linear combinations instead of showing the complete span region. Success check: Your visualization should clearly show that two non-parallel 2D vectors span the entire plane. Estimated time: 75 minutes for implementation and testing. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/tutorials/colors/colormaps.html" target="_blank" rel="noopener">Matplotlib Colormaps</a>' },
            { label: 'Generate artifacts/day03_span_coverage.png visualizing 2D span', estMinutes: 30, details: '<strong>Action:</strong> Create a publication-quality visualization showing: (1) Two basis vectors as arrows, (2) A filled region or dense scatter plot showing their span, (3) Several example linear combinations highlighted, (4) Labels and legend explaining the concept. Use professional color schemes and ensure high contrast for readability. <strong>Boundaries:</strong> Image should be self-explanatory to someone who hasn\'t taken linear algebra yet. Include a caption or title within the image. <strong>Deliverable:</strong> A clear, educational PNG file demonstrating span visually. <strong>Verification:</strong> Show the image to someone unfamiliar with linear algebra—can they grasp the basic idea? Common pitfall: cluttered visualizations with too much information—keep it simple and focused on one key insight. Success check: The span region should be visually obvious, and basis vectors should be clearly distinguishable. Save with <code>dpi=300</code> and appropriate figure size. Estimated time: 30 minutes including refinement. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/gallery/color/named_colors.html" target="_blank" rel="noopener">Matplotlib Colors Reference</a>' },
            { label: 'Start docs/questions/week_01.md with any unclear concepts', estMinutes: 20, details: '<strong>Action:</strong> Create a markdown document listing any concepts from Week 1 that remain unclear or confusing. Format as bullet points with specific questions, not vague concerns. For each unclear concept, note: (1) what you understand so far, (2) exactly what\'s confusing, (3) what you\'ve tried to clarify it. <strong>Boundaries:</strong> Be specific—instead of "vectors are confusing," write "I understand vector addition geometrically but don\'t see why we add components element-wise algebraically." This document is for your benefit and potential mentor discussions. <strong>Deliverable:</strong> A structured list of 3-10 specific questions or unclear concepts. <strong>Verification:</strong> Each question should be answerable by someone with expertise (i.e., not too vague). Common pitfall: not writing down questions because they seem "too basic"—if you\'re confused, document it! Success check: Questions are specific enough that researching or asking for help would give clear answers. Estimated time: 20 minutes for reflection and documentation. <strong>Resources:</strong> <a href="https://stackoverflow.com/help/how-to-ask" target="_blank" rel="noopener">How to Ask Good Questions</a>' }
          ],
          reflectionPrompt: 'What happens when vectors are parallel? How does this relate to linear independence?'
        },
        {
          globalDay: 4,
          week: 1,
          title: 'Matrix Basics & Transformations',
          priority: 'HIGH',
          tasks: [
            { label: 'Watch 3Blue1Brown Ep. 3 "Linear transformations and matrices"', estMinutes: 10, resourceLinks: ['https://www.youtube.com/watch?v=kYB8IZa5AuE'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=kYB8IZa5AuE" target="_blank" rel="noopener">3Blue1Brown Episode 3: Linear transformations and matrices</a> (10 minutes). Focus on understanding matrices as functions that transform space. <strong>Boundaries:</strong> Pay special attention to the column perspective: each matrix column shows where the corresponding basis vector lands after transformation. Pause to visualize how the grid transforms. <strong>Deliverable:</strong> Mental model of matrices as transformation machines that move every point in space systematically. <strong>Verification:</strong> Can you explain what the matrix [[1,2],[3,4]] does to the point (1,0)? Common pitfall: thinking of matrices as just arrays of numbers instead of geometric transformations. Success check: Can you predict the effect of a transformation matrix by looking at its columns? This perspective is fundamental for understanding neural network layers as transformations of representation space. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations" target="_blank" rel="noopener">Khan Academy: Matrix Transformations</a>' },
            { label: 'Complete Khan Academy: matrix intro and matrix-vector multiplication', estMinutes: 60, details: '<strong>Action:</strong> Complete <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations" target="_blank" rel="noopener">Khan Academy matrix introduction and matrix-vector multiplication</a> exercises. <strong>Boundaries:</strong> Work through all practice problems until achieving mastery level. Focus on both algebraic computation and geometric interpretation. Practice manually computing matrix-vector products before using calculators. <strong>Deliverable:</strong> Mastery badges on Khan Academy for matrix basics and matrix-vector multiplication sections. <strong>Verification:</strong> Can you compute a 3×3 matrix times a 3D vector by hand accurately? Can you explain what happens geometrically? Common pitfall: mechanical computation without understanding the geometric transformation. Success check: Given matrix A and vector v, predict how the transformation moves v before calculating. Estimated time: 60 minutes including all exercises. <strong>Resources:</strong> <a href="https://www.youtube.com/watch?v=kYB8IZa5AuE" target="_blank" rel="noopener">3Blue1Brown: Linear Transformations</a>' },
            { label: 'Create notebooks/foundations/day04_matrices.ipynb with transformation examples', estMinutes: 90, details: '<strong>Action:</strong> Build a comprehensive Jupyter notebook demonstrating matrix transformations: (1) Identity matrix (no change), (2) Rotation matrices (90°, 45°, arbitrary angle), (3) Scaling matrices (uniform and non-uniform), (4) Shear transformations, (5) Combined transformations. Visualize each with before/after grid plots showing how the transformation distorts space. Use matplotlib to plot transformed shapes and grid lines. <strong>Boundaries:</strong> Include both code for computation and visual output showing transformed grids. Add markdown cells explaining each transformation type and its matrix form. <strong>Deliverable:</strong> A tutorial-quality notebook with at least 5 different transformation examples, each with clear visualization and explanation. <strong>Verification:</strong> Notebook should run without errors, visualizations should clearly show the transformation effect, and explanations should be understandable to someone learning matrices for the first time. Common pitfall: creating visualizations without clear before/after comparison. Success check: Can someone else learn about matrix transformations from your notebook? Estimated time: 90 minutes for thorough implementation. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/tutorials/intermediate/transforms_tutorial.html" target="_blank" rel="noopener">Matplotlib Transforms Tutorial</a>' },
            { label: 'Generate artifacts/day04_transformation.png showing rotation/scaling transformations', estMinutes: 45, details: '<strong>Action:</strong> Create a publication-quality figure showing multiple transformations: (1) Original grid in one subplot, (2) Rotation transformation in second subplot, (3) Scaling transformation in third subplot, (4) Combined rotation+scaling in fourth subplot. Use a 2×2 subplot layout. Clearly label each transformation with its matrix. Show both the grid lines and a distinctive shape (like an "L" or arrow) being transformed. <strong>Boundaries:</strong> Use consistent colors: original in blue, transformed in red. Include the transformation matrix as text annotation on each subplot. Save as high-resolution PNG (300 DPI). <strong>Deliverable:</strong> A clear, educational visualization showing how different matrices transform space differently. <strong>Verification:</strong> Image should be self-explanatory with proper labels, axis labels, and transformation matrices visible. Common pitfall: cluttered visuals with too much information—keep it clean and focused. Success check: Could this image be used in a presentation about linear transformations? Estimated time: 45 minutes including design iterations. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplot.html" target="_blank" rel="noopener">Matplotlib Subplots Guide</a>' },
            { label: 'Add to docs/notes/day04_matrices.md explaining matrices as transformations', estMinutes: 30, details: '<strong>Action:</strong> Write a comprehensive markdown document (500-700 words) explaining: (1) Matrices as linear transformations, (2) The column perspective (columns = where basis vectors land), (3) Why transformations must be linear (preserves lines and origin), (4) Common transformation types (rotation, scaling, shear, reflection), (5) Connection to ML (neural network layers as learned transformations). Include examples and intuitive explanations. <strong>Boundaries:</strong> Write in your own words, avoiding copy-paste from sources. Use concrete examples like "rotating a square" rather than abstract language. Include at least one hand-drawn or described diagram concept. <strong>Deliverable:</strong> A well-structured markdown document that could help a peer understand matrix transformations conceptually. <strong>Verification:</strong> Document should be readable, accurate, and insightful. Check for spelling and grammar. Common pitfall: being too abstract without concrete examples. Success check: Can you explain to someone verbally what you wrote without looking at notes? Estimated time: 30 minutes for thoughtful writing. <strong>Resources:</strong> <a href="https://www.3blue1brown.com/topics/linear-algebra" target="_blank" rel="noopener">3Blue1Brown Linear Algebra Series</a>' }
          ],
          reflectionPrompt: 'Why is the column perspective for matrix-vector multiplication useful?'
        },
        {
          globalDay: 5,
          week: 1,
          title: 'Matrix Multiplication & Composition',
          priority: 'HIGH',
          tasks: [
            { label: 'Watch 3Blue1Brown Ep. 4 "Matrix multiplication as composition"', estMinutes: 10, resourceLinks: ['https://www.youtube.com/watch?v=XkY2DOUCWMU'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=XkY2DOUCWMU" target="_blank" rel="noopener">3Blue1Brown Episode 4: Matrix multiplication as composition</a> (10 minutes). Focus on understanding that multiplying matrices means composing their transformations—applying one transformation after another. <strong>Boundaries:</strong> Pay attention to the right-to-left reading order: AB means "first apply B, then apply A." This is counterintuitive but crucial. Pause when Grant shows the grid transformations to really see how compositions work. <strong>Deliverable:</strong> Clear mental model of matrix multiplication as transformation composition, not just number crunching. <strong>Verification:</strong> Can you explain why matrix multiplication is not commutative (AB ≠ BA) using transformation logic? Common pitfall: treating matrix mult as just multiplying numbers without geometric understanding. Success check: Explain why rotating then scaling gives different results than scaling then rotating. This concept is fundamental for understanding deep neural networks as compositions of learned transformations. <strong>Resources:</strong> <a href="https://www.youtube.com/watch?v=XkY2DOUCWMU" target="_blank" rel="noopener">3Blue1Brown Video</a>' },
            { label: 'DataCamp: Intermediate Python - functions and loops (refresher)', estMinutes: 60, details: '<strong>Action:</strong> Complete <a href="https://www.datacamp.com/courses/intermediate-python" target="_blank" rel="noopener">DataCamp Intermediate Python</a> sections on functions and loops as a refresher. <strong>Boundaries:</strong> Focus on writing clean, readable functions with proper documentation. Practice loops with enumerate() and zip(). Review list comprehensions. <strong>Deliverable:</strong> Completed exercises demonstrating proficiency with Python functions, loops, and functional programming patterns. <strong>Verification:</strong> Can you write a function that takes arguments, has a docstring, and returns values correctly? Can you choose between for loops and comprehensions appropriately? Common pitfall: writing overly complex nested loops—practice breaking problems into functions. Success check: Write a function that transposes a matrix (list of lists) using nested loops, then rewrite it with a list comprehension. Both should work correctly. Estimated time: 60 minutes for thorough review. <strong>Resources:</strong> <a href="https://docs.python.org/3/tutorial/controlflow.html#defining-functions" target="_blank" rel="noopener">Python Functions Documentation</a>' },
            { label: 'Create notebooks/foundations/day05_mat_mult.ipynb with composition examples', estMinutes: 90, details: '<strong>Action:</strong> Build a notebook demonstrating matrix multiplication as composition: (1) Define two transformation matrices (e.g., rotation and scaling), (2) Show their individual effects on a shape, (3) Show their composition AB (rotate then scale), (4) Show their composition BA (scale then rotate), (5) Visualize how order matters with side-by-side comparisons. Include the actual matrix multiplication computation with step-by-step annotation. <strong>Boundaries:</strong> Make it highly visual—every transformation should have a before/after plot. Use different colors for different stages. Add markdown explaining why order matters geometrically. <strong>Deliverable:</strong> A comprehensive notebook proving through visualization that matrix multiplication is non-commutative and represents transformation composition. <strong>Verification:</strong> Someone reading your notebook should understand both how to compute matrix products AND why order matters geometrically. Common pitfall: showing computation without geometric intuition. Success check: Your visualizations clearly demonstrate that AB ≠ BA. Estimated time: 90 minutes including multiple examples. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/generated/numpy.matmul.html" target="_blank" rel="noopener">NumPy matmul documentation</a>' },
            { label: 'Implement matrix multiplication from scratch (no NumPy matmul)', estMinutes: 60, details: '<strong>Action:</strong> Write a Python function <code>def matmul(A, B)</code> that multiplies two matrices using only basic Python (no NumPy matmul or dot). Use nested loops following the mathematical definition: C[i][j] = sum(A[i][k] * B[k][j] for k). Add error checking for incompatible dimensions. Include docstring with examples. Test thoroughly with known cases. <strong>Boundaries:</strong> Implement the algorithm yourself to understand what matrix multiplication really does. Don\'t look up implementations—derive it from the mathematical definition. After implementing, verify against NumPy for correctness. <strong>Deliverable:</strong> A working matmul function with tests showing it matches NumPy results for various matrix sizes. <strong>Verification:</strong> Your function should handle 2×2, 3×3, and non-square matrices correctly. Test edge cases like identity matrices. Common pitfall: index confusion in nested loops (i,j,k)—draw out the loops on paper first. Success check: Your implementation matches NumPy output exactly for at least 5 test cases of varying dimensions. Estimated time: 60 minutes including testing and debugging. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Matrix_multiplication#Definition" target="_blank" rel="noopener">Matrix Multiplication Definition</a>' },
            { label: 'Write docs/notes/day05_composition.md on why order matters', estMinutes: 30, details: '<strong>Action:</strong> Write a focused document (400-600 words) explaining why matrix multiplication order matters, using: (1) Concrete transformation examples (rotate then scale vs scale then rotate), (2) The mathematical reason (matrices represent functions, function composition is not commutative), (3) Practical implications for ML (network layer order matters), (4) When order doesn\'t matter (commuting matrices like diagonal matrices). <strong>Boundaries:</strong> Make it intuitive with real-world analogies—perhaps "putting on socks then shoes" vs "putting on shoes then socks". Avoid heavy mathematical notation, focus on understanding. <strong>Deliverable:</strong> A clear, insightful explanation that someone could reference when wondering about matrix multiplication order. <strong>Verification:</strong> Document should answer "why does AB ≠ BA?" in multiple complementary ways. Common pitfall: just stating the fact without explaining why it\'s true. Success check: After reading, someone should have geometric, algebraic, and intuitive reasons for non-commutativity. Estimated time: 30 minutes for clear writing. <strong>Resources:</strong> <a href="https://betterexplained.com/articles/matrix-multiplication/" target="_blank" rel="noopener">Better Explained: Matrix Multiplication</a>' }
          ],
          reflectionPrompt: 'Try composing rotation + scaling vs scaling + rotation. What changes?'
        },
        {
          globalDay: 6,
          week: 1,
          title: 'Determinants & Inverses',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Watch 3Blue1Brown Ep. 5 "The determinant"', estMinutes: 10, resourceLinks: ['https://www.youtube.com/watch?v=Ip3X9LOh2dk'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=Ip3X9LOh2dk" target="_blank" rel="noopener">3Blue1Brown Episode 5: The determinant</a> (10 minutes). Focus on the geometric meaning: determinant measures how much a transformation scales areas (or volumes in higher dimensions). <strong>Boundaries:</strong> Understand the three key cases: det > 1 (expands area), 0 < det < 1 (shrinks area), det < 0 (flips orientation). Pay attention to what det = 0 means (squishes space to lower dimension). <strong>Deliverable:</strong> Geometric intuition for determinants as area/volume scaling factors. <strong>Verification:</strong> Can you explain what it means if a transformation has determinant 2? What about -1? What about 0? Common pitfall: memorizing the formula without understanding the geometric meaning. Success check: Given a 2×2 matrix, predict whether it expands or shrinks areas before calculating. This concept is crucial for understanding when transformations are invertible and for change-of-variables in probability. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations/determinant-depth/v/linear-algebra-determinant-as-scaling-factor" target="_blank" rel="noopener">Khan Academy: Determinant as Scaling Factor</a>' },
            { label: 'Watch 3Blue1Brown Ep. 6 "Inverse matrices, column space and null space"', estMinutes: 12, resourceLinks: ['https://www.youtube.com/watch?v=uQhTuRlWMxw'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=uQhTuRlWMxw" target="_blank" rel="noopener">3Blue1Brown Episode 6: Inverse matrices, column space and null space</a> (12 minutes). Focus on understanding: (1) Inverse as "undoing" a transformation, (2) Column space as the span of the columns (where things can land), (3) Null space as what gets squished to zero. <strong>Boundaries:</strong> Connect these concepts: a matrix is invertible if and only if det ≠ 0, which means it doesn\'t squish to lower dimension, which means its null space is just {0}. Pause to visualize column space and null space geometrically. <strong>Deliverable:</strong> Clear mental models connecting determinants, invertibility, column space, and null space. <strong>Verification:</strong> Can you explain why a matrix with det = 0 has no inverse? Why does such a matrix have a non-trivial null space? Common pitfall: treating these as separate unrelated concepts instead of seeing how they interconnect. Success check: Explain the relationship between determinant, invertibility, and null space in one coherent explanation. Estimated time: 12 minutes, may want to watch twice for full understanding. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations/inverse-of-matrices/v/linear-algebra-inverse-matrix-introduction" target="_blank" rel="noopener">Khan Academy: Inverse Matrices</a>' },
            { label: 'Create notebooks/foundations/day06_determinants.ipynb with det calculations', estMinutes: 75, details: '<strong>Action:</strong> Build a comprehensive notebook covering: (1) Computing 2×2 determinants by hand and with NumPy, (2) Visualizing how transformations with different determinants affect area (show unit square transforming), (3) Computing 3×3 determinants, (4) Exploring special cases (identity: det=1, zero matrix: det=0, rotation: det=1), (5) Demonstrating that det(AB) = det(A)×det(B). Include both computational examples and geometric visualizations. <strong>Boundaries:</strong> For each determinant you compute, create a visualization showing how that transformation scales area. Use color-coding to show before and after areas. <strong>Deliverable:</strong> A tutorial notebook that teaches determinants through both computation and visualization, with at least 6 worked examples. <strong>Verification:</strong> Notebook should clearly demonstrate the geometric meaning of determinants, not just the computation. Common pitfall: computing determinants without showing what they mean visually. Success check: Someone should be able to develop geometric intuition for determinants from your notebook. Estimated time: 75 minutes for thorough implementation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.det.html" target="_blank" rel="noopener">NumPy Determinant Function</a>' },
            { label: 'Generate artifacts/day06_det_visual.png showing area scaling', estMinutes: 30, details: '<strong>Action:</strong> Create a compelling visualization showing: (1) Original unit square, (2-4) The same square after transformations with det=2, det=0.5, and det=-1, (5) Calculate and display the actual area after each transformation. Use a 2×2 subplot layout. Annotate each subplot with the transformation matrix and its determinant. Use semi-transparent fill to show area clearly. <strong>Boundaries:</strong> Make the area scaling visually obvious—perhaps fill with different color intensities proportional to area. For det=-1, show that orientation flips (e.g., with arrows showing clockwise vs counterclockwise). <strong>Deliverable:</strong> A publication-quality figure that makes the "determinant = area scaling" concept immediately obvious. <strong>Verification:</strong> Image should be self-explanatory with clear labels showing how determinant relates to area change. Common pitfall: not making the area scaling visually prominent enough. Success check: Someone unfamiliar with determinants should immediately understand the connection to area scaling from your visualization. Save at 300 DPI. Estimated time: 30 minutes including design iteration. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Polygon.html" target="_blank" rel="noopener">Matplotlib Polygon Patches</a>' },
            { label: 'Write docs/notes/day06_determinants.md on geometric meaning', estMinutes: 30, details: '<strong>Action:</strong> Write a comprehensive document (500-700 words) explaining: (1) Determinant as area/volume scaling factor, (2) Sign indicating orientation (flip or not), (3) Zero determinant meaning squishing to lower dimension, (4) Connection to invertibility (det ≠ 0 ↔ invertible), (5) Determinant properties (det(AB) = det(A)×det(B), det(A⁻¹) = 1/det(A)), (6) Why this matters for ML (checking if data transformations are information-preserving, understanding when systems have unique solutions). <strong>Boundaries:</strong> Emphasize geometric meaning over computational formulas. Use concrete examples like "a transformation that doubles all areas has det=2." <strong>Deliverable:</strong> A well-structured document that could serve as a reference for understanding determinants geometrically. <strong>Verification:</strong> Should explain both what determinants are and why they\'re useful. Common pitfall: listing properties without explaining why they\'re true or useful. Success check: After reading, someone should understand determinants intuitively, not just algorithmically. Estimated time: 30 minutes for thoughtful exposition. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Determinant#Geometric_meaning" target="_blank" rel="noopener">Wikipedia: Determinant Geometric Meaning</a>' }
          ],
          reflectionPrompt: 'What does det=0 mean for invertibility? Why?'
        },
        {
          globalDay: 7,
          week: 1,
          title: 'Week 1 Review & Reflection',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review all Week 1 notebooks and notes', estMinutes: 60, details: '<strong>Action:</strong> Systematically review all materials from Days 1-6: open each notebook, re-run all cells, review all notes documents. Look for: (1) Incomplete sections, (2) Confusing explanations, (3) Code that doesn\'t run anymore, (4) Concepts that felt unclear. <strong>Boundaries:</strong> This is active review, not passive reading—re-run code, modify examples to test understanding, add clarifying comments where needed. Spend about 10 minutes per day\'s materials. <strong>Deliverable:</strong> Refreshed understanding of Week 1 content, with any gaps or questions identified. <strong>Verification:</strong> You should feel confident you could explain any concept from Week 1 to someone else. Common pitfall: passive skimming instead of active engagement. Success check: Can you solve a random linear algebra problem (matrix multiplication, determinant calculation, vector projection) without looking up the formula? Estimated time: 60 minutes for thorough review across 6 days of content. <strong>Resources:</strong> Your own Week 1 notebooks and notes' },
            { label: 'Update docs/questions/week_01.md with remaining questions', estMinutes: 20, details: '<strong>Action:</strong> Review your Week 1 questions document and update it: (1) Mark which questions you can now answer, (2) Add any new questions that arose during review, (3) Note which concepts still feel shaky, (4) Identify topics needing more practice. Format clearly so you can discuss with mentors or research further. <strong>Boundaries:</strong> Be honest about what\'s still unclear—it\'s better to acknowledge confusion now than carry it forward. No question is too basic to document. <strong>Deliverable:</strong> An updated questions document showing your current understanding status after Week 1. <strong>Verification:</strong> Document should clearly distinguish between "answered" and "still unclear" questions. Common pitfall: pretending to understand everything—confusion is part of learning! Success check: You have a clear list of what to research further or ask about. Estimated time: 20 minutes for reflection and documentation. <strong>Resources:</strong> <a href="https://www.coursera.org/learn/learning-how-to-learn" target="_blank" rel="noopener">Learning How to Learn (Coursera)</a>' },
            { label: 'Write docs/weekly_logs/week_01.md summarizing key learnings', estMinutes: 45, details: '<strong>Action:</strong> Create a comprehensive week summary (800-1200 words) covering: (1) Main topics covered (vectors, matrices, transformations, determinants), (2) Key insights that clicked for you, (3) Hardest concepts and how you approached them, (4) Artifacts created (notebooks, visualizations, notes), (5) Skills developed (Python, NumPy, Jupyter, mathematical thinking), (6) Community engagement (Discord, questions asked/answered), (7) Time management lessons, (8) Plans for Week 2. <strong>Boundaries:</strong> Write reflectively in first person—this is your learning journal, not a formal report. Be specific about what worked and what didn\'t. Include challenges and how you overcame them. <strong>Deliverable:</strong> A thoughtful weekly log that captures your learning journey and serves as future reference. <strong>Verification:</strong> Log should read as honest reflection, not a checklist. It should help you recognize patterns in how you learn best. Common pitfall: writing generic summaries without personal insight. Success check: Rereading this log in 6 months should remind you of your experience and growth. Estimated time: 45 minutes for thoughtful writing. <strong>Resources:</strong> <a href="https://www.edutopia.org/article/powerful-benefits-reflective-journaling" target="_blank" rel="noopener">Benefits of Reflective Journaling</a>' },
            { label: 'Complete 5 practice problems from Khan Academy linear algebra', estMinutes: 60, details: '<strong>Action:</strong> Complete 5 practice problems from <a href="https://www.khanacademy.org/math/linear-algebra" target="_blank" rel="noopener">Khan Academy Linear Algebra</a> covering Week 1 topics: (1) Matrix-vector multiplication, (2) Matrix multiplication, (3) Determinant calculation, (4) Inverse matrix, (5) Transformation interpretation. Choose problems that challenge you—pick ones marked "Hard" or that you previously struggled with. <strong>Boundaries:</strong> Do problems by hand first, then verify with calculator/code. If you get stuck, review the concept rather than just looking at the answer. <strong>Deliverable:</strong> 5 correctly solved problems demonstrating Week 1 competency. <strong>Verification:</strong> You should get correct answers confidently, understanding each step. If struggling with more than 2 problems, you may need more review before Week 2. Common pitfall: rushing through problems without understanding—slow down and understand each step. Success check: Can you explain why your answer is correct, not just compute it? Estimated time: 60 minutes for 5 substantial problems including reflection. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations" target="_blank" rel="noopener">Khan Academy: Matrix Transformations</a>' },
            { label: 'Post Week 1 progress update in HF Discord or Twitter', estMinutes: 15, details: '<strong>Action:</strong> Share your Week 1 journey publicly on <a href="https://discord.gg/hugging-face-879548962464493619" target="_blank" rel="noopener">Hugging Face Discord</a> or Twitter. Include: (1) Brief summary of what you learned (vectors, matrices, transformations), (2) One cool visualization or insight you created, (3) One challenge you overcame, (4) One question you\'re pondering, (5) Your plan for Week 2. Keep it genuine and conversational—people appreciate authentic learning stories. <strong>Boundaries:</strong> Don\'t oversell or undersell your progress—be authentic. Share something visual if possible (a plot or notebook screenshot). Keep it concise (150-300 words or a thread). <strong>Deliverable:</strong> A public post sharing your learning progress and engaging with the ML community. <strong>Verification:</strong> Post should represent your journey honestly and invite engagement (questions or discussion). Common pitfall: waiting until you\'re "expert enough" to share—share the learning process itself! Success check: Did you get any responses, encouragement, or new connections? Did sharing help solidify your learning? Estimated time: 15 minutes for drafting and posting. <strong>Resources:</strong> <a href="https://www.swyx.io/learn-in-public/" target="_blank" rel="noopener">Learn in Public Philosophy</a>' }
          ],
          reflectionPrompt: 'What was hardest this week? What clicked? What still feels fuzzy?'
        },
        // Week 2: Days 8-14 (continuing pattern with eigenvalues expansion)
        {
          globalDay: 8,
          week: 2,
          title: 'Dot Product & Duality',
          priority: 'HIGH',
          tasks: [
            { label: 'Watch 3Blue1Brown Ep. 7 "Dot products and duality"', estMinutes: 15, resourceLinks: ['https://www.youtube.com/watch?v=LyGKycYT2v0'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=LyGKycYT2v0" target="_blank" rel="noopener">3Blue1Brown Episode 7: Dot products and duality</a> (15 minutes), focusing on two interpretations: the algebraic definition (multiply corresponding components and sum) and the geometric meaning (projection of one vector onto another, scaled by the second vector\'s length). <strong>Boundaries:</strong> Pay special attention to the duality concept—how we can think of dot products as either projecting vectors or as linear transformations from vectors to numbers. Pause when Grant shows the connection between these views. Don\'t worry about memorizing formulas; focus on the geometric intuition of dot products measuring "how much two vectors align." <strong>Deliverable:</strong> Clear understanding that dot product combines two key ideas: alignment (via projection) and magnitude. Mental model connecting algebraic computation to geometric meaning. <strong>Verification:</strong> Can you explain why perpendicular vectors have dot product zero? Why parallel vectors have maximum dot product? Can you visualize what a dot product represents geometrically? Common pitfall: treating dot product as just a computation without geometric understanding. Success check: Explain why cosine similarity (used everywhere in ML for measuring vector similarity) is essentially a normalized dot product. This concept is fundamental for understanding attention mechanisms in transformers. Estimated time: 15 minutes, watch twice if needed for full clarity. <strong>Resources:</strong> <a href="https://www.3blue1brown.com/topics/linear-algebra" target="_blank" rel="noopener">3Blue1Brown Linear Algebra Series</a>, <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length" target="_blank" rel="noopener">Khan Academy: Dot Product</a>' },
            { label: 'Complete Khan Academy: dot product and projections', estMinutes: 60, details: '<strong>Action:</strong> Work through <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products" target="_blank" rel="noopener">Khan Academy dot product and vector projections</a> exercises, completing all practice problems until achieving mastery. Focus on: (1) Computing dot products algebraically, (2) Understanding the geometric interpretation, (3) Using dot products to find vector projections, (4) Recognizing when vectors are orthogonal (perpendicular). <strong>Boundaries:</strong> Work through problems by hand first, then verify with calculator. If stuck on a problem for more than 10 minutes, review the concept video rather than just checking the answer. Aim for 100% mastery on each subtopic before moving forward. Practice both 2D and 3D examples. <strong>Deliverable:</strong> Mastery badges on Khan Academy for dot product computation, geometric interpretation, and vector projection sections. Ability to compute dot products quickly and accurately for vectors of any dimension. <strong>Verification:</strong> Can you compute a dot product in your head for simple vectors? Given two vectors, can you immediately tell if they\'re orthogonal by computing their dot product? Can you project one vector onto another and explain what this means geometrically? Common pitfall: memorizing the formula without understanding what you\'re computing or why it matters. Success check: Solve this problem confidently: given vectors a=[3,4] and b=[4,-3], compute their dot product and explain why the result tells you they\'re perpendicular. Understand that orthogonality (dot product = 0) is crucial in ML for feature independence and principal component analysis. Estimated time: 60 minutes including all practice problems and review. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products" target="_blank" rel="noopener">Khan Academy: Dot Products</a>, <a href="https://www.youtube.com/watch?v=LyGKycYT2v0" target="_blank" rel="noopener">3Blue1Brown: Dot Products</a>' },
            { label: 'Create notebooks/foundations/day08_dot_product.ipynb exploring orthogonality', estMinutes: 90, details: '<strong>Action:</strong> Create a comprehensive Jupyter notebook (notebooks/foundations/day08_dot_product.ipynb) demonstrating: (1) Dot product computation using NumPy\'s np.dot() and @ operator, (2) Visualizing dot products geometrically with vector plots, (3) Computing and visualizing vector projections, (4) Exploring orthogonality by checking when dot products equal zero, (5) Demonstrating the connection between dot product and angle (using np.arccos and the formula: dot(a,b) = ||a|| ||b|| cos(θ)). Include both 2D and 3D examples with clear matplotlib visualizations. <strong>Boundaries:</strong> Create at least 5 distinct examples: (1) parallel vectors (max dot product), (2) perpendicular vectors (zero dot product), (3) vectors at various angles, (4) projecting one vector onto another with visual representation, (5) checking orthogonality of multiple vectors. Add markdown cells explaining each concept before the code. Make visualizations publication-quality with proper labels and colors. <strong>Deliverable:</strong> A tutorial-style notebook that clearly demonstrates dot products, projections, and orthogonality through both computation and visualization. Should include at least 5 visualizations showing different scenarios and relationships. <strong>Verification:</strong> Notebook runs top-to-bottom without errors. Visualizations clearly show the geometric relationships. Code is well-commented. Markdown explanations connect the code to the concepts. Someone learning about dot products should gain both computational and geometric understanding from your notebook. Common pitfall: creating code without adequate visual representation or explanation—remember, this is about building geometric intuition. Success check: Your visualizations should clearly show why perpendicular vectors have zero dot product and how projection works geometrically. Can you use your notebook to explain these concepts to someone else? Estimated time: 90 minutes for thorough implementation and documentation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html" target="_blank" rel="noopener">NumPy dot function</a>, <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.quiver.html" target="_blank" rel="noopener">Matplotlib quiver for vector plots</a>, <a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html" target="_blank" rel="noopener">Python Data Science Handbook: NumPy</a>' },
            { label: 'Generate artifacts/day08_projection.png showing vector projection', estMinutes: 30, details: '<strong>Action:</strong> Create a high-quality publication-ready visualization (artifacts/day08_projection.png) demonstrating vector projection. Show: (1) Two vectors a and b as arrows from the origin, (2) The projection of a onto b (shown as a vector along b\'s direction), (3) The perpendicular component (from the projection to a\'s tip), (4) Right angle indicator showing orthogonality of the perpendicular component, (5) Labels and annotations explaining each component. Use clear colors: perhaps blue for a, red for b, green for the projection, and dotted lines for construction. <strong>Boundaries:</strong> Make this didactic—someone should understand projection from this image alone. Include a title like "Vector Projection: a onto b" and add text annotations explaining what\'s shown. Save at 300 DPI with <code>bbox_inches=\'tight\'</code> to avoid whitespace. Use a clean, professional color scheme with sufficient contrast. Consider showing a concrete example like projecting [3,2] onto [1,0] so viewers can verify the math. <strong>Deliverable:</strong> A clear, well-labeled PNG file that effectively teaches the concept of vector projection through visualization. Image should be self-contained—viewable without the notebook context. <strong>Verification:</strong> Image file exists and is viewable. Resolution is sufficient for projection use (300+ DPI). Labels are readable and accurate. The geometric relationship is immediately clear. Could this image be used in a presentation about vector projections? Common pitfall: creating cluttered visualizations with poor labeling or low resolution. Success check: Show this image to someone unfamiliar with projections—can they grasp the basic concept? The projection should visually appear as the "shadow" of vector a falling on vector b\'s direction. This visualization technique is important because projection is the geometric basis of many ML operations, including least squares regression and principal component analysis. Estimated time: 30 minutes including design iterations and quality checks. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/gallery/text_labels_and_annotations/annotation_demo.html" target="_blank" rel="noopener">Matplotlib Annotations</a>, <a href="https://matplotlib.org/stable/tutorials/colors/colors.html" target="_blank" rel="noopener">Matplotlib Color Specification</a>' },
            { label: 'Write docs/notes/day08_duality.md on dual interpretation of dot product', estMinutes: 30, details: '<strong>Action:</strong> Write a comprehensive markdown document (docs/notes/day08_duality.md) explaining the dual nature of dot products—the two seemingly different but equivalent interpretations. Cover: (1) Algebraic view: multiply corresponding components and sum (a·b = a₁b₁ + a₂b₂ + ...), (2) Geometric view: projection and scaling (a·b = ||a|| ||b|| cos θ), (3) The duality concept: how these two views are really the same thing, (4) Why this matters for ML: dot products appear everywhere (similarity measures, matrix operations, attention mechanisms), (5) Connection to linear transformations: dot product with a fixed vector is a linear transformation from vectors to scalars. Use examples and avoid heavy mathematical notation—focus on understanding. <strong>Boundaries:</strong> Aim for 500-700 words. Write in your own words, not copying from sources. Include at least one concrete numerical example showing both interpretations give the same answer. Explain the intuition: dot product measures "how much two vectors point in the same direction" and the angle between them determines this. <strong>Deliverable:</strong> A clear, well-structured markdown document that could serve as a reference for understanding dot product duality. Should help someone see that the algebraic formula and geometric interpretation aren\'t separate facts but two views of the same operation. <strong>Verification:</strong> Document accurately explains both interpretations. Includes concrete examples. Makes the connection explicit between the two views. No spelling or grammar errors. Could someone read this and understand why dot product is so fundamental? Common pitfall: treating the two interpretations as unrelated formulas instead of explaining their deep connection. Success check: After writing, can you explain to someone (or yourself aloud) why multiplying components and summing is the same as ||a|| ||b|| cos θ? Understanding this duality is crucial because it lets you switch between computational and geometric thinking in ML contexts. Estimated time: 30 minutes for thoughtful writing and examples. <strong>Resources:</strong> <a href="https://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/" target="_blank" rel="noopener">Better Explained: Dot Product</a>, <a href="https://www.youtube.com/watch?v=LyGKycYT2v0" target="_blank" rel="noopener">3Blue1Brown: Dot Products and Duality</a>, <a href="https://en.wikipedia.org/wiki/Dot_product#Geometric_definition" target="_blank" rel="noopener">Wikipedia: Dot Product Geometric Definition</a>' }
          ],
          reflectionPrompt: 'How does dot product relate to cosine similarity in ML?'
        },
        {
          globalDay: 9,
          week: 2,
          title: 'Cross Product & 3D Geometry',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Watch 3Blue1Brown Ep. 8 "Cross products"', estMinutes: 8, resourceLinks: ['https://www.youtube.com/watch?v=eu6i7WJeinw'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=eu6i7WJeinw" target="_blank" rel="noopener">3Blue1Brown Episode 8: Cross products</a> (8 minutes), focusing on understanding the geometric meaning: the cross product of two 3D vectors produces a third vector perpendicular to both, with magnitude equal to the parallelogram area formed by the original vectors. <strong>Boundaries:</strong> Pay attention to the right-hand rule for determining direction and the connection between cross product magnitude and the area interpretation. Note that cross products are specific to 3D (and 7D in pure math, but we won\'t use that). Unlike dot products which produce scalars, cross products produce vectors. Don\'t worry about memorizing the component formula yet—focus on the geometric intuition first. <strong>Deliverable:</strong> Clear mental model of cross products as producing perpendicular vectors with magnitude related to area. Understanding of the right-hand rule for direction. <strong>Verification:</strong> Can you explain why the cross product of parallel vectors is zero (they form no area)? Why the result is always perpendicular to both input vectors? Can you use your right hand to determine the direction? Common pitfall: treating cross product as an arbitrary formula without geometric meaning. Success check: Explain why cross product magnitude ||a × b|| = ||a|| ||b|| sin θ relates to the area of the parallelogram formed by a and b. While cross products are less common in ML than dot products, they appear in computer graphics (which relates to vision tasks), physics simulations, and understanding rotations in 3D space—relevant for robotics and 3D scene understanding. Estimated time: 8 minutes, consider watching twice for full clarity on the geometric meaning. <strong>Resources:</strong> <a href="https://www.3blue1brown.com/topics/linear-algebra" target="_blank" rel="noopener">3Blue1Brown Linear Algebra Series</a>, <a href="https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/x786f2022:vectors-and-matrices/a/cross-products-mvc" target="_blank" rel="noopener">Khan Academy: Cross Product Introduction</a>' },
            { label: 'Complete Khan Academy: cross product intro', estMinutes: 45, details: '<strong>Action:</strong> Work through <a href="https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/x786f2022:vectors-and-matrices" target="_blank" rel="noopener">Khan Academy cross product introduction</a> exercises, focusing on: (1) Computing cross products using the determinant formula, (2) Understanding the geometric interpretation (perpendicular vector with magnitude = area), (3) Practicing the right-hand rule for direction, (4) Recognizing properties like anti-commutativity (a × b = -(b × a)). <strong>Boundaries:</strong> Work through problems systematically. The determinant method for computing cross products involves: a × b = (a₂b₃ - a₃b₂)i - (a₁b₃ - a₃b₁)j + (a₁b₂ - a₂b₁)k. Practice this formula but always connect it back to the geometric meaning. Try both ways: computing by formula and checking with geometric intuition. <strong>Deliverable:</strong> Completed Khan Academy exercises with ability to compute cross products accurately and interpret results geometrically. Mastery of the right-hand rule for determining direction. <strong>Verification:</strong> Can you compute a × b given numerical vectors? Can you verify that the result is perpendicular to both inputs (by checking dot products equal zero)? Can you predict whether ||a × b|| will be large or small based on the angle between vectors? Common pitfall: getting lost in the formula without checking that your result makes geometric sense (perpendicularity and magnitude). Success check: Given a=[1,0,0] and b=[0,1,0], compute a × b and verify it equals [0,0,1] (pointing along z-axis by right-hand rule), and check it\'s perpendicular to both a and b. Understanding cross products helps with normal vectors in graphics and understanding orientation in 3D spaces. Estimated time: 45 minutes including all practice problems and verification checks. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/multivariable-calculus" target="_blank" rel="noopener">Khan Academy: Multivariable Calculus</a>, <a href="https://tutorial.math.lamar.edu/Classes/CalcII/CrossProduct.aspx" target="_blank" rel="noopener">Paul\'s Online Math: Cross Product</a>' },
            { label: 'Create notebooks/foundations/day09_cross_product.ipynb with 3D examples', estMinutes: 90, details: '<strong>Action:</strong> Create a comprehensive Jupyter notebook (notebooks/foundations/day09_cross_product.ipynb) demonstrating cross products in 3D with: (1) Computing cross products using NumPy\'s np.cross(), (2) Visualizing input vectors and their cross product in 3D using matplotlib\'s mplot3d, (3) Verifying perpendicularity by computing dot products, (4) Demonstrating the right-hand rule with labeled examples, (5) Showing how cross product magnitude relates to parallelogram area, (6) Exploring anti-commutativity (a × b vs b × a). <strong>Boundaries:</strong> Create at least 4 distinct 3D visualizations showing different vector pairs and their cross products. Use different colors for input vectors (blue and red) and the result (green). Include axes labels and grid for spatial context. Add interactive 3D rotation if using plotly, or multiple viewing angles if using matplotlib. Verify perpendicularity numerically for each example. <strong>Deliverable:</strong> A tutorial-style notebook with clear 3D visualizations demonstrating cross products. Should include both computation and geometric verification. Visualizations should clearly show the perpendicular relationship and use the right-hand rule. <strong>Verification:</strong> Notebook runs without errors. All 3D plots render correctly and are readable. Code computes cross products and verifies properties (perpendicularity, magnitude, anti-commutativity). Markdown cells explain each concept. Someone learning cross products should gain geometric intuition from your visualizations. Common pitfall: poor 3D visualizations that don\'t clearly show the spatial relationships—use good camera angles and clear colors. Success check: Your notebook should demonstrate that a × b points in the direction your thumb points when fingers curl from a to b (right-hand rule), and that ||a × b|| equals the area of the parallelogram formed by a and b. Can you use your notebook to explain cross products to someone visually? Estimated time: 90 minutes for implementation, visualization, and documentation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/generated/numpy.cross.html" target="_blank" rel="noopener">NumPy cross function</a>, <a href="https://matplotlib.org/stable/gallery/mplot3d/quiver3d.html" target="_blank" rel="noopener">Matplotlib 3D quiver plots</a>, <a href="https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html" target="_blank" rel="noopener">Python Data Science Handbook: 3D Plotting</a>' },
            { label: 'Generate artifacts/day09_cross_3d.png showing perpendicular result', estMinutes: 30, details: '<strong>Action:</strong> Create a publication-quality 3D visualization (artifacts/day09_cross_3d.png) showing a clear example of cross product with vectors a and b (in blue and red) and their cross product a × b (in green), all drawn from the origin. Choose vectors that make the perpendicularity obvious, like a=[2,0,0], b=[0,3,0], giving a × b=[0,0,6]. Include: (1) Clear axis labels (x,y,z), (2) Grid for spatial reference, (3) Arrows with proper scaling, (4) Labels on each vector, (5) A title explaining what\'s shown. Optionally show the parallelogram formed by a and b to illustrate the area relationship. <strong>Boundaries:</strong> Choose a camera angle that clearly shows the 3D relationships—avoid angles where vectors appear to overlap. Save at 300 DPI. Use professional colors with good contrast against a white or light background. Make sure the perpendicular relationship is visually obvious. Consider adding text annotations explaining "a × b is perpendicular to both a and b" and "||a × b|| = area of parallelogram." <strong>Deliverable:</strong> A clear, self-contained PNG image that effectively demonstrates cross product geometry in 3D. Should be suitable for presentations or educational materials. <strong>Verification:</strong> Image is high resolution (300+ DPI). The three vectors are clearly visible and properly labeled. The perpendicular relationship is visually apparent. Axes and grid provide spatial context. Could this image teach cross products to someone unfamiliar with them? Common pitfall: poor viewing angle making the 3D structure unclear, or cluttered visualization. Success check: The green result vector should clearly appear perpendicular to the blue and red input vectors. Someone viewing this should immediately grasp that cross products produce perpendicular vectors. While less common in ML than dot products, understanding 3D geometry is valuable for computer vision and robotics applications. Estimated time: 30 minutes including angle selection and quality checks. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/gallery/mplot3d/view_planes_3d.html" target="_blank" rel="noopener">Matplotlib 3D View Angles</a>, <a href="https://matplotlib.org/stable/tutorials/colors/colors.html" target="_blank" rel="noopener">Matplotlib Colors</a>' },
            { label: 'Write docs/notes/day09_cross.md on right-hand rule', estMinutes: 25, details: '<strong>Action:</strong> Write a focused markdown document (docs/notes/day09_cross.md) explaining the right-hand rule for determining cross product direction. Cover: (1) What the right-hand rule is: point fingers along first vector, curl toward second vector, thumb points in result direction, (2) Why we need it: the cross product direction isn\'t obvious from the formula alone, (3) How to apply it consistently: always go from first vector to second vector in order, (4) The anti-commutativity property: a × b = -(b × a) because swapping order reverses direction, (5) Practical examples with standard basis vectors: i × j = k, j × k = i, k × i = j. <strong>Boundaries:</strong> Aim for 400-500 words. Make it practical and visual—describe how to physically use your hand. Include ASCII art or describe a simple diagram showing hand position. Emphasize that the rule is a convention (we could have chosen left-hand rule) but right-hand rule is the standard. Explain that this matters for ensuring consistent orientation in calculations. <strong>Deliverable:</strong> A clear, practical guide to using the right-hand rule that someone could follow to determine cross product directions. Should demystify this geometric rule with concrete instructions. <strong>Verification:</strong> Document gives step-by-step instructions for applying the right-hand rule. Includes examples with basis vectors. Explains why the rule matters (consistency in direction). No errors in the examples. Could someone read this and correctly apply the right-hand rule? Common pitfall: being too abstract without giving concrete, physical instructions. Success check: After reading, someone should be able to use their actual right hand to determine that i × j = k (not -k). Practice this yourself: point fingers along x-axis, curl toward y-axis, thumb points up along z-axis. Understanding orientation conventions is important in 3D graphics and computer vision. Estimated time: 25 minutes for clear, practical writing. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Right-hand_rule" target="_blank" rel="noopener">Wikipedia: Right-hand Rule</a>, <a href="https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/x786f2022:vectors-and-matrices/a/cross-products-mvc" target="_blank" rel="noopener">Khan Academy: Cross Products</a>' }
          ],
          reflectionPrompt: 'Cross product is less common in ML—when might it appear?'
        },
        {
          globalDay: 10,
          week: 2,
          title: 'Change of Basis',
          priority: 'HIGH',
          tasks: [
            { label: 'Watch 3Blue1Brown Ep. 9 "Change of basis"', estMinutes: 12, resourceLinks: ['https://www.youtube.com/watch?v=P2LTAUO1TdA'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=P2LTAUO1TdA" target="_blank" rel="noopener">3Blue1Brown Episode 9: Change of basis</a> (12 minutes), focusing on understanding that the same vector can be represented differently depending on which basis (coordinate system) you use. The key insight: coordinates are instructions for how to scale and combine basis vectors to reach a point. <strong>Boundaries:</strong> Pay attention to the change-of-basis matrix construction: columns are where the new basis vectors land in the old coordinate system (or vice versa, depending on direction). Understand that changing basis is like translating between different "languages" for describing the same geometric object. Pause when Grant shows how to convert coordinates between bases—this involves matrix multiplication with the change-of-basis matrix. <strong>Deliverable:</strong> Clear mental model that vector coordinates depend on basis choice, and understanding how to construct and use change-of-basis matrices to translate between coordinate systems. <strong>Verification:</strong> Can you explain why the standard basis vectors i=[1,0] and j=[0,1] are just one possible choice? Why might we want to use a different basis? Can you describe what a change-of-basis matrix does geometrically? Common pitfall: thinking coordinates are absolute rather than relative to a chosen basis. Success check: Understand that if Jennifer uses different basis vectors than you, she describes the same vector with different coordinates, and you can translate between your descriptions using a change-of-basis matrix. This concept is fundamental for eigendecomposition (finding the "right" basis where transformations become simple) and PCA (finding the basis that best captures variance). Estimated time: 12 minutes, may want to watch twice as this is conceptually rich. <strong>Resources:</strong> <a href="https://www.3blue1brown.com/topics/linear-algebra" target="_blank" rel="noopener">3Blue1Brown Linear Algebra Series</a>, <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases" target="_blank" rel="noopener">Khan Academy: Change of Basis</a>' },
            { label: 'Complete Khan Academy: change of basis exercises', estMinutes: 60, details: '<strong>Action:</strong> Work through <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases" target="_blank" rel="noopener">Khan Academy change of basis exercises</a>, practicing: (1) Expressing vectors in different bases, (2) Constructing change-of-basis matrices, (3) Converting coordinates from one basis to another using matrix multiplication, (4) Understanding the inverse relationship (basis A to B, then B back to A). <strong>Boundaries:</strong> Start with 2D examples to build intuition before moving to higher dimensions. For each problem, verify your answer makes sense: converting to a new basis and back should give the original coordinates. Practice both directions: expressing a vector in a new basis, and converting basis-specific coordinates back to standard form. Don\'t just memorize the formula—understand what you\'re computing. <strong>Deliverable:</strong> Mastery of change-of-basis computations with clear understanding of what the mathematics represents geometrically. Ability to construct change-of-basis matrices and use them correctly. <strong>Verification:</strong> Can you take a vector in standard basis, express it in a new basis, then convert back and get the original? Can you construct the change-of-basis matrix given two sets of basis vectors? Can you explain why we need inverse matrices for certain basis conversions? Common pitfall: confusion about which direction the transformation goes (basis A to B vs B to A) and which matrix to use. Success check: Given basis vectors b₁=[2,1] and b₂=[1,2], express standard vector v=[5,4] in the new basis, then convert back to verify. Understanding basis changes is crucial because many ML algorithms (PCA, ICA, eigendecomposition) are fundamentally about finding the "right" basis for a problem. Estimated time: 60 minutes for thorough practice and understanding. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases" target="_blank" rel="noopener">Khan Academy: Alternate Bases</a>, <a href="https://tutorial.math.lamar.edu/Classes/LinAlg/ChangeOfBasis.aspx" target="_blank" rel="noopener">Paul\'s Online Math: Change of Basis</a>' },
            { label: 'Create notebooks/foundations/day10_basis_change.ipynb', estMinutes: 90, details: '<strong>Action:</strong> Create a comprehensive Jupyter notebook (notebooks/foundations/day10_basis_change.ipynb) demonstrating change of basis with: (1) Defining multiple basis sets (standard basis and custom bases), (2) Constructing change-of-basis matrices using NumPy, (3) Converting vectors between bases using matrix multiplication, (4) Visualizing the same vector in different coordinate systems, (5) Verifying round-trip conversions (change basis and change back), (6) Showing how transformations look simpler in certain bases (preview of eigendecomposition). <strong>Boundaries:</strong> Create clear visualizations showing: the same geometric vector with coordinates labeled in different bases, grid lines showing each basis system, and how the change-of-basis matrix acts. Use at least 3 different basis choices to demonstrate the concept thoroughly. Include both 2D visualizations (clear and intuitive) and code that works in any dimension. Verify all conversions by checking inverse operations. <strong>Deliverable:</strong> A tutorial-style notebook that clearly demonstrates change of basis through both computation and visualization. Should include at least 3 complete examples with different basis choices and visual representations showing the same vector in different coordinate systems. <strong>Verification:</strong> Notebook runs without errors. Visualizations clearly show the same vector represented in multiple coordinate systems. Code correctly constructs change-of-basis matrices and performs conversions. Verification checks confirm round-trip accuracy. Markdown explanations connect computations to geometric meaning. Common pitfall: treating change of basis as pure algebra without showing the geometric picture—your visualizations should make it clear that the vector hasn\'t changed, only its coordinate representation. Success check: Your notebook should demonstrate that choosing a basis is like choosing units (meters vs feet): the physical reality doesn\'t change, only the numbers we use to describe it. Can someone learn change of basis from your notebook? Estimated time: 90 minutes for thorough implementation and visualization. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/routines.linalg.html" target="_blank" rel="noopener">NumPy Linear Algebra</a>, <a href="https://matplotlib.org/stable/tutorials/introductory/pyplot.html" target="_blank" rel="noopener">Matplotlib PyPlot Tutorial</a>, <a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.00-introduction-to-numpy.html" target="_blank" rel="noopener">Python Data Science Handbook: NumPy</a>' },
            { label: 'Implement basis transformation matrices', estMinutes: 60, details: '<strong>Action:</strong> Implement functions to work with basis transformations: (1) <code>construct_change_of_basis_matrix(old_basis, new_basis)</code> that creates the transformation matrix, (2) <code>change_to_basis(vector, basis)</code> that converts a vector from standard basis to the specified basis, (3) <code>change_from_basis(coords, basis)</code> that converts coordinates in a custom basis back to standard basis, (4) <code>verify_roundtrip(vector, basis)</code> that checks conversions work correctly in both directions. Include comprehensive docstrings with examples and type hints. <strong>Boundaries:</strong> Implement from mathematical principles, not just calling high-level functions. The change-of-basis matrix is constructed by taking the basis vectors as columns (or rows, depending on convention—document your choice clearly). Include error checking for invalid inputs (non-invertible basis matrices). Write unit tests for each function with multiple test cases including edge cases. <strong>Deliverable:</strong> A clean, well-tested implementation of basis transformation utilities. Code should be reusable and well-documented. Include at least 5 test cases per function verifying correct behavior. <strong>Verification:</strong> All functions run without errors. Unit tests pass. Functions handle edge cases appropriately. Docstrings clearly explain inputs, outputs, and usage. Code follows Python best practices (PEP 8). Can these functions be used reliably in future projects? Common pitfall: inconsistent conventions about row vs column vectors or inverse matrix usage. Success check: Your <code>verify_roundtrip</code> function should confirm that changing to a basis and back returns the original vector (within numerical precision). Test with standard basis (should be identity operations), orthogonal bases, and non-orthogonal bases. Understanding how to implement these operations reinforces the mathematical concepts and provides tools for future work with PCA and eigenvectors. Estimated time: 60 minutes for implementation, documentation, and testing. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html" target="_blank" rel="noopener">NumPy matrix inverse</a>, <a href="https://docs.python.org/3/library/typing.html" target="_blank" rel="noopener">Python Type Hints</a>, <a href="https://realpython.com/documenting-python-code/" target="_blank" rel="noopener">Python Documentation Guide</a>' },
            { label: 'Write docs/notes/day10_basis.md', estMinutes: 30, details: '<strong>Action:</strong> Write a comprehensive markdown document (docs/notes/day10_basis.md) explaining change of basis conceptually. Cover: (1) What a basis is: a set of linearly independent vectors that span the space, (2) Why coordinates depend on basis choice: the same geometric vector has different numerical representations in different bases, (3) The analogy to language translation: same meaning, different words, (4) How to construct change-of-basis matrices, (5) Why this matters for ML: PCA finds the basis where data variance is maximized, eigendecomposition finds the basis where transformations become simple (diagonal matrices), (6) Connection to upcoming topics: eigenvectors are special basis vectors that don\'t change direction under transformation. <strong>Boundaries:</strong> Aim for 500-700 words. Use concrete examples, perhaps the standard basis vs a rotated basis for 2D vectors. Explain intuitively—coordinates are just "instructions" for how to combine basis vectors, so different bases give different instructions for reaching the same point. Avoid heavy notation; focus on conceptual understanding. <strong>Deliverable:</strong> A clear, conceptual document that explains change of basis and previews why it matters for upcoming topics (eigendecomposition, PCA). Should help someone understand this is not just an abstract mathematical concept but a practical tool. <strong>Verification:</strong> Document clearly explains basis concept and coordinates\' dependence on basis choice. Includes concrete examples or analogies. Connects to ML applications. No errors. Could someone read this before tackling eigenvectors and have useful context? Common pitfall: being too mathematical without building intuition—emphasize that this is about different perspectives on the same geometric reality. Success check: Can you explain to someone why PCA "rotates" your data into a new basis where the axes align with maximum variance directions? Your document should provide the foundation for understanding this. The insight that we can choose convenient bases is powerful: in the "right" basis, many problems become much simpler. Estimated time: 30 minutes for thoughtful writing with examples. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Change_of_basis" target="_blank" rel="noopener">Wikipedia: Change of Basis</a>, <a href="https://www.youtube.com/watch?v=P2LTAUO1TdA" target="_blank" rel="noopener">3Blue1Brown: Change of Basis</a>, <a href="https://math.stackexchange.com/questions/184863/what-is-the-importance-of-the-change-of-basis-in-linear-algebra" target="_blank" rel="noopener">Math StackExchange: Importance of Change of Basis</a>' }
          ],
          reflectionPrompt: 'Why is change of basis important for understanding eigenvalues?'
        },
        {
          globalDay: 11,
          week: 2,
          title: 'Eigenvalues & Eigenvectors Introduction',
          priority: 'HIGH',
          tasks: [
            { label: 'Watch 3Blue1Brown Ep. 10 "Eigenvectors and eigenvalues"', estMinutes: 17, resourceLinks: ['https://www.youtube.com/watch?v=PFDu9oVAE-g'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=PFDu9oVAE-g" target="_blank" rel="noopener">3Blue1Brown Episode 10: Eigenvectors and eigenvalues</a> (17 minutes), focusing on the key insight: eigenvectors are special vectors that don\'t change direction when a transformation is applied—they only get scaled. The scaling factor is the eigenvalue. <strong>Boundaries:</strong> Pay close attention to the geometric interpretation: most vectors get knocked off their span during a transformation, but eigenvectors "stay on their span" (they might flip or stretch, but keep the same direction). Watch the examples of rotations (no real eigenvectors in 2D since all vectors change direction) vs. stretching transformations (clear eigenvector directions). Pause when Grant shows the characteristic polynomial—this is how we find eigenvalues algebraically. Understanding this visually is crucial before diving into computations. <strong>Deliverable:</strong> Clear geometric intuition that eigenvectors are the "axis" directions that a transformation preserves, and eigenvalues tell you the scaling along those axes. <strong>Verification:</strong> Can you explain why a 90-degree rotation in 2D has no real eigenvectors? Why a pure scaling transformation has eigenvectors along all directions? Can you predict qualitatively where eigenvectors might be for a given transformation by looking at its geometric effect? Common pitfall: treating eigenvectors as just solutions to an equation without understanding the geometric meaning. Success check: Understand that eigenvectors reveal the "natural axes" of a transformation—the directions it treats simply. This is foundational for PCA (finding axes of maximum variance), understanding neural network dynamics, and diagonalization. Estimated time: 17 minutes, this is conceptually rich so consider watching twice. <strong>Resources:</strong> <a href="https://www.3blue1brown.com/topics/linear-algebra" target="_blank" rel="noopener">3Blue1Brown Linear Algebra Series</a>, <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything" target="_blank" rel="noopener">Khan Academy: Eigenvectors and Eigenvalues</a>' },
            { label: 'Complete Khan Academy: eigenvectors intro', estMinutes: 60, details: '<strong>Action:</strong> Work through <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything" target="_blank" rel="noopener">Khan Academy eigenvectors and eigenvalues introduction</a>, practicing: (1) Finding eigenvalues by solving det(A - λI) = 0, (2) Finding eigenvectors by solving (A - λI)v = 0 for each eigenvalue, (3) Verifying that Av = λv holds for your solutions, (4) Understanding geometric interpretation of what eigenvalues and eigenvectors mean. <strong>Boundaries:</strong> Start with 2×2 matrices to build computational fluency before attempting larger matrices. For each problem: compute eigenvalues first, then find corresponding eigenvectors, then verify by checking Av = λv. Understand that eigenvalues can be complex (for rotations) or repeated. Practice until the process feels systematic. Don\'t skip verification—it catches errors and reinforces understanding. <strong>Deliverable:</strong> Mastery of the computational process for finding eigenvalues and eigenvectors. Ability to solve characteristic equations and null space problems accurately. <strong>Verification:</strong> Can you find eigenvalues and eigenvectors for a 2×2 matrix by hand in under 5 minutes? Can you verify your answers by computing Av and checking it equals λv? Can you explain what it means geometrically when you find an eigenvalue λ=2 with eigenvector v=[1,1]? Common pitfall: making algebraic errors in the characteristic polynomial or null space solving—careful arithmetic is essential. Success check: Given matrix A=[[3,1],[0,2]], find eigenvalues (should get λ=3 and λ=2) and corresponding eigenvectors, then verify. Understanding this computation is essential because eigenvectors appear throughout ML: in PCA, spectral clustering, PageRank, and understanding optimization landscapes. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything" target="_blank" rel="noopener">Khan Academy: Eigeneverything</a>, <a href="https://tutorial.math.lamar.edu/Classes/DE/LA_Eigen.aspx" target="_blank" rel="noopener">Paul\'s Online Math: Eigenvalues and Eigenvectors</a>' },
            { label: 'Create notebooks/foundations/day11_eigenvalues_intro.ipynb', estMinutes: 90, details: '<strong>Action:</strong> Create a comprehensive Jupyter notebook (notebooks/foundations/day11_eigenvalues_intro.ipynb) demonstrating eigenvalues and eigenvectors with: (1) Computing eigenvalues and eigenvectors using NumPy\'s np.linalg.eig(), (2) Visualizing how transformations act on eigenvectors vs. regular vectors, (3) Verifying the eigenvalue equation Av = λv numerically, (4) Showing examples of different matrix types (stretching, rotation, shear) and their eigenvectors, (5) Exploring geometric interpretation with animated or multi-frame visualizations. <strong>Boundaries:</strong> Create clear 2D visualizations showing: original vectors, the transformed vectors, and highlighting that eigenvectors maintain direction (only scale). Use color coding: regular vectors that change direction in one color, eigenvectors that stay on their span in another color. Include at least 4 examples: (1) diagonal matrix (eigenvectors along axes), (2) symmetric matrix (orthogonal eigenvectors), (3) rotation matrix (complex eigenvalues, or show why there are none in 2D reals), (4) general matrix. Add code comments explaining each step. <strong>Deliverable:</strong> A tutorial-style notebook with clear visualizations demonstrating eigenvalue/eigenvector concepts geometrically and computationally. Should include verification code and multiple examples with different matrix types. <strong>Verification:</strong> Notebook runs without errors. Visualizations clearly show which vectors are eigenvectors (preserved direction). NumPy computations match hand calculations for simple examples. Verification code confirms Av = λv for all computed eigenpairs. Markdown explanations connect computation to geometry. Common pitfall: showing only the computation without geometric visualization—the visual insight is crucial. Success check: Your notebook should make it visually obvious why eigenvectors are special and what eigenvalues represent. Can someone learn eigenvalue intuition from your visualizations? Estimated time: 90 minutes for implementation, visualization, and documentation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html" target="_blank" rel="noopener">NumPy eig function</a>, <a href="https://matplotlib.org/stable/gallery/images_contours_and_fields/quiver_demo.html" target="_blank" rel="noopener">Matplotlib quiver demo</a>, <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html" target="_blank" rel="noopener">Python Data Science Handbook: PCA</a>' },
            { label: 'Find eigenvalues for 2×2 matrices by hand', estMinutes: 45, details: '<strong>Action:</strong> Practice finding eigenvalues and eigenvectors for 2×2 matrices entirely by hand (no calculator or NumPy) to build computational fluency. Work through at least 5 different matrices: (1) A diagonal matrix (easy: eigenvalues on diagonal), (2) A symmetric matrix, (3) An upper triangular matrix, (4) A general 2×2 matrix, (5) A matrix with repeated eigenvalues. For each: compute det(A - λI) = 0, solve for λ, then solve (A - λI)v = 0 for eigenvectors, verify Av = λv. <strong>Boundaries:</strong> Show all work step-by-step. For 2×2 matrices, the characteristic polynomial is quadratic: det([[a-λ, b],[c, d-λ]]) = (a-λ)(d-λ) - bc = λ² - (a+d)λ + (ad-bc). Use the quadratic formula to find eigenvalues. For eigenvectors, substitute each λ back and solve the resulting system. Check your work by computing Av and verifying it equals λv. <strong>Deliverable:</strong> Complete hand-written solutions for 5 different 2×2 matrices, showing all steps: characteristic polynomial, eigenvalue computation, eigenvector finding, and verification. <strong>Verification:</strong> All verifications should confirm Av = λv. Your arithmetic should be accurate. Can you complete a 2×2 eigenvalue/eigenvector problem in 5-8 minutes? Do you understand each step\'s purpose? Common pitfall: arithmetic errors in expanding determinants or solving systems—work carefully and check each step. Success check: You should be able to find eigenvalues and eigenvectors for matrix [[1,2],[2,1]] and get λ₁=3 with v₁∝[1,1], λ₂=-1 with v₂∝[1,-1]. Building computational fluency by hand ensures you understand what NumPy is doing and helps you catch errors in code. This skill is valuable for homework, exams, and debugging eigenvalue code. Estimated time: 45 minutes for 5 complete problems with verification. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors" target="_blank" rel="noopener">Khan Academy: Eigenvalues Introduction</a>, <a href="https://tutorial.math.lamar.edu/Classes/DE/LA_Eigen.aspx" target="_blank" rel="noopener">Paul\'s Online Math: Eigenvalues</a>' },
            { label: 'Write docs/notes/day11_eigen.md explaining geometric intuition', estMinutes: 30, details: '<strong>Action:</strong> Write a comprehensive markdown document (docs/notes/day11_eigen.md) explaining eigenvalues and eigenvectors with emphasis on geometric intuition. Cover: (1) The definition: Av = λv means vector v only gets scaled (not rotated) by transformation A, (2) Geometric meaning: eigenvectors are the "axis" directions that A treats simply, (3) Why they\'re called "eigen" (German for "own" or "characteristic")—they characterize the transformation, (4) Examples: stretching along axes has axis-aligned eigenvectors, rotation has no real eigenvectors (all vectors change direction), (5) Why they matter for ML: PCA finds eigenvectors of covariance matrix (directions of maximum variance), graph algorithms use eigenvectors of adjacency matrices, understanding neural network dynamics involves eigenvalues of weight matrices. <strong>Boundaries:</strong> Aim for 600-800 words. Focus on understanding over formulas. Use analogies: eigenvectors are like "preferred directions" of a transformation—directions that don\'t get "mixed" with other directions. Include concrete 2D examples you can visualize. Explain why finding eigenvectors is like finding the "natural coordinate system" for a transformation. <strong>Deliverable:</strong> A clear, intuitive explanation of eigenvalues/eigenvectors that builds geometric understanding and previews ML applications. Should help someone see why this abstraction is powerful and worth mastering. <strong>Verification:</strong> Document clearly explains the geometric meaning, not just the algebra. Includes concrete examples. Connects to ML applications. No errors. Could someone read this and understand why eigenvectors matter beyond being homework problems? Common pitfall: being too abstract or algebraic without building intuition. Success check: Can you explain why PCA\'s principal components are eigenvectors of the covariance matrix? Your document should provide the conceptual foundation for understanding this. Eigenvectors reveal structure: they show us the "natural" directions in our data or the "fundamental modes" of a system. Estimated time: 30 minutes for thoughtful, intuitive writing. <strong>Resources:</strong> <a href="https://www.youtube.com/watch?v=PFDu9oVAE-g" target="_blank" rel="noopener">3Blue1Brown: Eigenvectors</a>, <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Geometric_meaning" target="_blank" rel="noopener">Wikipedia: Eigenvalues Geometric Meaning</a>, <a href="https://math.stackexchange.com/questions/243533/what-is-the-importance-of-eigenvalues-eigenvectors" target="_blank" rel="noopener">Math StackExchange: Importance of Eigenvalues</a>' }
          ],
          reflectionPrompt: 'What does it mean for a vector to "stay on its span" after transformation?'
        },
        {
          globalDay: 12,
          week: 2,
          title: 'Eigendecomposition & Diagonalization',
          priority: 'HIGH',
          tasks: [
            { label: 'Complete Khan Academy: eigendecomposition and diagonalization', estMinutes: 75, details: '<strong>Action:</strong> Work through <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything" target="_blank" rel="noopener">Khan Academy eigendecomposition and diagonalization</a> exercises, learning how to decompose a matrix A into A = PDP⁻¹ where P contains eigenvectors as columns and D is a diagonal matrix of eigenvalues. <strong>Boundaries:</strong> Understand the key insight: if we have enough linearly independent eigenvectors, we can decompose any square matrix into this form. Practice constructing P (eigenvectors as columns) and D (eigenvalues on diagonal), then verifying A = PDP⁻¹. Understand that not all matrices are diagonalizable (need n independent eigenvectors for n×n matrix). Work through problems systematically: find eigenvalues, find eigenvectors, construct P and D, verify the decomposition. <strong>Deliverable:</strong> Mastery of eigendecomposition process with ability to construct and verify A = PDP⁻¹ decompositions. Understanding of when diagonalization is possible and what it means geometrically. <strong>Verification:</strong> Can you take a matrix A, find its eigendecomposition, and verify by computing PDP⁻¹ that you get A back? Can you explain geometrically what eigendecomposition means (change to eigenvector basis, apply diagonal scaling, change back)? Common pitfall: not verifying your work—always compute PDP⁻¹ to check. Success check: Eigendecompose matrix A=[[3,1],[0,2]], get P (eigenvectors) and D (diagonal with 3 and 2), verify PDP⁻¹=A. Understanding eigendecomposition is crucial: it\'s how PCA works (eigendecompose covariance matrix), how we efficiently compute matrix powers (A^n = PD^nP⁻¹), and how we understand dynamical systems. This is one of the most important concepts in applied linear algebra. Estimated time: 75 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything" target="_blank" rel="noopener">Khan Academy: Eigendecomposition</a>, <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" target="_blank" rel="noopener">Wikipedia: Eigendecomposition</a>' },
            { label: 'Create notebooks/foundations/day12_eigendecomp.ipynb', estMinutes: 120, details: '<strong>Action:</strong> Create a comprehensive Jupyter notebook (notebooks/foundations/day12_eigendecomp.ipynb) demonstrating eigendecomposition in depth: (1) Computing eigendecomposition using NumPy (np.linalg.eig), (2) Manually constructing P and D matrices from eigenpairs, (3) Verifying A = PDP⁻¹ numerically, (4) Visualizing what eigendecomposition means geometrically (change of basis to eigenvector coordinates where transformation becomes simple diagonal scaling), (5) Computing matrix powers efficiently using eigendecomposition (A^n = PD^nP⁻¹), (6) Exploring examples: symmetric matrices (orthogonal eigenvectors), defective matrices (can\'t be diagonalized), applications to Markov chains or difference equations. <strong>Boundaries:</strong> This is a substantial notebook—spend time making it thorough and clear. Include: detailed code comments, markdown explanations connecting algebra to geometry, visualizations showing how a transformation looks in standard basis vs eigenvector basis, numerical verification of all claims, at least 4 different matrix examples with different properties. Show both the computational process and the geometric interpretation. Demonstrate the power of eigendecomposition for computing A^100 efficiently. <strong>Deliverable:</strong> A comprehensive, tutorial-quality notebook that could serve as a definitive reference for eigendecomposition. Should include theory, computation, visualization, and practical applications. <strong>Verification:</strong> Notebook runs without errors (may take a minute due to extensive computations). All eigendecompositions are verified by reconstructing A from PDP⁻¹. Visualizations clearly show the geometric meaning. Code is well-documented. Someone could learn eigendecomposition thoroughly from this notebook. Common pitfall: treating eigendecomposition as pure mechanics without showing its geometric meaning and practical power. Success check: Your notebook should demonstrate both HOW to eigendecompose (computational steps) and WHY it\'s useful (matrix powers, understanding transformations, simplifying problems). Can you explain eigendecomposition from your notebook to someone else? Estimated time: 120 minutes for thorough, high-quality implementation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html" target="_blank" rel="noopener">NumPy eig</a>, <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_power.html" target="_blank" rel="noopener">NumPy matrix_power</a>, <a href="https://matplotlib.org/stable/gallery/index.html" target="_blank" rel="noopener">Matplotlib Gallery</a>' },
            { label: 'Implement eigendecomposition verification (A = PDP^-1)', estMinutes: 60, details: '<strong>Action:</strong> Implement a complete eigendecomposition verification system: (1) <code>eigendecompose(A)</code> that returns P (eigenvector matrix) and D (diagonal eigenvalue matrix), (2) <code>verify_eigendecomp(A, P, D)</code> that checks if A = PDP⁻¹ holds within numerical tolerance, (3) <code>compute_matrix_power(A, n)</code> that uses eigendecomposition to efficiently compute A^n, (4) <code>is_diagonalizable(A)</code> that checks if A has enough independent eigenvectors. Include comprehensive error handling, type hints, docstrings with examples, and unit tests. <strong>Boundaries:</strong> Build from NumPy\'s eig function but add verification layers. Handle edge cases: complex eigenvalues, defective matrices, numerical precision issues. For matrix power computation, show the speedup vs naive multiplication for large n. Your verification function should check np.allclose(A, P @ D @ np.linalg.inv(P)) accounting for floating-point precision. Document the mathematical background in docstrings. <strong>Deliverable:</strong> Production-quality eigendecomposition utilities with thorough testing and documentation. Code should be reusable for future projects. <strong>Verification:</strong> All functions run correctly. Unit tests pass for multiple test cases including edge cases. Verification function correctly identifies valid eigendecompositions. Matrix power function produces correct results faster than naive methods for large powers. Code follows best practices. Common pitfall: not handling numerical precision issues (eigenvalues/eigenvectors computed numerically have small errors). Success check: Your code should correctly decompose matrix [[3,1],[0,2]], verify the decomposition, and efficiently compute its 100th power. Test with symmetric matrices (should have real eigenvalues and orthogonal eigenvectors) and see that verification passes. These utilities will be valuable when you implement PCA from scratch or work with graph algorithms. Estimated time: 60 minutes for implementation, testing, and documentation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/routines.linalg.html" target="_blank" rel="noopener">NumPy Linear Algebra</a>, <a href="https://docs.python.org/3/library/typing.html" target="_blank" rel="noopener">Python Type Hints</a>, <a href="https://realpython.com/python-testing/" target="_blank" rel="noopener">Python Testing Guide</a>' },
            { label: 'Test matrix powers using diagonalization', estMinutes: 45, details: '<strong>Action:</strong> Demonstrate the power of eigendecomposition for computing matrix powers by comparing: (1) Naive method: multiply A by itself n times, (2) Eigendecomposition method: compute A^n = PD^nP⁻¹ where D^n is trivial (raise diagonal elements to power n). Test with different matrices and different values of n (10, 100, 1000) to see the computational benefit. Time both approaches. Explore applications: Fibonacci numbers via matrix powers, Markov chain convergence (what happens as n→∞), difference equations, population dynamics models. <strong>Boundaries:</strong> Choose several test matrices: (1) a 2×2 matrix (fast enough to see large powers), (2) the Fibonacci matrix [[1,1],[1,0]], (3) a Markov transition matrix (rows sum to 1), (4) a symmetric matrix. For each, compute various powers using both methods, time them, and verify they give the same results. Show that D^n is instant (just raise diagonal elements) while repeated multiplication is slow. Explore what A^∞ means (steady-state behavior). <strong>Deliverable:</strong> Comprehensive comparison showing eigendecomposition makes matrix powers tractable. Concrete applications demonstrating why this matters beyond mathematical curiosity. Timing data showing the computational advantage. <strong>Verification:</strong> Both methods produce same results (within numerical precision). Eigendecomposition method is faster for large n. Applications (Fibonacci, Markov chains) work correctly and provide insight. Code is clear and well-commented. Common pitfall: not exploring real applications—matrix powers appear in many practical contexts. Success check: Show that computing the 100th power of the Fibonacci matrix using eigendecomposition gives you Fibonacci(100) instantly, while naive multiplication would take many operations. Demonstrate that a Markov chain\'s long-run behavior is determined by eigenvectors corresponding to eigenvalue 1. Understanding matrix powers via eigendecomposition is crucial for understanding dynamical systems, recurrent neural networks, and graph algorithms. Estimated time: 45 minutes for implementation, testing, and exploration of applications. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Matrix_function#Powers" target="_blank" rel="noopener">Wikipedia: Matrix Powers</a>, <a href="https://math.stackexchange.com/questions/1482866/fibonacci-sequence-using-matrix-exponentiation" target="_blank" rel="noopener">Fibonacci via Matrix Powers</a>' },
            { label: 'Write docs/notes/day12_diagonalization.md', estMinutes: 30, details: '<strong>Action:</strong> Write a comprehensive markdown document (docs/notes/day12_diagonalization.md) explaining diagonalization conceptually. Cover: (1) What A = PDP⁻¹ means: transform to eigenvector basis (P⁻¹), apply simple diagonal scaling (D), transform back (P), (2) Geometric interpretation: in the eigenvector basis, the transformation is just stretching along axes, (3) Why it\'s powerful: makes matrix powers trivial (A^n = PD^nP⁻¹ and D^n is easy), simplifies differential equations, reveals long-term behavior of systems, (4) Connection to PCA: diagonalizing covariance matrix finds axes of maximum variance, (5) When it\'s possible: need n linearly independent eigenvectors for n×n matrix (not all matrices are diagonalizable), (6) Special cases: symmetric matrices are always diagonalizable with orthogonal eigenvectors. <strong>Boundaries:</strong> Aim for 700-900 words. Focus on understanding the power of diagonalization: it\'s about finding the "right" coordinate system where the transformation becomes simple. Use analogies: it\'s like choosing to measure a rectangle\'s dimensions along its sides rather than at an angle—the natural coordinates make everything simpler. Connect to upcoming applications in ML. <strong>Deliverable:</strong> A clear, insightful document that explains why diagonalization matters and previews its applications. Should build intuition about finding natural coordinate systems. <strong>Verification:</strong> Document clearly explains the concept and its importance. Includes geometric interpretation. Connects to ML applications. No errors. Could someone read this and understand why eigendecomposition is a fundamental tool in data science? Common pitfall: explaining the formula without conveying why it\'s useful—emphasize the power of simplification. Success check: Can you explain why PCA works: it finds the eigenvectors of the covariance matrix, which are the directions of maximum variance, and transforms data into these coordinates where dimensions are uncorrelated. Your document should provide the foundation for this understanding. Diagonalization is about finding simplicity: in the right basis, complex transformations become simple. Estimated time: 30 minutes for clear, conceptual writing. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Diagonalizable_matrix" target="_blank" rel="noopener">Wikipedia: Diagonalizable Matrix</a>, <a href="https://www.youtube.com/watch?v=PFDu9oVAE-g" target="_blank" rel="noopener">3Blue1Brown: Eigenvectors</a>, <a href="https://math.stackexchange.com/questions/3053388/intuition-behind-diagonalization" target="_blank" rel="noopener">Math StackExchange: Intuition Behind Diagonalization</a>' }
          ],
          reflectionPrompt: 'Why does diagonalization make matrix powers easy to compute?'
        },
        {
          globalDay: 13,
          week: 2,
          title: 'Abstract Vector Spaces & Basis',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Watch 3Blue1Brown Ep. 11 "Abstract vector spaces"', estMinutes: 16, resourceLinks: ['https://www.youtube.com/watch?v=TgKwz5Ikpc8'], details: '<strong>Action:</strong> Watch <a href="https://www.youtube.com/watch?v=TgKwz5Ikpc8" target="_blank" rel="noopener">3Blue1Brown Episode 11: Abstract vector spaces</a> (16 minutes), discovering that "vectors" aren\'t just arrows—they\'re anything that satisfies certain axioms (addition and scalar multiplication with specific properties). Functions, polynomials, even audio signals can be vectors! <strong>Boundaries:</strong> Pay attention to the axioms that define a vector space: closure under addition and scalar multiplication, associativity, commutativity, existence of zero vector and additive inverses, distributive properties. The key insight: if something satisfies these rules, all linear algebra tools apply—dot products, linear transformations, eigenvalues, everything. Watch Grant explore polynomial space and function space as examples. This abstraction is powerful: it means linear algebra applies far beyond geometric arrows. <strong>Deliverable:</strong> Understanding that "vector space" is an abstract structure defined by axioms, and many mathematical objects qualify as vectors under this definition. <strong>Verification:</strong> Can you explain why polynomials form a vector space? Why functions do? What are the "vectors," "addition," and "scalar multiplication" in these spaces? Can you verify the axioms for a non-standard example? Common pitfall: staying too concrete, thinking vectors must be arrows or lists of numbers. Success check: Understand that a polynomial like 3x² + 2x - 1 is a "vector" in polynomial space, and all linear algebra concepts apply. This abstraction is crucial for ML: feature spaces, function spaces, kernel methods, and reproducing kernel Hilbert spaces all rely on this generalization. Neural networks transform input spaces (vectors) to output spaces (vectors), and the theory applies regardless of what those "vectors" represent. Estimated time: 16 minutes, may want to watch twice to absorb the abstraction. <strong>Resources:</strong> <a href="https://www.3blue1brown.com/topics/linear-algebra" target="_blank" rel="noopener">3Blue1Brown Linear Algebra Series</a>, <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/abstract-vector-spaces/v/vector-space-introduction" target="_blank" rel="noopener">Khan Academy: Abstract Vector Spaces</a>' },
            { label: 'Complete Khan Academy: vector spaces exercises', estMinutes: 60, details: '<strong>Action:</strong> Work through <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/abstract-vector-spaces" target="_blank" rel="noopener">Khan Academy abstract vector spaces</a> exercises, learning to: (1) Verify axioms for different structures (do they form vector spaces?), (2) Identify basis vectors for different spaces (standard polynomial basis: 1, x, x², ...), (3) Understand subspaces (sets within a vector space that are themselves vector spaces), (4) Apply linear algebra concepts to non-geometric contexts. <strong>Boundaries:</strong> Practice with diverse examples: polynomial spaces, function spaces, solution spaces of differential equations, even matrix spaces. For each, identify: What are the "vectors"? What is "addition"? What is "scalar multiplication"? Does it satisfy all axioms? Don\'t rush—this abstraction takes time to internalize. Work through problems verifying axioms systematically. <strong>Deliverable:</strong> Ability to recognize and work with abstract vector spaces. Understanding of subspaces and basis concepts in non-standard contexts. <strong>Verification:</strong> Can you verify that the set of all polynomials of degree ≤2 forms a vector space? Can you identify a basis for this space (e.g., {1, x, x²})? Can you explain why the set of all invertible 2×2 matrices does NOT form a vector space (zero matrix isn\'t invertible)? Common pitfall: mechanically checking axioms without understanding the structure\'s meaning. Success check: Understand that functions form a vector space where "addition" is pointwise addition: (f+g)(x) = f(x) + g(x), and "scalar multiplication" is c·f where (c·f)(x) = c·f(x). Verify this satisfies all vector space axioms. This abstraction enables Fourier analysis (functions as vectors in a space with sine/cosine basis), kernel methods in ML (implicit feature spaces), and functional analysis foundations for neural networks. Estimated time: 60 minutes for conceptual understanding and practice. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/abstract-vector-spaces" target="_blank" rel="noopener">Khan Academy: Vector Spaces</a>, <a href="https://en.wikipedia.org/wiki/Vector_space#Definition" target="_blank" rel="noopener">Wikipedia: Vector Space Definition</a>' },
            { label: 'Create notebooks/foundations/day13_vector_spaces.ipynb', estMinutes: 90, details: '<strong>Action:</strong> Create a comprehensive Jupyter notebook (notebooks/foundations/day13_vector_spaces.ipynb) exploring abstract vector spaces: (1) Demonstrating polynomial space with basis {1, x, x²,...}, (2) Representing polynomials as coefficient vectors, (3) Showing addition and scalar multiplication in polynomial space, (4) Exploring function spaces (e.g., continuous functions on [0,1]), (5) Demonstrating that matrix spaces (all n×n matrices) form vector spaces, (6) Implementing inner products for abstract spaces (e.g., function inner product via integration). <strong>Boundaries:</strong> Make the abstraction concrete through code. For polynomials, use NumPy polynomial class or represent as coefficient arrays. Show that operations in abstract space correspond to familiar operations on representations. Create visualizations: plot polynomial "vectors" and their sums, show function addition, visualize subspaces. Include at least 3 different vector space examples with clear demonstrations that axioms hold. <strong>Deliverable:</strong> A tutorial-style notebook that makes abstract vector spaces concrete through code examples. Should demonstrate multiple non-standard vector spaces and operations within them. <strong>Verification:</strong> Notebook runs without errors. Code correctly implements operations in abstract spaces. Visualizations help build intuition. Examples clearly demonstrate vector space axioms. Markdown explanations connect abstract concepts to concrete implementations. Common pitfall: staying too abstract without concrete examples—use code to make the abstraction tangible. Success check: Your notebook should demonstrate that polynomials p(x) = 2x²+3x+1 and q(x) = x²-x+2 can be added like vectors (coefficient-wise), and show this geometrically by plotting. Someone should understand abstract vector spaces better after working through your examples. Estimated time: 90 minutes for implementation, visualization, and documentation. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/routines.polynomials.polynomial.html" target="_blank" rel="noopener">NumPy Polynomials</a>, <a href="https://scipy.org/" target="_blank" rel="noopener">SciPy</a>, <a href="https://matplotlib.org/stable/tutorials/index.html" target="_blank" rel="noopener">Matplotlib Tutorials</a>' },
            { label: 'Explore polynomial space as vector space example', estMinutes: 45, details: '<strong>Action:</strong> Deep dive into polynomial space as a concrete example of an abstract vector space. Work with the space P₂ of all polynomials of degree ≤2: (1) Identify the "vectors" (polynomials like 3x²+2x-1), (2) Define addition (add coefficients), (3) Define scalar multiplication (scale coefficients), (4) Verify all vector space axioms hold, (5) Identify a basis {1, x, x²}, (6) Represent polynomials as coordinate vectors [a₀, a₁, a₂], (7) Compute an inner product (e.g., integrate product over [0,1]), (8) Find orthogonal polynomials using Gram-Schmidt process. <strong>Boundaries:</strong> Be thorough—this is your concrete example to deeply understand abstract spaces. Verify each axiom explicitly: commutativity of addition, associativity, distributive property, etc. Show that changing from polynomial representation to coefficient vector representation is a coordinate system choice (isomorphism). Demonstrate that all linear algebra tools apply: linear combinations, span, linear independence, change of basis. <strong>Deliverable:</strong> Complete exploration of polynomial space demonstrating it\'s a legitimate vector space where linear algebra fully applies. Should include axiom verification, basis identification, coordinate representations, and inner product calculations. <strong>Verification:</strong> All axioms verified with examples. Basis correctly identified and proven linearly independent. Coordinate representations work correctly. Inner product calculations are accurate. Can you apply concepts like projection and orthogonality in polynomial space? Common pitfall: not actually verifying the axioms, just assuming they hold. Success check: Show that polynomials p(x)=x and q(x)=x² are orthogonal under the inner product ⟨p,q⟩ = ∫₀¹ p(x)q(x)dx (compute and show result is zero). Understanding polynomial spaces is preparation for understanding feature spaces in kernel methods and function approximation in neural networks. Estimated time: 45 minutes for thorough exploration and verification. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Polynomial_space" target="_blank" rel="noopener">Wikipedia: Polynomial Space</a>, <a href="https://mathworld.wolfram.com/OrthogonalPolynomials.html" target="_blank" rel="noopener">Wolfram: Orthogonal Polynomials</a>' },
            { label: 'Write docs/notes/day13_abstract_spaces.md', estMinutes: 25, details: '<strong>Action:</strong> Write a focused markdown document (docs/notes/day13_abstract_spaces.md) explaining abstract vector spaces and why the abstraction matters. Cover: (1) The axioms defining vector spaces (list them clearly), (2) Examples beyond geometric vectors: polynomials, functions, matrices, solution sets, (3) Why this abstraction is powerful: any theorem proven for abstract vector spaces applies to ALL these examples, (4) Connection to ML: feature spaces (data points as vectors in ℝⁿ), kernel methods (implicit high-dimensional spaces), function approximation (neural networks as function space transformations), (5) The insight: linear algebra is about structure, not specific representations. <strong>Boundaries:</strong> Aim for 500-600 words. Make this conceptual and motivational. Explain that the power of abstraction is generality: prove something once for abstract vectors, apply it everywhere. Use accessible language—avoid excessive formalism while maintaining accuracy. <strong>Deliverable:</strong> A clear explanation of abstract vector spaces that builds appreciation for the generalization and previews ML connections. <strong>Verification:</strong> Document clearly explains axioms and provides diverse examples. Motivates the abstraction convincingly. Connects to ML applications. No errors. Would someone understand why "abstract" vector spaces are worth studying? Common pitfall: listing axioms without explaining why this abstraction matters. Success check: Explain that kernel trick in SVMs works by implicitly mapping data into a high-dimensional feature space (abstract vector space) where linear methods become powerful. Your document should help someone understand why this mathematical abstraction enables practical ML techniques. Vector spaces are the fundamental structure of linear algebra; understanding them abstractly unlocks their power. Estimated time: 25 minutes for clear, motivational writing. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener">Wikipedia: Vector Space</a>, <a href="https://www.youtube.com/watch?v=TgKwz5Ikpc8" target="_blank" rel="noopener">3Blue1Brown: Abstract Vector Spaces</a>' }
          ],
          reflectionPrompt: 'How does thinking of functions as vectors help in ML?'
        },
        {
          globalDay: 14,
          week: 2,
          title: 'Week 2 Review & Linear Algebra Consolidation',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review all Week 2 notebooks (eigen focus)', estMinutes: 75, details: '<strong>Action:</strong> Systematically review all Week 2 materials with emphasis on eigenvalue concepts (Days 8-13). Open each notebook, re-run all cells to ensure they still work, review visualizations, read through notes. Focus on: dot products/projections, cross products, change of basis, eigenvalues/eigenvectors, eigendecomposition, abstract vector spaces. <strong>Boundaries:</strong> This is active review—test your understanding by trying to explain concepts without looking at notes first, then verify. Pay special attention to eigenvalue topics since they\'re conceptually dense. For each day, ask: What was the main concept? How does it connect to other topics? Where will I use this in ML? Spend extra time on areas that felt difficult during the week. Re-work key problems to verify fluency. <strong>Deliverable:</strong> Solid understanding of all Week 2 concepts with eigenvalues/eigenvectors as the centerpiece. Clear mental model of how topics interconnect (e.g., eigendecomposition uses change of basis, abstract spaces show where linear algebra applies). <strong>Verification:</strong> Can you explain each major topic from Week 2 without notes? Can you solve representative problems from each day? Do you see the connections between topics? Are there any remaining confusions that need addressing before Week 3? Common pitfall: passive re-reading without active testing of understanding. Success check: Explain the relationship between these Week 2 concepts: eigenvectors are special directions (Day 11), eigendecomposition is changing to eigenvector basis (Day 12, using Day 10\'s change of basis), and this applies beyond arrows because vectors are abstract (Day 13). Can you articulate these connections? Eigenvalues will reappear throughout your ML journey—in PCA, spectral clustering, graph algorithms, optimization analysis, RNN dynamics—so ensure solid understanding now. Estimated time: 75 minutes for thorough review across all days. <strong>Resources:</strong> Your Week 2 notebooks and notes, <a href="https://www.3blue1brown.com/topics/linear-algebra" target="_blank" rel="noopener">3Blue1Brown playlist for quick visual review</a>' },
            { label: 'Solve 10 practice problems on eigenvalues/eigenvectors', estMinutes: 90, details: '<strong>Action:</strong> Complete 10 substantial practice problems focusing on eigenvalues and eigenvectors to build mastery: (1-2) Find eigenvalues and eigenvectors for 2×2 matrices by hand, (3-4) Verify eigendecompositions: given A, P, D, check if A=PDP⁻¹, (5-6) Compute matrix powers using eigendecomposition, (7) Diagonalize a symmetric matrix and verify eigenvectors are orthogonal, (8) Determine if a given matrix is diagonalizable, (9) Find eigenvalues for a 3×3 matrix, (10) Application problem (Markov chain long-run behavior or Fibonacci via matrix powers). <strong>Boundaries:</strong> Choose challenging problems—this is mastery practice, not review. Work independently; only check solutions after completing your attempt. Show all work: characteristic polynomial, solving for eigenvalues, finding eigenvectors, verification steps. For any mistakes, understand exactly where you went wrong and what concept needs reinforcement. Time yourself on some problems to build fluency. <strong>Deliverable:</strong> 10 complete problem solutions demonstrating eigenvalue/eigenvector mastery. Should include a mix of computational, conceptual, and application problems. <strong>Verification:</strong> Aim for 90%+ accuracy. For computational problems, verify answers (check Av=λv, verify decompositions). For conceptual problems, ensure your reasoning is sound. Can you solve eigenvalue problems confidently and relatively quickly? Are any types of problems still difficult? Common pitfall: doing problems mechanically without learning from mistakes. Success check: You should comfortably find eigenvalues and eigenvectors for most 2×2 matrices in 5-10 minutes, and understand when eigendecomposition is possible and useful. Problems should reveal any remaining gaps in understanding—if you struggle with certain types, that indicates areas needing more study. This practice cements computational fluency essential for implementing ML algorithms that use eigenvectors (PCA, spectral methods). Estimated time: 90 minutes for focused problem-solving. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything" target="_blank" rel="noopener">Khan Academy eigenvalue exercises</a>, <a href="https://tutorial.math.lamar.edu/Problems/LA/EigenStuff.aspx" target="_blank" rel="noopener">Paul\'s Online Math: Eigenvalue Problems</a>' },
            { label: 'Write docs/weekly_logs/week_02.md', estMinutes: 45, details: '<strong>Action:</strong> Write a comprehensive weekly log for Week 2 (docs/weekly_logs/week_02.md) reflecting on your learning journey. Include: (1) Main topics covered (dot products, cross products, change of basis, eigenvalues/eigenvectors, eigendecomposition, diagonalization, abstract vector spaces), (2) Key insights and breakthrough moments (what clicked?), (3) Most challenging concepts and how you tackled them, (4) Eigenvalues deep dive—why they\'re important and how you understand them now, (5) Artifacts created (notebooks, visualizations, problem sets), (6) Skills developed (geometric intuition, computational fluency with eigenvalues, abstraction), (7) Connections between topics you discovered, (8) Time management and study approach reflections, (9) Plans for Week 3 (calculus preview). <strong>Boundaries:</strong> Write 700-900 words in first person. Be honest about challenges and successes. Include specific examples: particular problems that were hard, visualizations that helped understanding, moments when concepts connected. This is your learning narrative, not just a checklist. Reflect on meta-learning: what study techniques worked well? What would you adjust? <strong>Deliverable:</strong> A thoughtful, detailed weekly log capturing your complete Week 2 experience. Should read as a story of your learning, with struggles, insights, and growth. <strong>Verification:</strong> Log tells a coherent story of the week. Includes specific examples and genuine reflection. Shows both content learning and metacognitive awareness. Would future you find this log useful for remembering the week? Common pitfall: generic summaries without personal insight or specific examples. Success check: Your log should convey what it was actually like to learn eigenvalues—the initial confusion, the moment they clicked, why they matter. Rereading this in 6 months should transport you back to this week\'s experience and lessons. Reflective journaling strengthens learning by consolidating understanding and identifying patterns in how you learn best. Estimated time: 45 minutes for thoughtful writing. <strong>Resources:</strong> <a href="https://www.edutopia.org/article/powerful-benefits-reflective-journaling" target="_blank" rel="noopener">Reflective Journaling Benefits</a>, <a href="https://fs.blog/feynman-technique/" target="_blank" rel="noopener">Feynman Technique</a>' },
            { label: 'Update docs/questions/week_02.md', estMinutes: 20, details: '<strong>Action:</strong> Create or update your questions document for Week 2 (docs/questions/week_02.md). List: (1) Any concepts still unclear or confusing, (2) Questions that arose during learning (even if partially answered), (3) Connections you\'re curious about but haven\'t fully explored, (4) Topics you want to revisit or study more deeply. For each item, note: what you understand so far, what\'s unclear, what you\'ve tried to clarify it, and where you might find answers. <strong>Boundaries:</strong> Be specific and honest. Instead of "eigenvalues are confusing," write "I understand how to compute eigenvalues algebraically but don\'t fully grasp why the characteristic polynomial det(A-λI)=0 works geometrically—need to explore the connection between determinant zero and eigenvalue definition." Well-formulated questions are halfway to answers. Include questions about applications: "How exactly do eigenvalues determine neural network training stability?" <strong>Deliverable:</strong> A structured list of 3-10 specific, well-formulated questions or areas needing clarification. Should be actionable—you or a mentor could work on answering them. <strong>Verification:</strong> Questions are specific and clear. Each includes context about current understanding. Questions are neither too trivial nor too broad. Could you give this list to a mentor and have a productive discussion? Common pitfall: being vague or not writing down confusion because it seems "basic"—document everything unclear! Success check: Your questions should be specific enough to guide further study. Example: "Why do symmetric matrices always have orthogonal eigenvectors? I know it\'s true from practice, but what\'s the geometric or algebraic reason?" Maintaining a questions document helps identify knowledge gaps and guides focused study. Week 2 was conceptually dense; it\'s normal to have lingering questions. Estimated time: 20 minutes for reflection and documentation. <strong>Resources:</strong> <a href="https://stackoverflow.com/help/how-to-ask" target="_blank" rel="noopener">How to Ask Good Questions</a>' },
            { label: 'Post week 2 progress in HF Discord', estMinutes: 15, details: '<strong>Action:</strong> Share your Week 2 learning journey in the <a href="https://discord.gg/hugging-face-879548962464493619" target="_blank" rel="noopener">Hugging Face Discord</a>. Post in #introductions or relevant learning channel: (1) Brief summary of Week 2 topics (eigenvalues focus), (2) One cool insight or visualization you created, (3) One challenge you overcame or are working through, (4) A question for the community (about eigenvalues, ML applications, or learning strategies), (5) Your Week 3 plans. Keep it conversational and genuine—aim for 200-300 words or a short thread. <strong>Boundaries:</strong> Share authentically without overselling or underselling your progress. Include a visualization or code snippet if possible (e.g., an eigenvalue animation or interesting notebook output). Engage with any responses—learning in public builds community and accountability. Don\'t just post and disappear; participate in the discussion your post generates. <strong>Deliverable:</strong> A genuine, engaging post sharing your learning journey and inviting community interaction. Should represent your authentic experience and contribute to the community. <strong>Verification:</strong> Post is live and well-received. Did you get responses, advice, or encouragement? Did sharing help clarify your own understanding (teaching effect)? Did you make any new connections or learn from others\' responses? Common pitfall: waiting until you\'re "expert enough" to share—share the learning process itself, including struggles! Success check: Your post should invite engagement. Example: "Week 2 down! Deep dove into eigenvalues—they\'re like the \'natural frequencies\' of a transformation. Created this visualization [link] showing how eigenvectors stay on their span. Still wrapping my head around why this matters for PCA—any intuitive explanations?" Learning in public builds accountability, gets feedback, creates a network, and reinforces understanding through teaching. It also documents your journey. Estimated time: 15 minutes for composing and posting. <strong>Resources:</strong> <a href="https://www.swyx.io/learn-in-public/" target="_blank" rel="noopener">Learn in Public Philosophy</a>, <a href="https://discord.gg/hugging-face-879548962464493619" target="_blank" rel="noopener">Hugging Face Discord</a>' }
          ],
          reflectionPrompt: 'How confident are you with eigenvalues? Any gaps to revisit?'
        },
        {
          globalDay: 15,
          week: 3,
          title: 'Derivatives & Gradient Foundations',
          priority: 'HIGH',
          tasks: [
            { label: 'Study derivatives: definition, rules, and geometric interpretation', estMinutes: 90, details: '<strong>Action:</strong> Study the fundamental concept of derivatives through <a href="https://www.khanacademy.org/math/calculus-1/cs1-derivatives-definition-and-basic-rules" target="_blank" rel="noopener">Khan Academy Calculus 1: Derivatives</a> or <a href="https://www.youtube.com/watch?v=9vKqVkMQHKk" target="_blank" rel="noopener">3Blue1Brown Essence of Calculus Chapter 2</a>. Focus on: (1) Definition as instantaneous rate of change (limit of difference quotient), (2) Geometric meaning: slope of tangent line, (3) Basic rules: power rule, sum rule, product rule, quotient rule, chain rule, (4) Common derivatives: polynomials, exponentials, logarithms, trigonometric functions. <strong>Boundaries:</strong> Start with the intuition: derivative measures "how fast something changes." Work through the formal definition f\'(x) = lim[h→0] (f(x+h)-f(x))/h to understand it conceptually, not just as a formula. Practice computing derivatives using rules, then verify geometrically by plotting functions and their derivatives. Focus on single-variable calculus first. <strong>Deliverable:</strong> Solid understanding of what derivatives are, why they matter, and how to compute them using standard rules. Geometric intuition connecting derivative to slope. <strong>Verification:</strong> Can you explain derivatives as both rates of change and slopes? Can you compute derivatives of polynomials, exponentials, and compositions using the chain rule? Can you sketch the derivative of a function given its graph? Common pitfall: memorizing rules without understanding what a derivative means—always connect computation to geometric/physical meaning. Success check: Explain why the derivative of x² is 2x both algebraically (using difference quotient) and geometrically (slope changes as you move along parabola). Derivatives are the foundation of machine learning: gradients (multivariable derivatives) drive backpropagation and optimization. Every time a neural network learns, it\'s computing and following derivatives. Estimated time: 90 minutes including examples and practice. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/calculus-1" target="_blank" rel="noopener">Khan Academy: Calculus 1</a>, <a href="https://www.3blue1brown.com/topics/calculus" target="_blank" rel="noopener">3Blue1Brown: Essence of Calculus</a>, <a href="https://tutorial.math.lamar.edu/Classes/CalcI/DefnOfDerivative.aspx" target="_blank" rel="noopener">Paul\'s Online Math: Derivatives</a>' },
            { label: 'Implement numerical differentiation in Python/NumPy', estMinutes: 75, details: '<strong>Action:</strong> Implement numerical methods for computing derivatives since we often can\'t compute them analytically: (1) Forward difference: f\'(x) ≈ (f(x+h) - f(x))/h, (2) Central difference (more accurate): f\'(x) ≈ (f(x+h) - f(x-h))/(2h), (3) Second derivatives, (4) Comparison with analytical derivatives to understand numerical accuracy. Create functions <code>numerical_derivative(f, x, h=1e-5, method=\'central\')</code> with proper documentation. <strong>Boundaries:</strong> Implement from scratch to understand the mathematics—don\'t just use scipy.derivative. Test with known functions (x², sin(x), e^x) where you can compare to analytical derivatives. Explore how step size h affects accuracy (too large: poor approximation, too small: numerical errors). Plot error vs h to see the trade-off. Include edge case handling. <strong>Deliverable:</strong> Working numerical differentiation functions with tests comparing to analytical results. Exploration of numerical accuracy and step size selection. Should include visualization of how numerical approximation works. <strong>Verification:</strong> Functions compute derivatives accurately for test cases. Error analysis shows expected behavior. Code handles edge cases. Can you explain why central difference is more accurate than forward difference (cancels more error terms)? Common pitfall: not validating against known results—always test numerical methods. Success check: Your numerical derivative of sin(x) at x=0 should give approximately 1 (cosine of 0), and your code should show how accuracy improves with better methods. Numerical differentiation is important when functions are black boxes or too complex for analytical derivatives—this happens in scientific computing and some ML contexts. Estimated time: 75 minutes for implementation, testing, and analysis. <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Numerical_differentiation" target="_blank" rel="noopener">Wikipedia: Numerical Differentiation</a>, <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.derivative.html" target="_blank" rel="noopener">SciPy derivative</a>, <a href="https://numpy.org/doc/stable/reference/generated/numpy.gradient.html" target="_blank" rel="noopener">NumPy gradient</a>' },
            { label: 'Practice derivatives: 15 problems covering all derivative rules', estMinutes: 60, details: '<strong>Action:</strong> Complete 15 derivative problems covering all major rules and function types: (1-3) Power rule: derivatives of x³, x^(-2), √x, (4-5) Product rule: x²·sin(x), (x+1)·e^x, (6-7) Quotient rule: x/sin(x), e^x/(x²+1), (8-10) Chain rule: sin(x²), e^(x³), (2x+1)^5, (11-12) Combinations: x²·e^(sin(x)), (13) Implicit differentiation, (14-15) Second derivatives. Work by hand first, then verify using SymPy or your numerical implementation. <strong>Boundaries:</strong> Show all steps—don\'t skip the intermediate work. For each problem: identify which rules apply, apply them carefully, simplify the result. Verify answers by plotting the original function and your computed derivative using matplotlib to check if derivative values match slopes. Check corner cases: what happens at x=0, at discontinuities, as x→∞? <strong>Deliverable:</strong> 15 complete, verified solutions demonstrating fluency with all derivative rules. Should include both hand calculations and programmatic verification. <strong>Verification:</strong> All answers correct (verified numerically or with SymPy). Work is shown clearly. You can complete most problems in 3-4 minutes each. Can you recognize which rules to apply automatically? Are you making careless errors (redo those problem types)? Common pitfall: rushing through without understanding—slow down and internalize each rule. Success check: For chain rule problem d/dx[sin(x²)], you should automatically recognize it as cos(x²)·2x, understanding both the rule application and why it works. Computational fluency with derivatives is essential because you\'ll be thinking about gradients constantly in ML—backpropagation is just chain rule applied systematically. Build this fluency now. Estimated time: 60 minutes for focused problem-solving with verification. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/calculus-1/cs1-derivatives-definition-and-basic-rules" target="_blank" rel="noopener">Khan Academy Derivative Practice</a>, <a href="https://www.symbolab.com/solver/derivative-calculator" target="_blank" rel="noopener">Symbolab Derivative Calculator (for checking)</a>, <a href="https://docs.sympy.org/latest/modules/calculus/index.html" target="_blank" rel="noopener">SymPy Calculus</a>' }
          ],
          reflectionPrompt: 'How do derivatives connect to optimization in ML?'
        },
        {
          globalDay: 16,
          week: 3,
          title: 'Partial Derivatives & Gradients',
          priority: 'HIGH',
          tasks: [
            { label: 'Study partial derivatives and gradients for multivariable functions', estMinutes: 90, details: '<strong>Action:</strong> Learn multivariable calculus concepts through <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives" target="_blank" rel="noopener">Khan Academy Multivariable Derivatives</a> and <a href="https://www.youtube.com/watch?v=TrcCbdWwCBc" target="_blank" rel="noopener">3Blue1Brown: What is a Partial Derivative?</a>. Focus on: (1) Partial derivatives: ∂f/∂x treats other variables as constants, (2) Gradient vector: ∇f = [∂f/∂x, ∂f/∂y,...] points in direction of steepest increase, (3) Directional derivatives: rate of change in any direction, (4) Second partial derivatives and Hessian matrix. <strong>Boundaries:</strong> Start with 2D functions (easier to visualize) before generalizing. Understand that gradient ∇f at a point gives both the direction of steepest ascent and the magnitude of that steepness. Visualize: contour plots with gradient vectors showing they\'re perpendicular to contours. Connect to optimization: gradient descent moves opposite to gradient (steepest descent). <strong>Deliverable:</strong> Understanding of partial derivatives, gradients as vectors of partial derivatives, and geometric interpretation. Ability to compute gradients of multivariable functions. <strong>Verification:</strong> Can you compute ∇f for f(x,y) = x²y + sin(y)? Can you explain geometrically why gradient is perpendicular to level curves? Can you sketch gradient vectors on a contour plot? Common pitfall: confusing partial derivative notation (∂ vs d)—∂ is for multivariable, treating other variables as constant. Success check: For f(x,y) = x² + y², gradient is [2x, 2y], which points radially outward—away from the minimum at origin. This is the mathematical foundation of gradient descent, the core optimization algorithm in deep learning. Every neural network weight update follows the negative gradient. Estimated time: 90 minutes for concepts and visualization. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/multivariable-calculus" target="_blank" rel="noopener">Khan Academy: Multivariable Calculus</a>, <a href="https://www.3blue1brown.com/topics/calculus" target="_blank" rel="noopener">3Blue1Brown: Calculus</a>, <a href="https://tutorial.math.lamar.edu/Classes/CalcIII/PartialDerivatives.aspx" target="_blank" rel="noopener">Paul\'s Online Math: Partial Derivatives</a>' },
            { label: 'Implement gradient computation and visualization in Python', estMinutes: 75, details: '<strong>Action:</strong> Implement functions to compute and visualize gradients: (1) <code>compute_gradient(f, point)</code> using numerical partial derivatives, (2) <code>plot_contour_with_gradients(f, xlim, ylim)</code> showing contours and gradient vectors, (3) <code>gradient_descent_step(f, x, learning_rate)</code> implementing one step of gradient descent: x_new = x - α·∇f(x), (4) Animated gradient descent visualization showing path to minimum. Use NumPy for computations and matplotlib for visualization. <strong>Boundaries:</strong> Implement numerical partial derivatives: compute ∂f/∂x by varying x while holding y constant, similarly for ∂f/∂y. Create clear visualizations: use plt.contour for level curves, plt.quiver for gradient vectors, and ensure vectors are scaled appropriately. For gradient descent, choose test function like f(x,y) = (x-1)²+ (y-2)² with known minimum at (1,2). Show multiple learning rates to demonstrate behavior. <strong>Deliverable:</strong> Working gradient computation and visualization tools. Should include animations showing gradient descent converging to minima. Clean, documented code that could be reused for optimization problems. <strong>Verification:</strong> Gradient computations match analytical results for test functions. Visualizations clearly show relationship between contours and gradients (perpendicular). Gradient descent successfully finds minima. Different learning rates show expected behavior (too small: slow, too large: oscillation/divergence). Common pitfall: poor visualization scaling—normalize gradient vectors for clear display. Success check: Your gradient descent on f(x,y) = x² + y² starting from any point should converge to (0,0). Visualize this with a trail showing the optimization path. This is literally programming the core of how neural networks learn. Estimated time: 75 minutes for implementation and visualization. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/generated/numpy.gradient.html" target="_blank" rel="noopener">NumPy gradient</a>, <a href="https://matplotlib.org/stable/gallery/images_contours_and_fields/quiver_demo.html" target="_blank" rel="noopener">Matplotlib quiver</a>, <a href="https://matplotlib.org/stable/api/animation_api.html" target="_blank" rel="noopener">Matplotlib animation</a>' },
            { label: 'Practice computing gradients and understanding optimization geometry', estMinutes: 60, details: '<strong>Action:</strong> Complete 10 gradient problems with geometric interpretation: (1-3) Compute gradients: ∇(x²+y²), ∇(x·y), ∇(e^(x+2y)), (4-5) Evaluate at specific points, (6) Find where gradient is zero (critical points), (7-8) Determine if critical points are minima/maxima/saddle points using second derivative test (Hessian), (9) Given contour plot, sketch gradient field, (10) Given gradient field, sketch approximate function. Verify computations using SymPy and your numerical implementation. <strong>Boundaries:</strong> For each problem, compute analytically first, then verify numerically. For critical point problems, use the Hessian matrix (second partial derivatives) to classify: positive definite → minimum, negative definite → maximum, indefinite → saddle point. Visualize results: plot the function surface and its gradient field. Connect every computation to geometric meaning. <strong>Deliverable:</strong> 10 complete solutions with both analytical and visual verification. Should demonstrate understanding of gradients as direction of steepest ascent and their role in optimization. <strong>Verification:</strong> All analytical gradients correct (check with SymPy). Numerical approximations match. Geometric interpretations are accurate. Critical point classifications are correct. Can you quickly compute gradients and recognize optimization landscapes? Common pitfall: treating gradients as mere calculations without geometric understanding—always visualize. Success check: For f(x,y) = x²-y², gradient is [2x, -2y], and origin is a saddle point (verified by Hessian having one positive and one negative eigenvalue). Understand this is why some optimization landscapes have saddle points that slow training. Understanding gradient geometry is crucial for grasping why optimization works, what can go wrong (flat regions, saddle points, local minima), and how to improve training. Estimated time: 60 minutes for analytical work and visualization. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives" target="_blank" rel="noopener">Khan Academy: Applications of Multivariable Derivatives</a>, <a href="https://docs.sympy.org/latest/modules/vector/index.html" target="_blank" rel="noopener">SymPy Vector Calculus</a>' }
          ],
          reflectionPrompt: 'How do gradients enable neural network training?'
        },
        {
          globalDay: 17,
          week: 3,
          title: 'Chain Rule & Backpropagation Foundations',
          priority: 'HIGH',
          tasks: [
            { label: 'Deep dive into chain rule for composite and multivariable functions', estMinutes: 90, details: '<strong>Action:</strong> Master the chain rule in both single and multivariable contexts: (1) Single variable: d/dx[f(g(x))] = f\'(g(x))·g\'(x), (2) Multivariable chain rule: if z = f(x,y) and x=g(t), y=h(t), then dz/dt uses partial derivatives, (3) Nested compositions: d/dx[f(g(h(x)))], (4) Computational graphs: visualize function composition as graphs with forward and backward passes. Study through <a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction" target="_blank" rel="noopener">Khan Academy Chain Rule</a> and <a href="https://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">Chris Olah\'s Backpropagation blog</a>. <strong>Boundaries:</strong> Understand chain rule is about decomposing derivatives through intermediate steps. For multivariable: ∂z/∂x when z depends on x through multiple paths requires summing over all paths. Draw computation graphs showing dependencies and flow of derivatives. This is the mathematical foundation of backpropagation. <strong>Deliverable:</strong> Solid understanding of chain rule in various forms, with ability to draw and analyze computational graphs. Recognition that backprop is just systematic chain rule application. <strong>Verification:</strong> Can you compute d/dx[sin(x²)] using chain rule? Can you derive ∂L/∂w for a simple neural network L = (wx + b - y)²? Can you draw a computation graph and trace derivatives backward? Common pitfall: forgetting to multiply all terms in the chain—be systematic. Success check: For y = sin(e^(x²)), compute dy/dx = cos(e^(x²))·e^(x²)·2x, explaining each step. Understand that this nested derivative computation is exactly what happens in backpropagation through neural network layers. Chain rule is backpropagation; backpropagation is chain rule. This is the most important calculus concept for deep learning. Estimated time: 90 minutes including computational graphs. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/calculus-1/cs1-derivative-rules" target="_blank" rel="noopener">Khan Academy: Chain Rule</a>, <a href="https://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">Colah\'s Backprop Blog</a>, <a href="https://cs231n.github.io/optimization-2/" target="_blank" rel="noopener">CS231n: Backprop</a>' },
            { label: 'Implement backpropagation from scratch for simple computation graphs', estMinutes: 75, details: '<strong>Action:</strong> Implement a simple automatic differentiation system to understand backpropagation: (1) Create a <code>ComputationNode</code> class that stores value and gradient, (2) Implement basic operations (add, multiply, power, sin, exp) as nodes with forward and backward methods, (3) Build computation graphs for expressions like f(x) = sin(x²) + e^x, (4) Implement <code>forward()</code> to compute outputs and <code>backward()</code> to compute gradients via chain rule, (5) Verify against analytical derivatives. <strong>Boundaries:</strong> Start simple: implement operations one at a time, testing each. Forward pass computes and stores intermediate values. Backward pass propagates gradients from output to inputs using chain rule at each node. Don\'t use PyTorch/TensorFlow yet—build from scratch to understand the mechanics. Test with functions where you know the analytical derivative. <strong>Deliverable:</strong> Working mini autograd system that can compute derivatives through computation graphs. Should correctly handle chains of operations. Well-documented code showing how chain rule is applied automatically. <strong>Verification:</strong> System computes correct gradients for test expressions. Compare to SymPy or numerical derivatives. Gradients correctly handle multiple uses of same variable (need to sum gradients). Chain of operations produces correct derivatives. Common pitfall: not accumulating gradients when a variable is used multiple times—each use contributes to the gradient. Success check: For f(x,y) = x·y + x², your system should compute ∂f/∂x = y + 2x and ∂f/∂y = x correctly through automatic backpropagation. This is a simplified version of what PyTorch\'s autograd does. You\'re building the core mechanism of deep learning frameworks. Estimated time: 75 minutes for implementation and testing. <strong>Resources:</strong> <a href="https://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">Backprop Visualization</a>, <a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html" target="_blank" rel="noopener">PyTorch Autograd Tutorial</a>, <a href="https://cs231n.github.io/optimization-2/" target="_blank" rel="noopener">CS231n: Backprop Notes</a>' },
            { label: 'Practice chain rule problems with increasing complexity', estMinutes: 60, details: '<strong>Action:</strong> Complete 12 chain rule problems building from simple to complex: (1-2) Basic: d/dx[sin(2x)], d/dx[e^(3x²)], (3-4) Nested: d/dx[sin(e^x)], d/dx[ln(sin(x))], (5-6) Multiple composition: d/dx[sin(cos(x))], d/dx[(x²+1)^5], (7-8) Multivariable: ∂z/∂x for z=f(u,v), u=x², v=xy, (9-10) Product+chain: d/dx[x·sin(x²)], d/dx[e^x·cos(2x)], (11-12) Mini neural network derivatives: ∂L/∂w for L=(σ(wx)-y)² where σ(z)=1/(1+e^(-z)), compute for both w and showing full chain. <strong>Boundaries:</strong> Show complete work for each problem. Draw computation graphs for the more complex ones, labeling each intermediate step. For the neural network problem, explicitly show the chain rule application at each step: ∂L/∂σ, ∂σ/∂z, ∂z/∂w. Verify answers using your autograd implementation and SymPy. <strong>Deliverable:</strong> 12 complete solutions demonstrating mastery of chain rule in various contexts. Neural network problem should show full backpropagation derivation. <strong>Verification:</strong> All solutions correct (verified symbolically and numerically). Computation graphs accurately represent function composition. Neural network gradient derivation is complete and correct. Can you recognize chain rule situations automatically and apply correctly? Common pitfall: losing track of intermediate steps in complex chains—use computation graphs to stay organized. Success check: For the neural network problem L=(σ(wx)-y)², you should derive ∂L/∂w = 2(σ(wx)-y)·σ(wx)·(1-σ(wx))·x, recognizing this is how weights are updated in logistic regression. Understanding this derivation means understanding how neural networks learn. Estimated time: 60 minutes for focused problem-solving. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/calculus-1/cs1-derivative-rules/cs1-chain-rule" target="_blank" rel="noopener">Khan Academy: Chain Rule Practice</a>, <a href="https://docs.sympy.org/latest/modules/calculus/index.html" target="_blank" rel="noopener">SymPy for verification</a>' }
          ],
          reflectionPrompt: 'Can you explain backpropagation using the chain rule?'
        },
        {
          globalDay: 18,
          week: 3,
          title: 'Probability Foundations',
          priority: 'HIGH',
          tasks: [
            { label: 'Study probability basics: events, sample spaces, axioms', estMinutes: 90, details: '<strong>Action:</strong> Learn probability fundamentals through <a href="https://www.khanacademy.org/math/statistics-probability/probability-library" target="_blank" rel="noopener">Khan Academy Probability</a>: (1) Sample spaces and events, (2) Probability axioms (non-negativity, normalization, additivity), (3) Conditional probability P(A|B) = P(A∩B)/P(B), (4) Bayes\' theorem, (5) Independence, (6) Random variables and distributions (discrete and continuous). <strong>Boundaries:</strong> Focus on understanding probability as measure of uncertainty. Work through classic examples: coin flips, dice rolls, card draws. Understand conditional probability deeply—it\'s crucial for Bayes\' theorem and ML. Distinguish between discrete (countable outcomes) and continuous (real-valued) probabilities. <strong>Deliverable:</strong> Solid foundation in probability theory with ability to compute probabilities, use Bayes\' rule, and understand distributions. <strong>Verification:</strong> Can you compute P(A|B) given P(B|A), P(A), P(B)? Can you explain why P(A and B) = P(A)·P(B) only when independent? Common pitfall: confusing P(A|B) with P(B|A)—they\'re different! Success check: Given P(disease) = 0.01, P(positive test | disease) = 0.99, P(positive test | no disease) = 0.05, use Bayes\' theorem to find P(disease | positive test) ≈ 0.17. Understand why this is surprisingly low (base rate fallacy). Probability is the language of uncertainty in ML—classification outputs are probabilities, Bayesian methods model uncertainty, and generative models learn probability distributions. Estimated time: 90 minutes. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/statistics-probability" target="_blank" rel="noopener">Khan Academy: Probability</a>, <a href="https://seeing-theory.brown.edu/basic-probability/index.html" target="_blank" rel="noopener">Seeing Theory: Interactive Probability</a>, <a href="https://www.probabilitycourse.com/" target="_blank" rel="noopener">Introduction to Probability Course</a>' },
            { label: 'Implement probability simulations using NumPy random', estMinutes: 75, details: '<strong>Action:</strong> Use NumPy to simulate probabilistic phenomena: (1) Coin flips and law of large numbers, (2) Monte Carlo estimation of π by random sampling, (3) Birthday paradox simulation, (4) Conditional probability verification, (5) Sampling from common distributions (binomial, normal, exponential). Use np.random functions and visualize results with histograms. <strong>Boundaries:</strong> Create functions like <code>simulate_coin_flips(n, p=0.5)</code> and <code>estimate_pi_monte_carlo(n_samples)</code>. Demonstrate convergence: as sample size increases, empirical frequencies approach theoretical probabilities. Visualize distributions and compare empirical vs theoretical. Include proper random seed setting for reproducibility. <strong>Deliverable:</strong> Collection of probability simulations demonstrating key concepts. Should include visualizations showing convergence to theoretical values with increasing samples. <strong>Verification:</strong> Simulations converge to expected values. Histograms match theoretical distributions. Code is reproducible with seed control. Can you explain why 10,000 samples give better estimates than 100? Common pitfall: not using enough samples—probability emerges from large numbers. Success check: Your Monte Carlo π estimation should converge to 3.14159... with sufficient samples, demonstrating that simulation can solve problems without analytical solutions. Monte Carlo methods are fundamental in ML for approximate inference, reinforcement learning, and handling stochastic processes. Estimated time: 75 minutes. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/random/index.html" target="_blank" rel="noopener">NumPy Random</a>, <a href="https://matplotlib.org/stable/gallery/statistics/hist.html" target="_blank" rel="noopener">Matplotlib Histograms</a>' },
            { label: 'Practice probability problems including Bayes\' theorem', estMinutes: 60, details: '<strong>Action:</strong> Solve 10 probability problems: (1-2) Basic probability with dice/cards, (3) Conditional probability computation, (4-5) Two Bayes\' theorem applications (medical testing, spam filtering), (6) Independence verification, (7-8) Expected value and variance calculations, (9) Binomial probability, (10) Normal distribution application. Show all work using probability formulas. <strong>Boundaries:</strong> For Bayes\' problems, always draw a probability tree or table to organize information. Verify P(A∩B) = P(A|B)·P(B) = P(B|A)·P(A). For distributions, use both formula and simulation to verify. Check your intuition: does the answer make sense? Common error: forgetting to normalize in Bayes\' theorem. <strong>Deliverable:</strong> 10 complete solutions demonstrating probability fundamentals and Bayesian reasoning. <strong>Verification:</strong> All answers correct. Bayes\' problems show proper application of the theorem. Expected values and variances computed correctly. Can you recognize when to use each concept? Common pitfall: mechanical formula application without understanding. Success check: Explain why naive Bayes classifier works: P(class|features) ∝ P(features|class)·P(class), assuming feature independence. This probabilistic reasoning powers many ML algorithms. Estimated time: 60 minutes. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/statistics-probability/probability-library" target="_blank" rel="noopener">Khan Academy: Probability</a>, <a href="https://brilliant.org/wiki/bayes-theorem/" target="_blank" rel="noopener">Brilliant: Bayes\' Theorem</a>' }
          ],
          reflectionPrompt: 'How does probability relate to machine learning predictions?'
        },
        {
          globalDay: 19,
          week: 3,
          title: 'Statistics & Distributions',
          priority: 'HIGH',
          tasks: [
            { label: 'Study descriptive statistics and common probability distributions', estMinutes: 90, details: '<strong>Action:</strong> Learn statistics fundamentals through <a href="https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data" target="_blank" rel="noopener">Khan Academy Statistics</a>: (1) Descriptive stats: mean, median, mode, variance, standard deviation, (2) Normal (Gaussian) distribution and properties, (3) Central limit theorem, (4) Binomial, Poisson, exponential distributions, (5) Correlation vs causation, (6) Hypothesis testing intuition. <strong>Boundaries:</strong> Understand measures of center (mean, median) and spread (variance, std). Focus on normal distribution—it appears everywhere in ML due to CLT. Understand that sample statistics estimate population parameters. Don\'t get lost in hypothesis testing details yet—focus on distributions and their properties. <strong>Deliverable:</strong> Understanding of descriptive statistics, key probability distributions (especially normal), and their ML relevance. <strong>Verification:</strong> Can you explain why standard deviation measures spread? Why normal distribution is ubiquitous (CLT)? What μ and σ mean for normal distribution? Common pitfall: confusing variance and standard deviation—variance is squared, std is in original units. Success check: Understand that adding independent random variables leads to normal distribution (CLT), which is why error distributions, weight initializations, and many natural phenomena are Gaussian. This explains why normal distribution is central to statistics and ML. Estimated time: 90 minutes. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/statistics-probability" target="_blank" rel="noopener">Khan Academy: Statistics</a>, <a href="https://seeing-theory.brown.edu/probability-distributions/index.html" target="_blank" rel="noopener">Seeing Theory: Distributions</a>, <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener">Wikipedia: Central Limit Theorem</a>' },
            { label: 'Implement statistical computations and visualize distributions', estMinutes: 75, details: '<strong>Action:</strong> Implement statistical functions and visualizations: (1) Compute descriptive stats (mean, median, std, quartiles) using NumPy and from scratch, (2) Plot histograms and fit distributions, (3) Demonstrate CLT: sum of random variables converges to normal, (4) Generate samples from various distributions and visualize, (5) Compute correlation coefficients and create scatter plots. Use NumPy stats functions and scipy.stats for distributions. <strong>Boundaries:</strong> Implement mean and variance from formulas to understand them: mean = sum(x)/n, variance = sum((x-mean)²)/n. Verify against NumPy. For CLT demo, sum uniform random variables and show histogram approaches normal. Create clean visualizations comparing empirical and theoretical distributions. <strong>Deliverable:</strong> Statistical computing toolkit with clear visualizations. CLT demonstration showing convergence to normality. Should include both implementation and visualization code. <strong>Verification:</strong> Functions compute correct statistics. CLT demo clearly shows convergence. Distribution plots match theoretical shapes. Correlation computations are accurate. Code is well-documented. Common pitfall: not visualizing distributions—statistics are about understanding data distributions. Success check: Your CLT demo should show that summing 12 uniform(0,1) random variables produces approximately normal distribution, visually demonstrating why normal is universal. Understanding statistical distributions is essential for probabilistic ML, generative models, and understanding model uncertainties. Estimated time: 75 minutes. <strong>Resources:</strong> <a href="https://numpy.org/doc/stable/reference/routines.statistics.html" target="_blank" rel="noopener">NumPy Statistics</a>, <a href="https://scipy.org/doc/scipy/reference/stats.html" target="_blank" rel="noopener">SciPy Stats</a>, <a href="https://seaborn.pydata.org/tutorial/distributions.html" target="_blank" rel="noopener">Seaborn: Distribution Visualization</a>' },
            { label: 'Practice statistics: compute descriptive stats and work with distributions', estMinutes: 60, details: '<strong>Action:</strong> Complete 10 statistics problems: (1-2) Compute mean, median, std from datasets, (3) Interpret standard deviation (what does σ=5 mean?), (4-5) Normal distribution: find P(X < value) and percentiles, (6) Use 68-95-99.7 rule, (7-8) Compare distributions (which has higher variance?), (9) Correlation computation and interpretation, (10) CLT application: distribution of sample means. Use scipy.stats for distribution functions. <strong>Boundaries:</strong> For distribution problems, use both tables/functions and simulation to verify. Understand what you\'re computing: P(X < 1.5) for X~N(0,1) means area under curve left of 1.5. For correlation, distinguish between correlation coefficient r and causation. Interpret statistics in context—don\'t just compute numbers. <strong>Deliverable:</strong> 10 complete solutions showing statistical reasoning and distribution understanding. Should demonstrate both computation and interpretation. <strong>Verification:</strong> All computations correct. Interpretations are meaningful. Distribution calculations use proper methods. Understanding of what statistics represent is clear. Common pitfall: computing statistics without understanding what they mean—always interpret results. Success check: Given data with mean=100, std=15, know that ~68% falls in [85,115], ~95% in [70,130]. Understand this characterizes the distribution\'s shape and spread. Statistical literacy is essential for evaluating ML models, understanding datasets, and making data-driven decisions. Estimated time: 60 minutes. <strong>Resources:</strong> <a href="https://scipy.org/doc/scipy/reference/stats.html" target="_blank" rel="noopener">SciPy Stats Module</a>, <a href="https://www.khanacademy.org/math/statistics-probability" target="_blank" rel="noopener">Khan Academy Practice</a>' }
          ],
          reflectionPrompt: 'Why is the normal distribution so important in ML?'
        },
        {
          globalDay: 20,
          week: 3,
          title: 'Python Data Manipulation',
          priority: 'HIGH',
          tasks: [
            { label: 'Learn pandas DataFrames and data manipulation operations', estMinutes: 90, details: '<strong>Action:</strong> Master pandas basics through <a href="https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html" target="_blank" rel="noopener">pandas tutorials</a>: (1) Creating DataFrames, (2) Selecting rows/columns (.loc, .iloc, boolean indexing), (3) Data cleaning (handling missing values, duplicates), (4) GroupBy and aggregation, (5) Merging and joining DataFrames, (6) Basic visualizations with pandas plotting. <strong>Boundaries:</strong> Start with creating DataFrames from dictionaries and CSV files. Practice selecting subsets—this is crucial for data exploration. Understand how to handle NaN values (dropna, fillna). GroupBy is powerful for aggregations like mean per category. Focus on practical data manipulation you\'ll use for ML. <strong>Deliverable:</strong> Proficiency with pandas for common data operations. Ability to load, explore, clean, and transform tabular data. <strong>Verification:</strong> Can you load a CSV, handle missing values, filter rows, group by category and compute means, merge datasets? Can you quickly explore data structure with .head(), .info(), .describe()? Common pitfall: not checking data after operations—always inspect results with .head() or .shape. Success check: Load iris dataset, filter to setosa species, compute mean petal length. This basic workflow appears in every ML project. Pandas is the standard tool for data manipulation in Python—mastering it is essential for ML. Estimated time: 90 minutes with examples. <strong>Resources:</strong> <a href="https://pandas.pydata.org/docs/user_guide/index.html" target="_blank" rel="noopener">Pandas User Guide</a>, <a href="https://www.datacamp.com/tutorial/pandas" target="_blank" rel="noopener">DataCamp pandas Tutorial</a>, <a href="https://github.com/pandas-dev/pandas/tree/main/doc/cheatsheet" target="_blank" rel="noopener">Pandas Cheat Sheet</a>' },
            { label: 'Practice data manipulation with real datasets using pandas', estMinutes: 75, details: '<strong>Action:</strong> Work with real datasets to practice pandas: (1) Load Titanic or Iris dataset, (2) Explore with .info(), .describe(), value_counts(), (3) Clean data: handle missing values, fix dtypes, (4) Filter and select subsets, (5) Compute group statistics, (6) Create new columns (feature engineering), (7) Merge with another dataset, (8) Export cleaned data. Document each step in a notebook. <strong>Boundaries:</strong> Choose a dataset from sklearn.datasets or download CSV (Titanic, iris, wine, etc.). Make this realistic: explore first, then decide cleaning strategy. Practice both .loc/.iloc indexing and boolean indexing. Create at least one new feature (e.g., family_size from sibsp+parch in Titanic). Comment your code explaining decisions. <strong>Deliverable:</strong> Complete data manipulation notebook showing exploration, cleaning, transformation, and analysis of a real dataset. Should demonstrate practical pandas workflows. <strong>Verification:</strong> Notebook runs without errors. Data transformations are appropriate. New features make sense. Group statistics are meaningful. Code is well-commented and follows good practices. Common pitfall: not exploring data before cleaning—understand your data first. Success check: Your notebook should tell a story: "I loaded X, found Y issues, cleaned them, discovered Z pattern, created W feature." This is the workflow of data preparation for ML. Every ML project starts with data manipulation—building this skill is crucial. Estimated time: 75 minutes for thorough practice. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/datasets.html" target="_blank" rel="noopener">sklearn Datasets</a>, <a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">Kaggle Datasets</a>, <a href="https://pandas.pydata.org/docs/user_guide/10min.html" target="_blank" rel="noopener">10 Minutes to pandas</a>' },
            { label: 'Learn data visualization with matplotlib and seaborn', estMinutes: 60, details: '<strong>Action:</strong> Study visualization through <a href="https://matplotlib.org/stable/tutorials/index.html" target="_blank" rel="noopener">matplotlib tutorials</a> and <a href="https://seaborn.pydata.org/tutorial.html" target="_blank" rel="noopener">seaborn documentation</a>: (1) Basic plots: line, scatter, bar, histogram, (2) Customization: labels, titles, legends, colors, (3) Subplots and figure layout, (4) Seaborn for statistical plots: distributions, relationships, categorical, (5) Heatmaps and correlation matrices. Create example plots of each type. <strong>Boundaries:</strong> Master basic plot types first. Understand when to use each: scatter for relationships, histogram for distributions, bar for categories, line for sequences. Seaborn provides higher-level interface—use it for statistical plots. Practice making plots publication-ready with proper labels and titles. <strong>Deliverable:</strong> Collection of example visualizations demonstrating different plot types and customization. Should include both matplotlib and seaborn examples. <strong>Verification:</strong> Can you create a scatter plot with colored points by category? Histogram with appropriate bins? Correlation heatmap? Are plots well-labeled and informative? Common pitfall: creating plots without proper labels—always label axes and add titles. Success check: Create a pairplot with seaborn showing relationships between iris features colored by species. This one visualization reveals patterns that would take many separate plots. Visualization is essential for understanding data, debugging models, and communicating results. Build this skill early. Estimated time: 60 minutes. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/gallery/index.html" target="_blank" rel="noopener">Matplotlib Gallery</a>, <a href="https://seaborn.pydata.org/examples/index.html" target="_blank" rel="noopener">Seaborn Gallery</a>, <a href="https://python-graph-gallery.com/" target="_blank" rel="noopener">Python Graph Gallery</a>' }
          ],
          reflectionPrompt: 'How important is data manipulation for ML workflows?'
        },
        {
          globalDay: 21,
          week: 3,
          title: 'Week 3 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review all Week 3 materials: calculus, probability, statistics, pandas', estMinutes: 75, details: '<strong>Action:</strong> Comprehensive review of Week 3 covering diverse topics: (1) Derivatives and gradients (Days 15-16), (2) Chain rule and backprop foundations (Day 17), (3) Probability fundamentals (Day 18), (4) Statistics and distributions (Day 19), (5) Pandas data manipulation (Day 20). Re-run notebooks, review notes, test understanding by explaining concepts without references. <strong>Boundaries:</strong> This week covered a lot—focus on connections: derivatives enable optimization via gradients, chain rule enables backpropagation, probability and statistics provide the language for uncertainty in ML predictions, pandas provides tools for data preparation. Don\'t just review mechanically—understand how these pieces fit into the ML pipeline. <strong>Deliverable:</strong> Integrated understanding of Week 3 concepts with clear sense of how they support machine learning. Identification of any gaps needing more study. <strong>Verification:</strong> Can you explain the ML relevance of each topic? Can you solve representative problems from each day? Do you see how calculus→optimization, probability→predictions, statistics→evaluation, pandas→data preparation? Common pitfall: treating topics as isolated—see the connections. Success check: Explain: "Gradients (calculus) tell us how to adjust weights (optimization). Chain rule (backprop) computes gradients efficiently through layers. Probability models uncertainty in predictions. Statistics evaluates model performance. Pandas prepares data for training." Understand this end-to-end flow. Week 3 provided mathematical and practical foundations essential for ML. Ensure solid understanding before proceeding. Estimated time: 75 minutes. <strong>Resources:</strong> Your Week 3 notebooks and notes' },
            { label: 'Complete 8 integration problems connecting all Week 3 topics', estMinutes: 60, details: '<strong>Action:</strong> Solve 8 problems integrating Week 3 concepts: (1) Compute gradient of loss function L(w,b) = (ŷ-y)² where ŷ=wx+b, (2) Use chain rule to derive backprop equations for 2-layer network, (3) Given dataset, compute mean/std and probability within 1 std, (4) Apply Bayes\' theorem to classify examples, (5) Load CSV with pandas, clean data, compute statistics, (6) Visualize data distribution and fit normal curve, (7) Implement gradient descent on a function and visualize convergence, (8) Mini-project: load dataset, compute gradients for simple linear regression, perform gradient descent, evaluate using statistics. <strong>Boundaries:</strong> These problems should span topics, not silo them. Show all work. Verify computations. For the mini-project, bring everything together: load data (pandas), compute gradients (calculus), optimize (gradient descent), evaluate (statistics), visualize (plotting). <strong>Deliverable:</strong> 8 complete solutions culminating in mini-project integrating all Week 3 skills. Should demonstrate how concepts connect in practice. <strong>Verification:</strong> All problems correct. Mini-project works end-to-end. Code runs and produces reasonable results. Can you explain how each Week 3 topic contributed to the solution? Common pitfall: not seeing how topics connect—the problems force integration. Success check: Your mini-project should demonstrate the basic ML workflow using Week 3 foundations. This preview of what\'s coming. Estimated time: 60 minutes. <strong>Resources:</strong> Your Week 3 materials' },
            { label: 'Write docs/weekly_logs/week_03.md reflecting on diverse topics', estMinutes: 30, details: '<strong>Action:</strong> Write weekly log for Week 3 (700-800 words) covering: (1) Overview of topics: calculus, probability, statistics, data manipulation, (2) Connections discovered between topics and to ML, (3) Most challenging concepts (chain rule? Bayes? pandas?), (4) Breakthrough moments, (5) Artifacts created, (6) Skills developed (computational math, statistical thinking, data wrangling), (7) Integration: how do these pieces fit together for ML?, (8) Week 4 preview: putting math into practice with classical ML. <strong>Boundaries:</strong> Reflect on the breadth of Week 3—this was less cohesive than Weeks 1-2 (linear algebra focus) but equally important. Consider what felt like a gear shift: moving from pure math to probability/statistics to practical tools (pandas). Reflect on readiness for classical ML. <strong>Deliverable:</strong> Thoughtful weekly log capturing the journey through diverse but essential foundations. Should show understanding of how topics support ML. <strong>Verification:</strong> Log integrates topics rather than listing them separately. Shows genuine reflection on challenges and growth. Anticipates how Week 3 skills enable upcoming work. Common pitfall: treating Week 3 as disconnected topics—show the integration. Success check: Your log should convey understanding that ML requires diverse foundations: calculus for optimization, probability for modeling uncertainty, statistics for evaluation, data tools for practice. Estimated time: 30 minutes. <strong>Resources:</strong> <a href="https://www.edutopia.org/article/powerful-benefits-reflective-journaling" target="_blank" rel="noopener">Reflective Journaling</a>' }
          ],
          reflectionPrompt: 'How do Week 3 topics (calculus, probability, data tools) connect to ML?'
        },
        {
          globalDay: 22,
          week: 4,
          title: 'Advanced Calculus for ML',
          priority: 'HIGH',
          tasks: [
            { label: 'Study optimization theory: convexity, local vs global minima', estMinutes: 90, details: '<strong>Action:</strong> Study optimization fundamentals for ML through <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions" target="_blank" rel="noopener">Khan Academy optimization</a>: (1) First derivative test (critical points where ∇f=0), (2) Second derivative test (Hessian matrix for classification), (3) Convex functions and why they\'re nice (single global minimum), (4) Saddle points in high dimensions, (5) Gradient descent convergence conditions. <strong>Boundaries:</strong> Focus on concepts relevant to ML: understanding loss landscapes, why some functions are easy to optimize (convex) vs hard (non-convex with many local minima), role of second derivatives in Newton\'s method. <strong>Deliverable:</strong> Understanding of optimization theory underlying ML training algorithms. <strong>Verification:</strong> Can you explain why convex functions are ideal for optimization? Why neural networks are non-convex but still trainable? What saddle points are and why they matter? Common pitfall: thinking all optimization is about finding where derivative is zero—also need to verify it\'s a minimum. Success check: Understand that MSE loss for linear regression is convex (bowl-shaped, one global minimum), while neural network loss is non-convex (complex landscape with local minima and saddle points). Estimated time: 90 minutes. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/multivariable-calculus" target="_blank" rel="noopener">Khan Academy: Multivariable Calculus</a>, <a href="https://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes1.pdf" target="_blank" rel="noopener">Stanford CS229: Optimization Notes</a>, <a href="https://en.wikipedia.org/wiki/Convex_optimization" target="_blank" rel="noopener">Wikipedia: Convex Optimization</a>' },
            { label: 'Implement various gradient descent variants (batch, mini-batch, SGD)', estMinutes: 75, details: '<strong>Action:</strong> Implement optimization algorithms: (1) Batch gradient descent (use all data per update), (2) Stochastic gradient descent (SGD, one sample per update), (3) Mini-batch gradient descent (compromise), (4) Compare convergence on a test function, (5) Visualize optimization paths. Create <code>gradient_descent(f, x0, learning_rate, method=\'batch\')</code> with different update strategies. <strong>Boundaries:</strong> Use simple 2D function like f(x,y)=(x-2)²+(y-3)² so you can visualize convergence. For SGD, simulate by adding noise to gradients. Compare speed vs stability: batch is stable but slow, SGD is fast but noisy, mini-batch balances both. Implement early stopping. <strong>Deliverable:</strong> Working implementations of gradient descent variants with comparative analysis and visualizations of convergence paths. <strong>Verification:</strong> All methods converge to minima. Batch is smoothest path, SGD is noisiest but potentially faster. Mini-batch is intermediate. Visualizations clearly show trade-offs. Common pitfall: not tuning learning rate per method—SGD often needs smaller rate. Success check: Your SGD with mini-batches should converge faster than full batch gradient descent while being more stable than pure SGD. This is why mini-batch is standard in deep learning. Estimated time: 75 minutes. <strong>Resources:</strong> <a href="https://ruder.io/optimizing-gradient-descent/" target="_blank" rel="noopener">Sebastian Ruder: Gradient Descent Optimization</a>, <a href="https://cs231n.github.io/optimization-1/" target="_blank" rel="noopener">CS231n: Optimization</a>' },
            { label: 'Practice optimization problems: finding minima, analyzing convergence', estMinutes: 60, details: '<strong>Action:</strong> Solve 8 optimization problems: (1-2) Find critical points and classify (min/max/saddle) for 2D functions, (3) Verify convexity of f(x)=x² and non-convexity of f(x)=x⁴-2x², (4-5) Apply gradient descent by hand for 3 iterations on simple functions, (6) Analyze why learning rate too large causes divergence, (7) Explain why momentum helps escape local minima, (8) Given loss landscape plot, identify challenges for optimization (flat regions, saddle points). <strong>Boundaries:</strong> Show complete work. For classification problems, compute Hessian and check eigenvalues. For gradient descent, show x_new = x_old - α·∇f step by step. For conceptual problems, explain reasoning clearly. <strong>Deliverable:</strong> 8 complete solutions demonstrating optimization analysis skills. <strong>Verification:</strong> Critical point classifications correct (verify with Hessian). Gradient descent iterations computed accurately. Conceptual explanations demonstrate understanding of optimization challenges. Common pitfall: not checking second derivatives—first derivative zero isn\'t sufficient for minimum. Success check: Explain why learning rate 0.1 might work for one function but cause divergence for another (depends on curvature/Lipschitz constant of gradient). Understanding optimization is crucial—it\'s how models learn. Estimated time: 60 minutes. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives" target="_blank" rel="noopener">Khan Academy: Optimization</a>' }
          ],
          reflectionPrompt: 'How does optimization theory explain neural network training?'
        },
        {
          globalDay: 23,
          week: 4,
          title: 'Calculus for ML: Convexity & Gradient Descent',
          priority: 'HIGH',
          tasks: [
            { label: 'Study convex functions and their properties for optimization', estMinutes: 90, details: '<strong>Action:</strong> Learn about convex functions and why they matter for machine learning optimization. Work through <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/v/second-partial-derivative-test" target="_blank" rel="noopener">Khan Academy multivariable calculus</a> sections on second derivative test and optimization. Study the properties: (1) A function is convex if its second derivative is non-negative, (2) Convex functions have unique global minima, (3) Local minima are global minima for convex functions. Focus on understanding why convexity guarantees gradient descent will find the optimal solution. <strong>Boundaries:</strong> Focus on intuitive understanding of bowl-shaped functions rather than rigorous mathematical proofs. Study 1D examples first (parabolas), then extend to 2D (paraboloids). Don\'t get lost in advanced theorems—build geometric intuition. <strong>Deliverable:</strong> Create notebooks/foundations/day23_convexity.ipynb with visualizations of convex vs non-convex functions, showing how gradient descent behaves differently. Include plots of loss surfaces and gradient descent trajectories. <strong>Verification:</strong> Can you identify whether a function is convex by looking at its graph? Can you explain why MSE loss for linear regression is convex? Common pitfall: confusing convex with concave—remember convex curves upward like ∪. Success check: Plot f(x)=x², f(x)=|x|, and f(x)=sin(x) and identify which are convex. <strong>Resources:</strong> <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Chapter 4</a>, <a href="https://www.youtube.com/watch?v=kcOodzDGV4c" target="_blank" rel="noopener">StatQuest: Gradient Descent</a>, <a href="https://web.stanford.edu/~boyd/cvxbook/" target="_blank" rel="noopener">Boyd & Vandenberghe Convex Optimization</a>' },
            { label: 'Implement gradient descent variants: batch, mini-batch, and stochastic', estMinutes: 75, details: '<strong>Action:</strong> Implement three variants of gradient descent from scratch in Python: (1) Batch GD—compute gradient using entire dataset, (2) Mini-batch GD—compute gradient using subset of data, (3) Stochastic GD (SGD)—compute gradient using single sample. Create a clear comparison showing convergence behavior and computation time for each variant. Use a simple quadratic function f(x)=x² to start, then apply to a linear regression problem. Track and plot the loss curve for each variant across iterations. <strong>Boundaries:</strong> Keep the implementation simple—focus on the algorithmic differences, not performance optimization. Use learning rates between 0.001 and 0.1. Run for at least 100 iterations to see convergence patterns. Document the trade-offs: batch GD is stable but slow, SGD is fast but noisy, mini-batch balances both. <strong>Deliverable:</strong> Add to day23_convexity.ipynb with three functions: <code>batch_gd()</code>, <code>minibatch_gd(batch_size)</code>, and <code>sgd()</code>, each returning the optimization path. Create side-by-side comparison plots. <strong>Verification:</strong> All three methods should converge to the same minimum for a convex function. SGD path should be noisier than batch GD. Mini-batch with size=32 should show intermediate behavior. Common pitfall: using learning rate too high, causing divergence. Success check: Can you explain when to use each variant in practice? <strong>Resources:</strong> <a href="https://ruder.io/optimizing-gradient-descent/" target="_blank" rel="noopener">Sebastian Ruder: Gradient Descent Optimization</a>, <a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">Overview of gradient descent optimization algorithms</a>' },
            { label: 'Create comparison visualization and write notes on GD convergence guarantees', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day23_gd_comparison.png showing three subplots comparing batch, mini-batch, and stochastic gradient descent on the same optimization problem. Plot the loss curve over iterations for each method, showing convergence speed and noise levels. Use different colors and clear legends. Write docs/notes/day23_calculus_ml.md (400-600 words) explaining: (1) Why convexity matters for ML optimization, (2) Convergence guarantees for convex vs non-convex functions, (3) Trade-offs between GD variants (memory, speed, stability), (4) When each GD variant is preferred in practice (batch for small datasets, mini-batch for most cases, SGD for online learning). <strong>Boundaries:</strong> Keep the visualization clean and readable with proper axis labels and titles. In your notes, use concrete examples from ML (training neural networks, linear regression) rather than abstract mathematics. Explain convergence guarantees intuitively: "If the function is convex and we use an appropriate learning rate, GD is guaranteed to reach the global minimum." <strong>Deliverable:</strong> High-quality comparison plot saved at 300 DPI and comprehensive notes document. <strong>Verification:</strong> Visualization should clearly show that batch GD has smooth convergence, SGD is noisy but fast, and mini-batch is in between. Notes should be understandable to someone who completed Weeks 1-3. Common pitfall: making plots too complex with too much information—keep it simple and focused. Success check: Could you use these materials to teach someone about gradient descent? <strong>Resources:</strong> <a href="https://cs231n.github.io/optimization-1/" target="_blank" rel="noopener">CS231n Optimization Notes</a>' }
          ],
          reflectionPrompt: 'Why does convexity guarantee that gradient descent finds the global minimum?'
        },
        {
          globalDay: 24,
          week: 4,
          title: 'Probability: Bayes\' Theorem & Conditional Independence',
          priority: 'HIGH',
          tasks: [
            { label: 'Study Bayes\' theorem and work through conditional probability examples', estMinutes: 90, details: '<strong>Action:</strong> Learn Bayes\' theorem and its applications to machine learning. Complete <a href="https://www.khanacademy.org/math/statistics-probability/probability-library/bayes-theorem/v/bayes-theorem-visualized" target="_blank" rel="noopener">Khan Academy Bayes\' theorem unit</a> including all exercises. Understand the formula: P(A|B) = P(B|A)×P(A)/P(B). Study concrete examples: medical testing (disease given positive test), spam classification (spam given words), and document classification. Focus on understanding prior probability P(A), likelihood P(B|A), evidence P(B), and posterior probability P(A|B). Work through at least 5 practice problems computing posteriors from priors and likelihoods. <strong>Boundaries:</strong> Focus on discrete probability first—continuous distributions come later. Practice with concrete numbers before moving to symbolic manipulation. Make sure you understand why P(A|B) ≠ P(B|A) with clear examples. <strong>Deliverable:</strong> Create notebooks/foundations/day24_bayes.ipynb implementing Bayes\' theorem for spam classification toy example. Show prior, likelihood, evidence, and posterior calculations step-by-step. <strong>Verification:</strong> Can you explain Bayes\' theorem in plain English? Can you solve: Given P(spam)=0.3, P(word|spam)=0.8, P(word|not spam)=0.1, compute P(spam|word)? Common pitfall: confusing P(A|B) with P(B|A)—these are fundamentally different quantities. Success check: Implement Bayes\' rule and verify your result makes intuitive sense. <strong>Resources:</strong> <a href="https://www.youtube.com/watch?v=HZGCoVF3YvM" target="_blank" rel="noopener">3Blue1Brown: Bayes Theorem</a>, <a href="https://arbital.com/p/bayes_rule/?l=1zq" target="_blank" rel="noopener">Arbital: Bayes\' Rule Guide</a>' },
            { label: 'Implement naive Bayes classifier from scratch on text data', estMinutes: 75, details: '<strong>Action:</strong> Build a simple naive Bayes classifier for text classification from scratch. Create a toy dataset with 20 labeled documents (10 spam, 10 not spam) with simple word features. Implement: (1) <code>train()</code> function that computes P(class) and P(word|class) from training data using frequency counts with Laplace smoothing, (2) <code>predict()</code> function that applies Bayes\' theorem to classify new documents by computing P(class|document) for each class. Use log probabilities to avoid numerical underflow (since multiplying many small probabilities causes precision issues). Test your classifier on held-out examples and compute accuracy. <strong>Boundaries:</strong> Keep the vocabulary small (20-30 unique words) to make debugging easier. Use simple tokenization (split on spaces). The "naive" assumption means treating words as independent—P(w1,w2|class) = P(w1|class)×P(w2|class). Don\'t implement advanced features like TF-IDF or n-grams yet. <strong>Deliverable:</strong> Add to day24_bayes.ipynb with working implementation showing training phase (computing probabilities) and prediction phase (classifying new documents). Include accuracy calculation. <strong>Verification:</strong> Classifier should achieve >70% accuracy on your toy dataset. Verify that P(spam|doc) + P(not spam|doc) = 1 for any document. Common pitfall: forgetting Laplace smoothing causes zero probabilities for unseen words. Success check: Can you trace through how the classifier computes P(spam|document) for a specific example? <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/naive_bayes.html" target="_blank" rel="noopener">sklearn Naive Bayes Documentation</a>, <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf" target="_blank" rel="noopener">Jurafsky & Martin: Naive Bayes Classification</a>' },
            { label: 'Write notes on conditional independence and its importance for ML', estMinutes: 60, details: '<strong>Action:</strong> Write docs/notes/day24_probability.md (500-700 words) explaining: (1) Conditional independence—events A and B are conditionally independent given C if P(A,B|C) = P(A|C)×P(B|C), (2) Why naive Bayes assumes conditional independence of features given the class, (3) Why this assumption is "naive" but still works well in practice, (4) Examples where conditional independence holds vs where it\'s violated, (5) How conditional independence simplifies probability calculations—instead of computing P(w1,w2,...,wn|class) which requires exponentially many parameters, we compute ∏P(wi|class) which is linear in vocabulary size. Include concrete examples from your naive Bayes implementation. Discuss why violating independence assumption doesn\'t always hurt performance much. <strong>Boundaries:</strong> Make it intuitive with examples rather than heavy mathematical formalism. Explain why independence assumptions are necessary for tractable computation in high dimensions. Use your spam classifier as a running example. <strong>Deliverable:</strong> Comprehensive notes document with clear explanations and examples from today\'s implementation work. <strong>Verification:</strong> Notes should explain both the mathematical concept and its practical implications for ML. Common pitfall: not connecting mathematical concepts to concrete ML applications. Success check: After reading your notes, could someone explain why naive Bayes is called "naive"? <strong>Resources:</strong> <a href="https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf" target="_blank" rel="noopener">CMU: Naive Bayes and Logistic Regression</a>, <a href="https://alliance.seas.upenn.edu/~cis520/dynamic/2017/wiki/index.php?n=Lectures.NaiveBayes" target="_blank" rel="noopener">UPenn CIS 520: Naive Bayes</a>' }
          ],
          reflectionPrompt: 'Why is the conditional independence assumption useful even when it\'s not strictly true?'
        },
        {
          globalDay: 25,
          week: 4,
          title: 'Statistics: Variance, Entropy & Information Theory',
          priority: 'HIGH',
          tasks: [
            { label: 'Study variance, standard deviation, and their role in ML', estMinutes: 90, details: '<strong>Action:</strong> Deep dive into variance and standard deviation as measures of spread in data. Complete <a href="https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/variance-standard-deviation-population/v/range-variance-and-standard-deviation-as-measures-of-dispersion" target="_blank" rel="noopener">Khan Academy variance and standard deviation unit</a>. Understand: (1) Variance as average squared deviation from mean: Var(X) = E[(X-μ)²], (2) Standard deviation as sqrt(variance), giving interpretable scale, (3) Why we square deviations (to handle positive and negative deviations symmetrically), (4) Sample vs population variance (Bessel\'s correction: n-1 vs n). Study applications to ML: feature scaling requires understanding variance, variance in predictions indicates model uncertainty, low variance in training error but high variance across folds indicates overfitting. Work through at least 5 calculation problems by hand before using NumPy. <strong>Boundaries:</strong> Focus on univariate statistics today—covariance and correlation come later. Understand both the computational formula (easier for hand calculation) and the conceptual formula (easier to understand). Practice with different datasets to build intuition. <strong>Deliverable:</strong> Create notebooks/foundations/day25_variance_entropy.ipynb computing variance and standard deviation for synthetic datasets, showing how they change with data spread. Visualize using histograms and boxplots. <strong>Verification:</strong> Can you compute variance by hand for dataset [1,2,3,4,5]? Can you explain why standard deviation is more interpretable than variance? Common pitfall: forgetting to divide by n-1 for sample variance. Success check: Given two datasets with same mean but different spreads, can you predict which has higher variance before calculating? <strong>Resources:</strong> <a href="https://seeing-theory.brown.edu/basic-probability/index.html" target="_blank" rel="noopener">Seeing Theory: Probability Distributions</a>, <a href="https://www.youtube.com/watch?v=E4HAYd0QnRc" target="_blank" rel="noopener">StatQuest: Variance and Standard Deviation</a>' },
            { label: 'Learn entropy and its connection to information and uncertainty', estMinutes: 75, details: '<strong>Action:</strong> Study entropy as a measure of uncertainty and information content. Learn Shannon entropy: H(X) = -∑P(x)log₂P(x). Understand: (1) Maximum entropy when all outcomes equally likely (uniform distribution), (2) Minimum entropy (zero) when one outcome certain, (3) Entropy measured in bits when using log₂, (4) Connection to surprise—rare events are more surprising and carry more information. Study applications to ML: cross-entropy loss for classification, entropy used in decision tree splits, KL divergence measures distribution difference. Implement entropy calculation for discrete distributions. Test on: uniform distribution (maximum entropy), deterministic distribution (zero entropy), and intermediate cases. <strong>Boundaries:</strong> Focus on discrete entropy today—differential entropy for continuous distributions comes later. Use base-2 logarithm for intuitive bit interpretation. Work with small probability distributions (3-5 outcomes) to build intuition. <strong>Deliverable:</strong> Add to day25_variance_entropy.ipynb implementing <code>entropy(probs)</code> function that takes a probability distribution and returns its entropy. Show entropy for various distributions with bar charts showing probability distributions and their corresponding entropy values. <strong>Verification:</strong> For uniform distribution over n outcomes, entropy should be log₂(n). For deterministic distribution [1,0,0,...], entropy should be 0. For [0.5,0.5], entropy should be 1 bit. Common pitfall: forgetting to handle P(x)=0 cases—use convention that 0×log(0)=0. Success check: Can you explain why a fair coin flip has 1 bit of entropy? <strong>Resources:</strong> <a href="https://www.youtube.com/watch?v=ErfnhcEV1O8" target="_blank" rel="noopener">Khan Academy: Information Entropy</a>, <a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">Visual Information Theory</a>, <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Chapter 3</a>' },
            { label: 'Create visualizations and write notes connecting variance to entropy', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day25_variance_entropy.png showing: (1) Multiple datasets with same mean but different variances visualized as histograms, (2) Entropy values for different probability distributions visualized as bar charts, (3) Side-by-side comparison showing how both variance and entropy measure "spread" or "uncertainty" but in different contexts (continuous vs discrete). Write docs/notes/day25_statistics.md (500-700 words) explaining: (1) How variance and entropy both measure uncertainty but apply to different scenarios, (2) Why these concepts are fundamental to ML (loss functions, regularization, decision criteria), (3) Mathematical connection—both involve expectations of functions of random variables, (4) When to use each metric in practice. Include formulas but emphasize intuitive understanding. <strong>Boundaries:</strong> Keep visualizations clean with clear labels and legends. In notes, connect to specific ML applications: variance in feature scaling and PCA, entropy in decision trees and cross-entropy loss. Use concrete examples rather than abstract theory. <strong>Deliverable:</strong> Publication-quality figure at 300 DPI and comprehensive notes document linking today\'s statistical concepts to machine learning applications. <strong>Verification:</strong> Visualizations should make the concepts immediately clear. Notes should be accessible to someone who completed Weeks 1-3. Common pitfall: treating variance and entropy as unrelated—emphasize they\'re both measures of uncertainty. Success check: Can you explain when you\'d use variance vs entropy in an ML context? <strong>Resources:</strong> <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" target="_blank" rel="noopener">Wikipedia: Information Entropy</a>, <a href="https://www.youtube.com/watch?v=YtebGVx-Fxw" target="_blank" rel="noopener">Mutual Information and Entropy</a>' }
          ],
          reflectionPrompt: 'How do variance and entropy both measure "spread" but in different contexts?'
        },
        {
          globalDay: 26,
          week: 4,
          title: 'Python Data: Pandas & Matplotlib Integration',
          priority: 'HIGH',
          tasks: [
            { label: 'Complete DataCamp "Intermediate Python" pandas section', estMinutes: 90, details: '<strong>Action:</strong> Work through <a href="https://www.datacamp.com/courses/intermediate-python" target="_blank" rel="noopener">DataCamp Intermediate Python</a> focusing on pandas DataFrames for data manipulation. Learn: (1) Creating DataFrames from dictionaries, lists, and CSV files, (2) Selecting columns and rows using bracket notation and .loc/.iloc, (3) Filtering data with boolean indexing, (4) Computing statistics with .describe(), .mean(), .std(), (5) Grouping and aggregating with .groupby(), (6) Handling missing data with .fillna() and .dropna(). Complete all exercises and challenges. Focus on the pandas API patterns—most operations return new DataFrames rather than modifying in place. <strong>Boundaries:</strong> Master the basics before moving to advanced operations like multi-indexing or time series. Practice with the provided datasets in the course. Take notes on common operations you\'ll use frequently. <strong>Deliverable:</strong> Create notebooks/foundations/day26_pandas_plotting.ipynb demonstrating each major pandas operation with the Iris dataset or another simple dataset. Include examples of selection, filtering, grouping, and statistical summaries. <strong>Verification:</strong> Can you load a CSV, filter rows based on conditions, compute group statistics, and save results? Can you explain the difference between .loc and .iloc? Common pitfall: confusion between Series and DataFrame—a column is a Series, multiple columns form a DataFrame. Success check: Load iris dataset, filter to setosa species, compute mean petal length. <strong>Resources:</strong> <a href="https://pandas.pydata.org/docs/user_guide/10min.html" target="_blank" rel="noopener">Pandas 10 Minutes Tutorial</a>, <a href="https://www.youtube.com/watch?v=vmEHCJofslg" target="_blank" rel="noopener">Data School: Pandas Intro</a>' },
            { label: 'Create comprehensive data visualization examples with matplotlib', estMinutes: 75, details: '<strong>Action:</strong> Build a comprehensive visualization notebook demonstrating key matplotlib plot types useful for ML: (1) Scatter plots for feature relationships with color-coding by class, (2) Histograms for distribution visualization, (3) Box plots for comparing distributions across groups, (4) Line plots for showing trends or training curves, (5) Heatmaps for correlation matrices using plt.imshow() or seaborn, (6) Subplots for comparing multiple visualizations. For each plot type, show both basic usage and customization (labels, titles, legends, colors, styling). Use a real dataset like Iris or Boston Housing to make examples concrete. <strong>Boundaries:</strong> Focus on plots most relevant to ML exploratory data analysis and model evaluation. Use the pyplot interface (plt.plot, plt.scatter) for consistency. Add proper labels, titles, and legends to every plot—publication-ready quality. <strong>Deliverable:</strong> Extend day26_pandas_plotting.ipynb with a "Visualization Gallery" section showing all major plot types with well-commented code. Each plot should demonstrate a real data analysis insight. <strong>Verification:</strong> All plots should have clear labels and titles. Notebook should run top-to-bottom without errors. Can you create any plot type from memory? Common pitfall: creating plots without labels or titles—they\'re meaningless without context. Success check: Create a scatter plot showing sepal length vs width colored by species with legend. <strong>Resources:</strong> <a href="https://matplotlib.org/stable/gallery/index.html" target="_blank" rel="noopener">Matplotlib Gallery</a>, <a href="https://www.python-graph-gallery.com/" target="_blank" rel="noopener">Python Graph Gallery</a>' },
            { label: 'Write docs/notes/day26_python_data.md on pandas and plotting best practices', estMinutes: 60, details: '<strong>Action:</strong> Write comprehensive notes (500-700 words) documenting: (1) Key pandas operations you\'ll use frequently in ML workflows (loading data, filtering, grouping, handling missing values), (2) Common pandas gotchas (copy vs view, chained indexing warnings, Series vs DataFrame), (3) Best practices for exploratory data analysis with visualizations, (4) When to use each plot type (scatter for relationships, histograms for distributions, box plots for comparisons, line plots for sequences), (5) Tips for creating publication-quality figures (consistent colors, clear labels, appropriate figure sizes, saving with high DPI). Include code snippets showing common patterns. Reference your notebook examples. <strong>Boundaries:</strong> Focus on practical workflow tips rather than exhaustive API documentation. Include warnings about common mistakes. Organize as a quick reference guide you can revisit. <strong>Deliverable:</strong> Well-structured notes document that serves as your personal pandas/matplotlib reference guide with practical examples and gotchas. <strong>Verification:</strong> Notes should be scannable with clear sections. Include both code examples and explanatory text. Common pitfall: writing notes that are too abstract—include concrete code snippets. Success check: Could you use these notes as a cheat sheet during ML projects? <strong>Resources:</strong> <a href="https://pandas.pydata.org/docs/user_guide/gotchas.html" target="_blank" rel="noopener">Pandas Gotchas</a>, <a href="https://realpython.com/pandas-python-explore-dataset/" target="_blank" rel="noopener">Real Python: Pandas Tutorial</a>' }
          ],
          reflectionPrompt: 'How do pandas and matplotlib streamline ML data analysis workflows?'
        },
        {
          globalDay: 27,
          week: 4,
          title: 'ML Math Integration: Bringing It All Together',
          priority: 'HIGH',
          tasks: [
            { label: 'Implement linear regression from scratch using matrix operations', estMinutes: 90, details: '<strong>Action:</strong> Build a complete linear regression implementation using only NumPy, applying concepts from Weeks 1-4. Implement: (1) Normal equation solution: θ = (XᵀX)⁻¹Xᵀy using np.linalg.inv() and matrix multiplication, (2) Prediction function: ŷ = Xθ, (3) MSE loss calculation: (1/n)∑(yᵢ-ŷᵢ)², (4) R² score: 1 - SS_res/SS_tot. Create synthetic data using y = 3x + 2 + noise to test your implementation. Verify that your learned parameters are close to the true parameters [2, 3] (intercept, slope). This exercise connects linear algebra (matrix operations), calculus (minimizing MSE), and statistics (R² evaluation). <strong>Boundaries:</strong> Focus on the matrix formulation—this generalizes to multiple features naturally. Add bias term by augmenting X with a column of ones. Use at least 100 samples for stable learning. Don\'t use sklearn yet—implement from mathematical first principles. <strong>Deliverable:</strong> Create notebooks/foundations/day27_ml_math_integration.ipynb with complete implementation including data generation, training (solving normal equation), prediction, and evaluation. Plot true vs predicted values. <strong>Verification:</strong> Your implementation should recover parameters close to ground truth on synthetic data. R² should be >0.95 on clean data. Common pitfall: forgetting to add bias/intercept term—X needs column of ones. Success check: Train on y=2+3x+noise, verify learned parameters ≈ [2,3]. <strong>Resources:</strong> <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Andrew Ng ML Course Week 2</a>, <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Chapter 5</a>' },
            { label: 'Derive and implement gradient descent for linear regression', estMinutes: 75, details: '<strong>Action:</strong> Derive the gradient descent update rule for linear regression and implement it from scratch. Derive: (1) Loss function L(θ) = (1/n)∑(yᵢ-θᵀxᵢ)², (2) Gradient ∇L = (2/n)Xᵀ(Xθ-y), (3) Update rule θ := θ - α∇L. Implement gradient descent with: learning rate α=0.01, 1000 iterations, tracking loss at each step. Compare with normal equation solution—both should converge to same parameters. Plot the loss curve showing convergence over iterations. This connects calculus (derivatives), optimization (gradient descent), and linear algebra (vector operations). Test on the same synthetic dataset as the normal equation approach. <strong>Boundaries:</strong> Start with batch gradient descent using full dataset. Implement vectorized operations—avoid Python loops over samples. Choose learning rate carefully: too large causes divergence, too small takes forever. Try α ∈ {0.001, 0.01, 0.1} and observe convergence behavior. <strong>Deliverable:</strong> Add to day27_ml_math_integration.ipynb with gradient descent implementation. Include loss curve plot showing convergence and comparison with normal equation parameters. <strong>Verification:</strong> GD and normal equation should yield same final parameters (within numerical tolerance). Loss should decrease monotonically if learning rate is appropriate. Common pitfall: forgetting to initialize parameters (start with zeros or small random values). Success check: Can you explain why GD and normal equation give same answer? <strong>Resources:</strong> <a href="https://www.youtube.com/watch?v=sDv4f4s2SB8" target="_blank" rel="noopener">Andrew Ng: Gradient Descent</a>, <a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">Overview of optimization algorithms</a>' },
            { label: 'Create comprehensive comparison and write integration notes', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day27_ml_math.png showing: (1) Scatter plot of data with fitted line, (2) Loss curve for gradient descent showing convergence, (3) Table comparing normal equation vs gradient descent (parameters learned, computation time, convergence behavior). Write docs/notes/day27_ml_math.md (600-800 words) explaining: (1) How linear algebra, calculus, and statistics come together in linear regression, (2) When to use normal equation (small datasets, exact solution) vs gradient descent (large datasets, iterative approximation), (3) The connection between mathematical concepts and practical ML: matrix multiplication for efficient computation, derivatives for optimization, variance/R² for evaluation, (4) Key insights from Weeks 1-4 that enable understanding ML algorithms. This is your integration document showing how foundational math enables machine learning. <strong>Boundaries:</strong> Make the visualization comprehensive but not cluttered. In notes, emphasize connections between mathematical concepts and ML practice. Use today\'s implementation as concrete examples throughout. <strong>Deliverable:</strong> High-quality figure at 300 DPI and comprehensive integration notes tying together four weeks of foundational mathematics. <strong>Verification:</strong> Notes should demonstrate deep understanding of how math concepts enable ML. Should be your best synthesis document yet. Common pitfall: treating concepts as isolated—emphasize their interconnections. Success check: Could you use this to explain to someone why math matters for ML? <strong>Resources:</strong> <a href="https://mml-book.github.io/" target="_blank" rel="noopener">Mathematics for Machine Learning Book</a>' }
          ],
          reflectionPrompt: 'How do matrix operations, derivatives, and statistical measures come together in linear regression?'
        },
        {
          globalDay: 28,
          week: 4,
          title: 'Week 4 Review: Advanced Math for ML',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review Week 4 materials: calculus, probability, statistics, data tools', estMinutes: 75, details: '<strong>Action:</strong> Systematically review all Week 4 content from Days 23-27: (1) Convexity and gradient descent variants—can you explain when each GD variant is appropriate? (2) Bayes\' theorem and conditional independence—can you derive posteriors from priors? (3) Variance and entropy—can you compute both by hand? (4) Pandas and matplotlib—can you perform common data operations from memory? (5) ML math integration—can you implement linear regression from scratch? For each topic, open the relevant notebook, re-run all cells, and verify understanding. Identify any gaps or confusing concepts. <strong>Boundaries:</strong> This is active review requiring practice, not passive reading. Work through 2-3 practice problems per topic without looking at solutions first. Spend about 15 minutes per day reviewing notebooks and notes. Focus on concepts that felt unclear during the week. <strong>Deliverable:</strong> Create docs/reviews/week04_review.md listing: (1) Concepts mastered this week, (2) Concepts that need more practice, (3) Specific questions for further study. Include self-assessment ratings (1-5) for each major topic. <strong>Verification:</strong> You should feel confident explaining any Week 4 concept. Can you solve a random problem from each area? Common pitfall: passive review without testing understanding. Success check: Pick a random concept from Week 4—can you explain it clearly without notes? <strong>Resources:</strong> Your Week 4 notebooks and notes documents' },
            { label: 'Complete practice exercises testing Week 4 understanding', estMinutes: 60, details: '<strong>Action:</strong> Work through focused practice exercises testing Week 4 concepts: (1) Implement mini-batch GD on a new synthetic dataset with learning rate tuning, (2) Solve a Bayes\' theorem problem: given medical test accuracy and disease prevalence, compute posterior probability, (3) Calculate entropy for probability distribution [0.7, 0.2, 0.1], (4) Use pandas to load a dataset, filter based on conditions, group by category, and compute aggregated statistics, (5) Implement linear regression using both normal equation and gradient descent, compare results. Set a 60-minute timer and see how many you can complete without looking up solutions. Then review and correct any mistakes. <strong>Boundaries:</strong> Challenge yourself with new problems, not the same examples from the week. Use different datasets and parameter values. If stuck for more than 10 minutes, review relevant notes then try again. <strong>Deliverable:</strong> Add to week04_review.md with your solutions to practice problems, noting which were easy vs challenging and why. <strong>Verification:</strong> You should complete at least 4/5 exercises correctly. Errors indicate topics needing more review. Common pitfall: giving up too quickly—struggle builds understanding. Success check: Did you complete the exercises without referring to previous code? <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/statistics-probability" target="_blank" rel="noopener">Khan Academy practice problems</a>' },
            { label: 'Write weekly log summarizing Week 4 progress and insights', estMinutes: 30, details: '<strong>Action:</strong> Create docs/logs/week04_log.md documenting: (1) What you accomplished—list major deliverables (notebooks, implementations, visualizations), (2) Key insights—what were the "aha" moments? How do concepts connect? (3) Challenges faced—what was difficult and how did you overcome it? (4) Time estimation—compare estimated vs actual time spent on different topics, (5) Looking ahead—what from Week 4 connects to upcoming topics? What needs reinforcement? Write 400-600 words total. Be honest about struggles and victories. This log is your learning journal for tracking progress and patterns. <strong>Boundaries:</strong> Make it personal and reflective, not just a task checklist. Identify learning patterns: do you learn better from videos or implementation? Morning or evening study? With or without music? Note what worked well to optimize future weeks. <strong>Deliverable:</strong> Thoughtful weekly log that captures both technical progress and meta-learning insights. <strong>Verification:</strong> Log should show genuine reflection, not just summaries. Should identify specific insights and patterns. Common pitfall: superficial summaries without reflection. Success check: Can you identify one concrete way to improve your learning process for Week 5? Reading this log in 6 months, will it remind you of your learning journey? <strong>Resources:</strong> <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Farnam Street: Learning</a>' }
          ],
          reflectionPrompt: 'What were the most important connections you made between different mathematical concepts this week?'
        },
        {
          globalDay: 29,
          week: 5,
          title: 'Linear Regression: Normal Equation Deep Dive',
          priority: 'HIGH',
          tasks: [
            { label: 'Derive normal equation solution for linear regression from first principles', estMinutes: 90, details: '<strong>Action:</strong> Derive the normal equation θ = (XᵀX)⁻¹Xᵀy from scratch starting with the loss function. Begin with MSE loss L(θ) = (1/n)||Xθ - y||², take the gradient ∇L = (2/n)Xᵀ(Xθ - y), set it to zero to find the minimum, and algebraically solve for θ. Understand each step: (1) Why we minimize squared error, (2) Why setting gradient to zero finds the minimum (first-order optimality condition), (3) Why XᵀX must be invertible (full rank assumption), (4) Geometric interpretation: normal equation finds θ that projects y onto column space of X. Work through the derivation multiple times until you can reproduce it from memory. Study when the solution fails (singular XᵀX when features are linearly dependent). <strong>Boundaries:</strong> Focus on the matrix calculus derivation—take time to understand each step rather than memorizing. Use <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthogonal-projections/v/linear-alg-visualizing-a-projection-onto-a-plane" target="_blank" rel="noopener">Khan Academy projection videos</a> for geometric intuition. Don\'t worry about computational efficiency yet, focus on mathematical correctness. <strong>Deliverable:</strong> Create notebooks/foundations/day29_normal_equation.ipynb with step-by-step derivation in markdown cells, including the projection interpretation. <strong>Verification:</strong> Can you derive the normal equation on paper without reference? Can you explain why it represents a projection? Common pitfall: treating as black-box formula without understanding derivation. Success check: Derive it from scratch, then verify matches textbook formulation. <strong>Resources:</strong> <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Ch 5</a>, <a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf" target="_blank" rel="noopener">Matrix Algebra for Linear Regression</a>, <a href="https://www.youtube.com/watch?v=sDv4f4s2SB8" target="_blank" rel="noopener">Andrew Ng: Normal Equation</a>' },
            { label: 'Implement linear regression with normal equation on multiple datasets', estMinutes: 75, details: '<strong>Action:</strong> Implement a complete LinearRegression class from scratch using normal equation. Create class with: (1) <code>fit(X, y)</code> method computing θ = (XᵀX)⁻¹Xᵀy, (2) <code>predict(X)</code> method computing ŷ = Xθ, (3) <code>score(X, y)</code> method computing R², (4) Error handling for singular XᵀX (use np.linalg.lstsq as fallback). Test on three datasets: (1) Synthetic 1D (y = 2x + 3 + noise), (2) Synthetic 2D (y = 2x₁ + 3x₂ + 1 + noise), (3) Real dataset from sklearn.datasets (Boston housing or California housing). For each dataset, visualize: fitted line/plane, residuals, predicted vs actual values. Compare your implementation against sklearn.linear_model.LinearRegression to verify correctness. <strong>Boundaries:</strong> Use pure NumPy for implementation—no sklearn for the core algorithm. Add proper docstrings and comments. Handle edge cases: empty data, single sample, perfect fit. Feature scaling is not required yet (normal equation works regardless). <strong>Deliverable:</strong> Complete LinearRegression class in day29_normal_equation.ipynb with three worked examples showing fits, predictions, and R² scores >0.90. <strong>Verification:</strong> Your implementation should match sklearn results within numerical tolerance. All three datasets should show good fits. Common pitfall: forgetting to add intercept column—augment X with column of ones. Success check: R² >0.85 on all datasets; predictions match sklearn. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares" target="_blank" rel="noopener">sklearn LinearRegression docs</a>, <a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">Hands-On ML Chapter 4</a>' },
            { label: 'Analyze computational complexity and create comparison with sklearn', estMinutes: 60, details: '<strong>Action:</strong> Analyze the computational complexity of normal equation method and compare with sklearn. Study: (1) Computing XᵀX is O(nd²) where n=samples, d=features, (2) Inverting (XᵀX)⁻¹ is O(d³), (3) Total complexity O(nd² + d³), dominated by d³ for d>n or nd² for n>d. Time your implementation vs sklearn.linear_model.LinearRegression on datasets of varying sizes: (100, 10), (1000, 10), (100, 100), (1000, 100), (10000, 10). Plot timing comparison as function of n and d. Write docs/notes/day29_normal_equation.md (500-700 words) explaining: (1) When normal equation is appropriate (small to medium datasets, d<10000), (2) When gradient descent is better (large datasets, many features), (3) Numerical stability considerations (condition number of XᵀX), (4) Why sklearn is faster (optimized BLAS/LAPACK libraries). <strong>Boundaries:</strong> Use time.time() or timeit for timing comparisons. Run each timing experiment multiple times and average. Make plots clear with proper axis labels. In notes, connect back to computational complexity theory from CS. <strong>Deliverable:</strong> Timing comparison plots and comprehensive notes document comparing normal equation with gradient descent trade-offs. <strong>Verification:</strong> Timing results should show expected O(nd² + d³) scaling. Notes should provide practical guidance. Common pitfall: comparing single runs—use averaged timings for reliability. Success check: Can you advise when to use normal equation vs gradient descent? <strong>Resources:</strong> <a href="https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation-noninvertibility" target="_blank" rel="noopener">Andrew Ng: Normal Equation Complexity</a>' }
          ],
          reflectionPrompt: 'Why does the normal equation give an exact solution while gradient descent gives an approximate one?'
        },
        {
          globalDay: 30,
          week: 5,
          title: 'Linear Regression: Gradient Descent Implementation',
          priority: 'HIGH',
          tasks: [
            { label: 'Implement batch gradient descent for linear regression with convergence monitoring', estMinutes: 90, details: '<strong>Action:</strong> Build a complete gradient descent implementation for linear regression from scratch. Implement: (1) Loss function <code>compute_loss(X, y, theta)</code> returning MSE, (2) Gradient function <code>compute_gradient(X, y, theta)</code> returning ∇L = (2/n)Xᵀ(Xθ-y), (3) Training loop <code>fit_gd(X, y, alpha, iterations)</code> that iteratively updates θ := θ - α∇L and tracks loss at each step, (4) Convergence detection: stop when |loss_new - loss_old| < tolerance=1e-6 or gradient norm < 1e-6. Test with multiple learning rates α ∈ {0.0001, 0.001, 0.01, 0.1, 1.0} to observe convergence behavior. Plot loss curves showing how different learning rates affect convergence: too small converges slowly, too large diverges, optimal balances speed and stability. <strong>Boundaries:</strong> Use vectorized NumPy operations—no Python loops over samples. Initialize parameters to zeros or small random values. Implement early stopping based on convergence criteria. Normalize features using z-score (subtract mean, divide by std) to improve convergence. <strong>Deliverable:</strong> Create notebooks/foundations/day30_gradient_descent.ipynb with complete GD implementation, convergence plots for different learning rates, and comparison with normal equation solution. <strong>Verification:</strong> GD with appropriate learning rate should converge to same solution as normal equation. Loss should decrease monotonically. Common pitfall: divergence due to learning rate too high—implement gradient checking to debug. Success check: Find learning rate that converges in <1000 iterations matching normal equation parameters. <strong>Resources:</strong> <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Andrew Ng ML Course</a>, <a href="https://ruder.io/optimizing-gradient-descent/" target="_blank" rel="noopener">Sebastian Ruder: GD Optimization</a>, <a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">Overview of optimization algorithms (arxiv)</a>' },
            { label: 'Implement mini-batch and stochastic gradient descent variants', estMinutes: 75, details: '<strong>Action:</strong> Extend your gradient descent implementation with mini-batch and stochastic variants. Implement: (1) <code>fit_minibatch_gd(X, y, alpha, batch_size, epochs)</code> that randomly samples batches and updates parameters after each batch, (2) <code>fit_sgd(X, y, alpha, epochs)</code> that updates after each single sample (batch_size=1), (3) Learning rate scheduling: reduce learning rate over time using <code>alpha_t = alpha_0 / (1 + decay_rate * t)</code> to improve convergence. Compare all three variants (batch, mini-batch with size=32, SGD) on the same dataset tracking: convergence speed (time to reach loss threshold), final loss value, parameter trajectory (plot path in parameter space for 2D case). Visualize loss curves showing batch GD is smooth, SGD is noisy but explores more, mini-batch balances both. <strong>Boundaries:</strong> Implement random shuffling at start of each epoch for mini-batch/SGD. Use consistent random seed for reproducibility. Try batch sizes [1, 8, 32, 128, full_dataset]. Test on dataset with n>1000 samples to see clear differences. <strong>Deliverable:</strong> Extend day30_gradient_descent.ipynb with mini-batch and SGD implementations. Create comparison plots showing loss curves and convergence times for all variants. <strong>Verification:</strong> All variants should converge to similar final parameters. SGD should be noisier but potentially faster. Mini-batch should show intermediate behavior. Common pitfall: forgetting to shuffle data—this causes poor convergence. Success check: Mini-batch GD with size=32 converges faster than batch GD in wall-clock time on large dataset. <strong>Resources:</strong> <a href="https://www.deeplearningbook.org/contents/optimization.html" target="_blank" rel="noopener">Deep Learning Book Ch 8</a>, <a href="https://cs231n.github.io/optimization-1/" target="_blank" rel="noopener">CS231n Optimization Notes</a>' },
            { label: 'Write comprehensive notes comparing optimization methods', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day30_gd_comparison.png showing: (1) Loss curves for batch/mini-batch/SGD over iterations, (2) Parameter trajectory visualization in 2D showing optimization paths, (3) Timing comparison bar chart, (4) Table summarizing pros/cons of each method. Write docs/notes/day30_gradient_descent.md (600-800 words) explaining: (1) Trade-offs between gradient accuracy (batch best) and computation speed (SGD fastest), (2) Why mini-batch is the practical choice (GPU efficiency with batches, faster than batch GD, more stable than SGD), (3) Learning rate selection strategies (grid search, adaptive methods preview), (4) Convergence guarantees: convex functions guarantee convergence with proper learning rate, non-convex only find local minima, (5) When to use which method: batch for small data with GPU, mini-batch for most cases, SGD for online learning. Connect to Week 4 convexity concepts. <strong>Boundaries:</strong> Make visualizations publication-quality with clear legends and labels. In notes, provide actionable guidance for practitioners. Include pseudo-code for each GD variant. Discuss both theory (convergence guarantees) and practice (wall-clock time). <strong>Deliverable:</strong> High-quality comparison figure at 300 DPI and comprehensive notes serving as your optimization reference guide. <strong>Verification:</strong> Visualizations should clearly show the trade-offs. Notes should be useful reference material. Common pitfall: presenting results without interpretation—explain what patterns mean. Success check: Could you use these materials to teach someone about gradient descent variants? <strong>Resources:</strong> <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">Toronto CSC321: Optimization</a>' }
          ],
          reflectionPrompt: 'Why is mini-batch gradient descent often the best practical choice?'
        },
        {
          globalDay: 31,
          week: 5,
          title: 'Regularization: L2 Ridge Regression',
          priority: 'HIGH',
          tasks: [
            { label: 'Study regularization theory and L2 penalty mathematics', estMinutes: 90, details: '<strong>Action:</strong> Learn L2 regularization (Ridge regression) theory and mathematics. Study: (1) Why regularization is needed—prevents overfitting by penalizing large weights, (2) L2 penalty adds λ∑θᵢ² to loss function: L(θ) = MSE + λ||θ||², (3) Geometric interpretation: L2 constraint θᵀθ ≤ t creates circular constraint region, solution is tangent point, (4) Bayesian interpretation: L2 prior assumes θ ~ N(0, σ²I), (5) How λ controls trade-off: λ=0 is unregularized OLS, λ→∞ drives weights toward zero. Study the closed-form solution: θ_ridge = (XᵀX + λI)⁻¹Xᵀy. Understand that adding λI to XᵀX ensures invertibility even when XᵀX is singular. Complete <a href="https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/squared-error-of-regression-line" target="_blank" rel="noopener">Khan Academy regression unit</a> and study overfitting examples. <strong>Boundaries:</strong> Focus on L2/Ridge today—L1/Lasso comes later. Understand both the optimization view (penalty) and constraint view (Lagrange multipliers connect them). Study the bias-variance trade-off: regularization adds bias but reduces variance. <strong>Deliverable:</strong> Create notebooks/foundations/day31_regularization.ipynb with mathematical derivation of ridge solution and geometric visualizations. <strong>Verification:</strong> Can you derive θ_ridge from penalized loss? Can you explain why λI fixes singular XᵀX? Common pitfall: not understanding the λ hyperparameter—it controls regularization strength. Success check: Explain why Ridge never sets coefficients exactly to zero (unlike Lasso). <strong>Resources:</strong> <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Ch 7</a>, <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" target="_blank" rel="noopener">Elements of Statistical Learning Ch 3</a>, <a href="https://www.youtube.com/watch?v=Q81RR3yKn30" target="_blank" rel="noopener">StatQuest: Ridge Regression</a>' },
            { label: 'Implement Ridge regression with cross-validation for λ selection', estMinutes: 75, details: '<strong>Action:</strong> Implement Ridge regression from scratch and use cross-validation to select optimal λ. Implement: (1) <code>RidgeRegression</code> class with <code>fit(X, y, lambda)</code> computing θ = (XᵀX + λI)⁻¹Xᵀy, (2) <code>cross_validate(X, y, lambdas)</code> using k-fold CV (k=5) to evaluate each λ, (3) <code>plot_regularization_path()</code> showing how coefficients shrink as λ increases. Test on synthetic data where overfitting is obvious: generate high-dimensional data (d=50 features) with small sample size (n=30) where only 5 features are truly predictive. Without regularization, model overfits badly. Show that Ridge with optimal λ (selected via CV) generalizes much better. Compare train vs validation error for different λ values—optimal λ minimizes validation error. <strong>Boundaries:</strong> Standardize features before applying Ridge—regularization is scale-dependent. Try λ ∈ {0, 0.01, 0.1, 1, 10, 100, 1000}. Use sklearn.model_selection.KFold for consistent splits. Compare your implementation with sklearn.linear_model.Ridge. <strong>Deliverable:</strong> Complete Ridge implementation in day31_regularization.ipynb with cross-validation analysis showing optimal λ selection and regularization path plot. <strong>Verification:</strong> Ridge with optimal λ should significantly outperform unregularized regression on validation set. Coefficients should shrink toward zero as λ increases. Common pitfall: not standardizing features—L2 penalty becomes unfair to features with different scales. Success check: Show train error increases but validation error decreases with appropriate λ. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression" target="_blank" rel="noopener">sklearn Ridge docs</a>, <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Andrew Ng: Regularization</a>' },
            { label: 'Write notes on bias-variance trade-off and regularization', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day31_ridge_analysis.png showing: (1) Train vs validation error as function of λ (U-shaped validation curve), (2) Regularization path showing coefficient shrinkage, (3) Comparison of unregularized vs optimal Ridge predictions on test set. Write docs/notes/day31_regularization.md (600-800 words) explaining: (1) Bias-variance trade-off: unregularized has low bias but high variance (overfits), heavy regularization has high bias but low variance (underfits), optimal λ balances both, (2) Why L2 regularization improves generalization even though it increases training error, (3) How to select λ using cross-validation rather than test set (would leak information), (4) Connection to Bayesian priors—L2 assumes Gaussian prior on weights, (5) When regularization is critical: high-dimensional data, small sample sizes, correlated features. Connect to Week 4 statistics concepts. <strong>Boundaries:</strong> Make visualizations show the U-shaped curve for validation error clearly. In notes, emphasize the generalization benefit—regularization helps models perform better on unseen data. Explain why we never tune hyperparameters on test set. <strong>Deliverable:</strong> Publication-quality figures at 300 DPI and comprehensive notes explaining regularization theory and practice. <strong>Verification:</strong> Notes should clearly explain bias-variance trade-off with concrete examples. Visualizations should demonstrate regularization benefit. Common pitfall: conflating training and validation error—regularization helps validation not training. Success check: Can you explain why adding bias can improve overall model performance? <strong>Resources:</strong> <a href="https://www.youtube.com/watch?v=EuBBz3bI-aA" target="_blank" rel="noopener">StatQuest: Bias-Variance Trade-off</a>, <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" target="_blank" rel="noopener">Understanding Bias-Variance Trade-off</a>' }
          ],
          reflectionPrompt: 'How does adding bias through regularization improve model performance?'
        },
        {
          globalDay: 32,
          week: 5,
          title: 'PCA from Scratch: SVD & Eigendecomposition',
          priority: 'HIGH',
          tasks: [
            { label: 'Study PCA theory: variance maximization and dimensionality reduction', estMinutes: 90, details: '<strong>Action:</strong> Learn Principal Component Analysis (PCA) theory from multiple perspectives. Study: (1) Goal: find orthogonal directions of maximum variance in data, (2) First PC is direction with highest variance, second PC is orthogonal with next highest variance, etc., (3) Mathematical formulation: maximize variance σ² = vᵀΣv subject to ||v||=1 where Σ is covariance matrix, (4) Solution via eigendecomposition: PCs are eigenvectors of covariance matrix, ordered by eigenvalue magnitude, (5) Dimensionality reduction: project data onto top k PCs to reduce from d to k dimensions while preserving most variance. Study the relationship between SVD and PCA: if X = UΣVᵀ is SVD of centered data, then VᵀX gives PC scores and V columns are PC directions. Watch <a href="https://www.youtube.com/watch?v=FgakZw6K1QQ" target="_blank" rel="noopener">StatQuest PCA video</a> for intuitive understanding. Complete <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors" target="_blank" rel="noopener">Khan Academy eigenvalue unit</a>. <strong>Boundaries:</strong> Understand both covariance matrix eigendecomposition and SVD approaches—they\'re equivalent but SVD is more numerically stable. Study the explained variance ratio: eigenvalue_i / sum(eigenvalues). Practice with 2D examples first to build geometric intuition. <strong>Deliverable:</strong> Create notebooks/foundations/day32_pca.ipynb with PCA theory, derivations, and 2D geometric visualization showing PC directions on scatter plot. <strong>Verification:</strong> Can you explain why PCs are orthogonal? Why they maximize variance? Common pitfall: forgetting to center data before PCA—always subtract mean first. Success check: Explain connection between covariance eigenvectors and PCA directions. <strong>Resources:</strong> <a href="https://arxiv.org/abs/1404.1100" target="_blank" rel="noopener">Tutorial on PCA (arxiv)</a>, <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Ch 2</a>, <a href="https://www.youtube.com/watch?v=g-Hb26agBFg" target="_blank" rel="noopener">3Blue1Brown: Eigenvectors and Eigenvalues</a>' },
            { label: 'Implement PCA using both eigendecomposition and SVD methods', estMinutes: 75, details: '<strong>Action:</strong> Implement PCA from scratch using two approaches. Method 1 (Covariance eigendecomposition): (1) Center data: X_centered = X - mean(X), (2) Compute covariance: Σ = (1/n)X_centeredᵀX_centered, (3) Eigendecompose: eigenvalues, eigenvectors = eig(Σ), (4) Sort by eigenvalue and select top k eigenvectors, (5) Project: X_pca = X_centered @ eigenvectors[:k]. Method 2 (SVD): (1) Center data, (2) SVD: U, s, Vt = svd(X_centered), (3) PCs are Vt rows (or V columns), (4) Project: X_pca = U[:, :k] @ diag(s[:k]). Implement <code>PCA</code> class with <code>fit(X, n_components)</code> and <code>transform(X)</code> methods. Test on Iris dataset (4D→2D) and visualize projected data colored by species. Compare your implementation with sklearn.decomposition.PCA—results should match within numerical precision. <strong>Boundaries:</strong> Use NumPy linear algebra functions: np.linalg.eig, np.linalg.svd. Handle numerical issues: eigenvalues might have tiny negative values due to numerical precision—clip to zero. Store explained variance ratio for each component. Verify PCs are orthonormal: PCᵀPC = I. <strong>Deliverable:</strong> Complete PCA implementation in day32_pca.ipynb with both methods, comparison, and Iris visualization showing clear species separation in 2D. <strong>Verification:</strong> Both methods should give identical results (up to sign flip). Explained variance should sum to total variance. Common pitfall: not centering data first—PCA assumes zero-mean data. Success check: Top 2 PCs on Iris should explain >95% variance; species should cluster in 2D plot. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/decomposition.html#pca" target="_blank" rel="noopener">sklearn PCA docs</a>, <a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">Hands-On ML Chapter 8</a>' },
            { label: 'Create reconstruction demonstration and write PCA notes', estMinutes: 60, details: '<strong>Action:</strong> Implement data reconstruction from PC space to show information preservation. Add <code>inverse_transform(X_pca)</code> to your PCA class: X_reconstructed = X_pca @ PCs.T + mean. Demonstrate reconstruction quality vs number of PCs: use MNIST digits (if available) or face images, project to k={1,2,5,10,20,50} dimensions, reconstruct, and visualize original vs reconstruction. Plot reconstruction error (MSE) vs number of components—should decrease as k increases. Generate artifacts/day32_pca_reconstruction.png showing original images and reconstructions with different k values. Write docs/notes/day32_pca.md (600-800 words) explaining: (1) PCA as lossy compression—trade-off between dimensionality and information loss, (2) How to choose k: elbow method on explained variance plot, or set threshold like 95% variance explained, (3) Applications: visualization (3D→2D), noise reduction, feature extraction, compression, (4) Limitations: PCA is linear (can\'t capture nonlinear structure), sensitive to outliers, assumes directions of max variance are most important, (5) When to use PCA vs other methods (t-SNE for visualization, autoencoders for nonlinear). <strong>Boundaries:</strong> Make reconstruction visualization compelling—show clear quality degradation with too few PCs. In notes, balance theory with practical guidance. Discuss both benefits and limitations. <strong>Deliverable:</strong> Reconstruction demo, visualization at 300 DPI, and comprehensive PCA notes with practical guidance. <strong>Verification:</strong> Reconstruction error should decrease monotonically with k. Notes should explain when to use PCA. Common pitfall: using PCA blindly—it\'s not always the right choice. Success check: Can you choose k appropriately for a new dataset? <strong>Resources:</strong> <a href="https://www.youtube.com/watch?v=HMOI_lkzW08" target="_blank" rel="noopener">StatQuest: PCA Step-by-Step</a>' }
          ],
          reflectionPrompt: 'Why are principal components orthogonal, and why does this matter?'
        },
        {
          globalDay: 33,
          week: 5,
          title: 'Probability Distributions & Hypothesis Testing',
          priority: 'HIGH',
          tasks: [
            { label: 'Study common probability distributions and their ML applications', estMinutes: 90, details: '<strong>Action:</strong> Learn key probability distributions used in machine learning. Study: (1) Normal/Gaussian: N(μ,σ²), bell curve, parameterized by mean and variance, Central Limit Theorem says sums converge to normal, (2) Bernoulli: p(x=1)=p, p(x=0)=1-p, single binary trial like coin flip, (3) Binomial: sum of n Bernoulli trials, counts successes, (4) Uniform: all values equally likely in range, (5) Exponential: models time between events in Poisson process. For each, understand: PDF/PMF formula, mean, variance, when to use in ML (e.g., Gaussian for continuous features, Bernoulli for binary classification output, Binomial for counting). Complete <a href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library" target="_blank" rel="noopener">Khan Academy probability distributions unit</a>. Study maximum likelihood estimation (MLE) for fitting distribution parameters: find parameters that maximize P(data|parameters). For Gaussian, MLE gives sample mean and variance. <strong>Boundaries:</strong> Focus on the five distributions above—others come later. Practice computing probabilities by hand before using scipy.stats. Understand both discrete (PMF) and continuous (PDF) cases. Study relationship between distributions (Bernoulli is Binomial with n=1). <strong>Deliverable:</strong> Create notebooks/foundations/day33_distributions.ipynb with: (1) Visualizations of each distribution with different parameters, (2) MLE implementations for Gaussian and Bernoulli, (3) Examples of sampling and parameter estimation. <strong>Verification:</strong> Can you sample from each distribution? Estimate parameters from data using MLE? Common pitfall: confusing PDF (continuous) with PMF (discrete)—PDFs don\'t give probabilities directly. Success check: Generate Gaussian samples, estimate μ and σ² using MLE, verify matches true parameters. <strong>Resources:</strong> <a href="https://seeing-theory.brown.edu/probability-distributions/index.html" target="_blank" rel="noopener">Seeing Theory: Distributions</a>, <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Ch 3</a>, <a href="https://www.youtube.com/watch?v=rzFX5NWojp0" target="_blank" rel="noopener">StatQuest: Probability Distributions</a>' },
            { label: 'Learn and implement hypothesis testing with t-tests and p-values', estMinutes: 75, details: '<strong>Action:</strong> Study hypothesis testing framework and implement t-tests from scratch. Learn: (1) Null hypothesis H₀ (default assumption, e.g., "no difference between groups") vs alternative H₁, (2) Test statistic (e.g., t-statistic) measures how extreme observed data is, (3) p-value: probability of seeing data this extreme if H₀ is true, (4) Significance level α (often 0.05): reject H₀ if p < α, (5) Type I error (false positive) vs Type II error (false negative). Implement one-sample t-test: tests if sample mean differs from hypothesized value. Formula: t = (x̄ - μ₀)/(s/√n) where x̄ is sample mean, s is sample std, n is sample size. Compare with scipy.stats.ttest_1samp. Implement two-sample t-test: tests if two groups have different means. Use on synthetic data where you know ground truth: generate two groups from N(0,1) and N(0.5,1), test if means differ (should reject H₀). <strong>Boundaries:</strong> Focus on t-tests today—other tests come later. Understand degrees of freedom (n-1 for one-sample, n₁+n₂-2 for two-sample). Study assumptions: normality, independence, equal variance for two-sample. Learn about Welch\'s t-test for unequal variances. <strong>Deliverable:</strong> Extend day33_distributions.ipynb with hypothesis testing section: t-test implementations, p-value calculations, comparison with scipy, examples on synthetic data with known effects. <strong>Verification:</strong> Your t-statistic and p-value should match scipy.stats. When groups truly differ, p-value should be < 0.05. When groups are same, p-value should be uniformly distributed. Common pitfall: misinterpreting p-values—low p-value means "data is unlikely under H₀", not "H₀ is unlikely". Success check: Generate data with effect size 0.5, verify t-test correctly rejects H₀ most of the time. <strong>Resources:</strong> <a href="https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/tests-about-population-mean/v/hypothesis-testing-and-p-values" target="_blank" rel="noopener">Khan Academy: Hypothesis Testing</a>, <a href="https://www.youtube.com/watch?v=0oc49DyA3hU" target="_blank" rel="noopener">StatQuest: t-tests and p-values</a>' },
            { label: 'Write notes on statistical inference and testing in ML context', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day33_distributions.png showing: (1) PDFs/PMFs of the five distributions with different parameters, (2) Visual explanation of p-value with t-distribution, (3) Comparison of one-sample and two-sample t-test scenarios. Write docs/notes/day33_statistics.md (600-800 words) explaining: (1) Role of probability distributions in ML: modeling data, defining loss functions (negative log-likelihood), generative models, (2) Maximum likelihood estimation as principled way to fit models to data, (3) Hypothesis testing in ML: A/B testing (is new model better?), feature importance (does feature help?), model comparison (are differences significant?), (4) Limitations of p-values: don\'t measure effect size, susceptible to p-hacking, need corrections for multiple comparisons, (5) Practical guidelines: report confidence intervals not just p-values, consider effect size and practical significance, beware multiple testing problem. Connect to Week 4 Bayes\' theorem—Bayesian alternative to frequentist hypothesis testing. <strong>Boundaries:</strong> Make distribution plots clear and well-labeled. In notes, balance statistical rigor with practical ML applications. Explain common misconceptions about p-values. Provide guidance for responsible statistical inference. <strong>Deliverable:</strong> Publication-quality figure at 300 DPI and comprehensive notes connecting statistics to ML practice with emphasis on proper inference. <strong>Verification:</strong> Notes should explain both mechanics and interpretation of statistical tests. Should warn against common pitfalls. Common pitfall: treating p-value as "probability hypothesis is true"—it\'s not! Success check: Can you explain when and how to use hypothesis testing in ML projects? <strong>Resources:</strong> <a href="https://www.nature.com/articles/d41586-019-00857-9" target="_blank" rel="noopener">Nature: Scientists rise up against statistical significance</a>, <a href="https://www.youtube.com/watch?v=5OL1RqHrZQ8" target="_blank" rel="noopener">StatQuest: P-values</a>' }
          ],
          reflectionPrompt: 'How do probability distributions and hypothesis testing enable rigorous ML evaluation?'
        },
        {
          globalDay: 34,
          week: 5,
          title: 'ML Math: End-to-End Pipeline Integration',
          priority: 'HIGH',
          tasks: [
            { label: 'Build complete ML pipeline: data prep, training, evaluation, visualization', estMinutes: 90, details: '<strong>Action:</strong> Build a comprehensive end-to-end machine learning pipeline integrating all Week 5 concepts. Use the California Housing dataset from sklearn. Pipeline stages: (1) Load and explore data with pandas (shape, describe(), correlations, missing values), (2) Visualize features with matplotlib (histograms, scatter plots, correlation heatmap), (3) Train-test split (80/20), (4) Feature scaling with StandardScaler (zero mean, unit variance), (5) Train three models: Linear Regression (normal equation), Ridge Regression (with CV for λ), Linear Regression with PCA preprocessing (k=10→8→5), (6) Evaluate on test set: MSE, RMSE, R², MAE, (7) Compare models with visualization: predicted vs actual scatter plots, residual plots, coefficient comparisons. Document every step with markdown cells explaining what and why. This demonstrates how linear algebra (matrix operations), calculus (gradient descent), statistics (hypothesis tests for coefficients), and probability (distributional assumptions) all come together. <strong>Boundaries:</strong> Make the notebook tutorial-quality with clear sections and explanations. Use sklearn for preprocessing (StandardScaler, train_test_split) but implement core models yourself. Include error analysis: where does model fail? Which features matter most? Handle any missing values appropriately. <strong>Deliverable:</strong> Create notebooks/foundations/day34_ml_pipeline.ipynb with complete documented pipeline achieving R² >0.60 on California Housing test set. <strong>Verification:</strong> Pipeline should run end-to-end without errors. All models should show reasonable performance. Visualizations should reveal insights. Common pitfall: data leakage—fit scaler on train only, then transform both train and test. Success check: Can you explain every step and why it\'s necessary? <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html" target="_blank" rel="noopener">sklearn: Putting it all together</a>, <a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">Hands-On ML Chapter 2</a>' },
            { label: 'Perform ablation study showing impact of each technique', estMinutes: 75, details: '<strong>Action:</strong> Conduct systematic ablation study to measure the impact of each technique in your pipeline. Test variants: (1) Baseline: unregularized linear regression, no scaling, no PCA, (2) +Scaling: add feature scaling, (3) +Ridge: add L2 regularization, (4) +PCA: add dimensionality reduction. For each variant, record: train R², test R², training time, coefficient magnitudes. Create comparison table and bar charts showing relative performance. This demonstrates the value of each technique empirically. Additionally, analyze: (1) Impact of training set size: use 10%, 30%, 50%, 70%, 90% of data, plot learning curve (train/test error vs sample size), (2) Impact of regularization: plot validation error vs λ, (3) Impact of dimensionality: plot test error vs number of PCA components. These analyses reveal when each technique helps most. <strong>Boundaries:</strong> Use consistent random seeds for reproducibility. Run each experiment multiple times (3-5) and report mean and std. Make plots clear with error bars when showing averaged results. Keep experimental protocol rigorous: always evaluate on same test set. <strong>Deliverable:</strong> Extend day34_ml_pipeline.ipynb with ablation study section showing systematic comparison and learning curve analysis with clear visualizations. <strong>Verification:</strong> Results should show: scaling helps convergence, Ridge reduces overfitting on small data, PCA reduces computation but may hurt performance, more data improves performance up to saturation. Common pitfall: unfair comparisons due to different random seeds—control for randomness. Success check: Can you explain which technique helps most and when? <strong>Resources:</strong> <a href="https://www.coursera.org/learn/machine-learning/lecture/Kont7/learning-curves" target="_blank" rel="noopener">Andrew Ng: Learning Curves</a>' },
            { label: 'Write comprehensive ML workflow notes with best practices', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day34_pipeline_comparison.png showing: (1) Bar chart comparing test R² across all variants, (2) Learning curves showing train/test error vs sample size, (3) Regularization plot showing validation error vs λ, (4) Table summarizing best practices. Write docs/notes/day34_ml_workflow.md (700-900 words) documenting complete ML workflow best practices: (1) Always split data before any processing, (2) Fit preprocessing (scaling, PCA) on train only, apply to both train and test (avoid data leakage), (3) Use cross-validation for hyperparameter tuning (λ, k), never tune on test set, (4) Monitor both train and test metrics to detect overfitting, (5) Report multiple metrics (R², RMSE, MAE) not just one, (6) Visualize results (predicted vs actual, residuals) to understand failures, (7) Ablation studies reveal what actually helps, (8) Consider computational cost vs accuracy trade-offs (normal equation vs GD, PCA speedup). This is your comprehensive reference for ML best practices learned through Weeks 1-5. <strong>Boundaries:</strong> Make visualizations publication-ready with clear labels and legends. In notes, emphasize practical workflow guidelines you\'ll follow in future projects. Include concrete examples from today\'s pipeline. Organize as actionable checklist. <strong>Deliverable:</strong> High-quality comparison figures at 300 DPI and comprehensive workflow guide serving as your ML project template. <strong>Verification:</strong> Notes should be actionable and comprehensive. Figures should clearly demonstrate experimental findings. Common pitfall: generic advice without concrete examples—ground everything in today\'s experiments. Success check: Could you use this workflow guide as template for future ML projects? <strong>Resources:</strong> <a href="https://developers.google.com/machine-learning/guides/rules-of-ml" target="_blank" rel="noopener">Google: Rules of ML</a>, <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Ch 11</a>' }
          ],
          reflectionPrompt: 'How do the mathematical foundations from Weeks 1-5 enable practical ML pipelines?'
        },
        {
          globalDay: 35,
          week: 5,
          title: 'Week 5 Review: Advanced ML Implementation',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review Week 5 materials: regression, regularization, PCA, distributions, pipeline', estMinutes: 75, details: '<strong>Action:</strong> Comprehensive review of Week 5 covering all major implementations: (1) Linear regression—can you derive and implement both normal equation and gradient descent? (2) Regularization—can you explain bias-variance trade-off and select optimal λ? (3) PCA—can you implement from scratch using both eigendecomposition and SVD? (4) Probability distributions—can you sample from and fit distributions using MLE? (5) Hypothesis testing—can you conduct and interpret t-tests? (6) End-to-end pipeline—can you build complete ML workflow with proper validation? For each topic, open relevant notebooks, re-run all cells, verify understanding. Identify any concepts that remain unclear. Test yourself: close notebooks and try to implement key algorithms from memory (don\'t aim for perfection, just test recall). <strong>Boundaries:</strong> This is active review involving coding practice, not passive reading. Allocate about 12-15 minutes per topic (6 topics × 12 min ≈ 75 min). Focus extra time on topics that felt challenging. Re-derive mathematical results on paper. Re-implement key functions without looking at code. <strong>Deliverable:</strong> Create docs/reviews/week05_review.md with: (1) List of concepts mastered with confidence ratings (1-5), (2) Concepts needing more practice with specific gaps identified, (3) Self-assessment: which implementation was most challenging? Which was most enlightening? (4) Connection map: how do all Week 5 topics relate? <strong>Verification:</strong> You should feel confident implementing any Week 5 algorithm from scratch with only documentation as reference. Common pitfall: surface-level review without testing implementation skills. Success check: Can you implement Ridge regression and PCA from memory (with syntax errors OK, logic should be right)? <strong>Resources:</strong> Your Week 5 notebooks and notes' },
            { label: 'Complete comprehensive practice problems testing all Week 5 skills', estMinutes: 60, details: '<strong>Action:</strong> Work through challenging practice exercises integrating Week 5 concepts: (1) Implement Ridge regression with gradient descent (combining regularization + iterative optimization), test on synthetic high-dimensional data, (2) Apply PCA to MNIST digits, project to 2D, visualize with colored clusters by digit class—can you see digit separation? (3) Generate samples from Gaussian Mixture Model (combine two Gaussians), fit each component using MLE, compare with ground truth parameters, (4) Design and conduct statistical test: generate two datasets with known small effect size (Cohen\'s d=0.3), run t-test, report p-value and confidence interval, (5) Build mini-pipeline: load dataset, apply PCA for dimensionality reduction, train Ridge with CV, evaluate with learning curves. Set 60-minute timer and complete as many as possible. Then review solutions, analyze mistakes, identify knowledge gaps. <strong>Boundaries:</strong> Challenge yourself with problems you haven\'t seen before. Use different datasets than training examples. Refer to documentation (NumPy, SciPy) but not your previous implementations. If stuck >10 min, make note and continue—review that concept after. <strong>Deliverable:</strong> Add to week05_review.md with your solutions, noting completion time, difficulty rating (1-5), and key learnings from each problem. <strong>Verification:</strong> Aim to complete 4/5 problems correctly. Errors reveal topics needing reinforcement. Common pitfall: giving up too quickly on challenging problems—productive struggle builds understanding. Success check: Did you successfully integrate multiple concepts (e.g., PCA + Ridge + CV)? <strong>Resources:</strong> <a href="https://www.kaggle.com/c/digit-recognizer" target="_blank" rel="noopener">Kaggle MNIST dataset</a>, <a href="https://scikit-learn.org/stable/datasets.html" target="_blank" rel="noopener">sklearn datasets</a>' },
            { label: 'Write weekly log reflecting on Week 5 progress and ML understanding', estMinutes: 30, details: '<strong>Action:</strong> Create docs/logs/week05_log.md documenting: (1) Accomplishments—list major implementations (normal equation, GD variants, Ridge, PCA, distributions, pipeline), (2) Key insights—what were the major "aha moments"? How has your understanding of ML deepened? (3) Challenges and solutions—what was difficult? How did you overcome it? What debugging strategies worked? (4) From-scratch implementations—what did you learn by building algorithms yourself vs using sklearn? (5) Time management—compare estimated vs actual time, identify efficiency patterns, (6) Looking ahead—Week 6 is capstone foundations week. What from Weeks 1-5 needs final reinforcement? What concepts would you like to integrate? Write 500-700 words. Be honest and reflective. This is your learning journal tracking both technical skills and meta-learning insights. <strong>Boundaries:</strong> Make it personal and reflective, not just task summaries. Reflect on your learning process: what study methods worked best? When did concepts "click"? What would you do differently? Identify patterns in your learning for future optimization. Consider: morning vs evening study, video vs reading vs coding first, social learning vs solo. <strong>Deliverable:</strong> Thoughtful weekly log capturing both technical progress and meta-cognitive insights about your learning journey. <strong>Verification:</strong> Log should show genuine reflection and self-awareness, not just summaries. Should identify specific improvements for Week 6. Common pitfall: superficial logs without insight—dig deeper into your learning process. Success check: Reading this log in future, will it help you understand your learning patterns? Can you identify one concrete study habit to improve? <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/ultralearning/" target="_blank" rel="noopener">Ultralearning principles</a>' }
          ],
          reflectionPrompt: 'What was the most valuable skill or insight you gained from Week 5?'
        },
        {
          globalDay: 36,
          week: 6,
          title: 'Foundations Capstone: LR Implementation & Validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Implement production-quality Linear Regression from scratch with full API', estMinutes: 90, details: '<strong>Action:</strong> Create a polished, production-quality LinearRegression class implementing complete scikit-learn-like API. Implement: (1) <code>__init__(method="normal_equation", alpha=0.01, max_iter=1000, tol=1e-6)</code> allowing choice between normal equation and gradient descent, (2) <code>fit(X, y)</code> with input validation (check shapes, handle missing values, ensure numeric types), (3) <code>predict(X)</code> with fitted check, (4) <code>score(X, y)</code> computing R², (5) Properties: <code>coef_</code> (coefficients), <code>intercept_</code> (bias term), <code>n_iter_</code> (GD iterations if applicable). Include comprehensive docstrings with parameter descriptions, return types, and usage examples. Add error handling: raise ValueError for invalid inputs, warn if GD doesn\'t converge. Test edge cases: single feature, single sample, perfect fit, rank-deficient X. <strong>Boundaries:</strong> Code should be clean, well-commented, and follow PEP 8 style. Split into logical private methods: <code>_add_intercept()</code>, <code>_normal_equation()</code>, <code>_gradient_descent()</code>. Add type hints for all parameters. Include assert statements checking preconditions. <strong>Deliverable:</strong> Create notebooks/foundations/capstone/day36_lr_from_scratch.ipynb with complete LinearRegression class, usage examples, and comprehensive tests demonstrating all functionality. <strong>Verification:</strong> Class should handle all sklearn.datasets regression problems correctly. Should match sklearn.linear_model.LinearRegression results within 1e-4 tolerance. Common pitfall: not handling edge cases—test with unusual inputs. Success check: Your class should be usable as drop-in replacement for sklearn LinearRegression on simple problems. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_blank" rel="noopener">sklearn LinearRegression API</a>, <a href="https://www.python.org/dev/peps/pep-0257/" target="_blank" rel="noopener">PEP 257: Docstring Conventions</a>, <a href="https://realpython.com/python-type-checking/" target="_blank" rel="noopener">Python Type Checking</a>' },
            { label: 'Generate synthetic datasets and validate LR achieving R² ≥ 0.90', estMinutes: 75, details: '<strong>Action:</strong> Create comprehensive synthetic dataset suite to validate your LinearRegression implementation. Generate: (1) Simple 1D: y = 2x + 3 + ε, n=100, σ=1, (2) Multi-feature: y = 2x₁ + 3x₂ - x₃ + 5 + ε, n=200, d=3, (3) High-dimensional: y = ∑wᵢxᵢ + ε, n=100, d=20 with sparse weights (only 5 non-zero), (4) Noisy: same as (2) but σ=5, (5) Nearly collinear features: x₂ = x₁ + small_noise. For each dataset: split 80/20 train/test, fit with both normal equation and gradient descent, compute train/test R², verify R² ≥ 0.90 on test set for datasets 1-3 (high noise in dataset 4 prevents this). Plot: predicted vs actual scatter plots, residual plots showing random scatter around zero (no patterns), coefficient comparisons (your implementation vs sklearn). This validation demonstrates your implementation works correctly across diverse scenarios. <strong>Boundaries:</strong> Use sklearn.datasets.make_regression for complex scenarios. Set random_state for reproducibility. For collinear features, test that normal equation handles it (may need regularization) while GD still works. Document expected behavior for each scenario. <strong>Deliverable:</strong> Extend day36_lr_from_scratch.ipynb with synthetic data validation section achieving R² ≥ 0.90 on suitable datasets, with comprehensive visualizations and analysis. <strong>Verification:</strong> Your implementation should achieve R² ≥ 0.90 on synthetic datasets with clear relationships. Residuals should show no systematic patterns. Common pitfall: not checking residual plots—they reveal model deficiencies. Success check: R² ≥ 0.90 on datasets 1-3, with clear visualizations validating correctness. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html" target="_blank" rel="noopener">sklearn.datasets.make_regression</a>, <a href="https://www.youtube.com/watch?v=zPG4NjIkCjc" target="_blank" rel="noopener">StatQuest: Residual Plots</a>' },
            { label: 'Write comprehensive test suite and documentation for LR class', estMinutes: 60, details: '<strong>Action:</strong> Create comprehensive test functions validating LinearRegression behavior. Write: (1) <code>test_fit_predict()</code> checking basic functionality, (2) <code>test_score()</code> verifying R² calculation, (3) <code>test_coefficients()</code> checking learned parameters match known values on synthetic data, (4) <code>test_normal_equation_vs_gd()</code> verifying both methods converge to same solution, (5) <code>test_edge_cases()</code> checking single feature, single sample, perfect fit, (6) <code>test_input_validation()</code> ensuring proper errors for invalid inputs, (7) <code>test_convergence_warning()</code> for GD with insufficient iterations. Each test should have clear assertions with informative messages. Also write docs/notes/day36_lr_implementation.md (600-800 words) documenting: design decisions (why choose normal equation vs GD based on data size), implementation challenges (handling singular matrices, GD convergence), API design (following sklearn conventions), testing strategy, performance characteristics (time/space complexity), and future improvements (regularization, feature engineering). <strong>Boundaries:</strong> Tests should be independent and reproducible (use random seeds). Follow testing best practices: one assertion per test when possible, clear test names, isolated setup. Documentation should explain rationale behind design choices. <strong>Deliverable:</strong> Test suite in notebook and comprehensive implementation notes documenting your LinearRegression class thoroughly. <strong>Verification:</strong> All tests should pass with clear output. Documentation should explain implementation thoroughly. Common pitfall: tests that don\'t actually verify correctness—use known ground truth. Success check: Tests catch bugs if you intentionally break functionality. <strong>Resources:</strong> <a href="https://docs.pytest.org/en/stable/goodpractices.html" target="_blank" rel="noopener">Pytest Best Practices</a>' }
          ],
          reflectionPrompt: 'What design decisions did you make in implementing your LinearRegression class and why?'
        },
        {
          globalDay: 37,
          week: 6,
          title: 'Foundations Capstone: LR on Real Dataset with Analysis',
          priority: 'HIGH',
          tasks: [
            { label: 'Apply LR to California Housing dataset with full EDA and preprocessing', estMinutes: 90, details: '<strong>Action:</strong> Conduct comprehensive exploratory data analysis (EDA) and preprocessing on California Housing dataset from sklearn. EDA: (1) Load data and examine shape, types, missing values, (2) Compute descriptive statistics (mean, std, min, max, quartiles), (3) Visualize distributions with histograms for all features, (4) Compute and visualize correlation matrix as heatmap—identify highly correlated features, (5) Create scatter plots for top 3 correlated features vs target, (6) Check for outliers using box plots and IQR method, (7) Analyze feature ranges and scales—note that features have very different scales. Preprocessing pipeline: (1) Handle outliers (clip to reasonable ranges or remove if extreme), (2) Train-test split 80/20 with stratification if appropriate, (3) Feature scaling using StandardScaler (fit on train, transform both), (4) Consider feature engineering: interaction terms, polynomial features, log transforms for skewed features. Document each preprocessing decision with rationale—why is scaling necessary? Which outliers to remove? <strong>Boundaries:</strong> Make EDA thorough with at least 8-10 visualizations. Document every preprocessing step in markdown cells explaining why it\'s necessary. Don\'t just apply transformations blindly—show before/after distributions. Keep preprocessing relatively simple—focus on scaling and basic outlier handling. <strong>Deliverable:</strong> Create notebooks/foundations/capstone/day37_real_dataset.ipynb with comprehensive EDA section (visualizations + analysis) and documented preprocessing pipeline. <strong>Verification:</strong> EDA should reveal data characteristics (distributions, correlations, outliers). Preprocessing should address identified issues. Common pitfall: insufficient EDA leading to missed data issues. Success check: Can you explain every preprocessing decision based on EDA findings? <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset" target="_blank" rel="noopener">California Housing Dataset</a>, <a href="https://www.kaggle.com/code/kashnitsky/topic-1-exploratory-data-analysis-with-pandas" target="_blank" rel="noopener">Kaggle: EDA Tutorial</a>' },
            { label: 'Train and evaluate multiple models achieving R² ≥ 0.90 with error analysis', estMinutes: 75, details: '<strong>Action:</strong> Train multiple linear models and conduct thorough evaluation. Models: (1) Baseline: your LinearRegression (normal equation) with standardized features, (2) Ridge with CV for λ selection (try λ ∈ {0.01, 0.1, 1, 10, 100}), (3) Feature-reduced: PCA to k components then LinearRegression (try k ∈ {5, 10, 15}), (4) Feature-engineered: add interaction terms or polynomial features then Ridge. For each model: record train R², test R², training time, predictions. Compare models: create table summarizing metrics, plot predicted vs actual for best model, create residual analysis. Aim for test R² ≥ 0.70 (California Housing is challenging, R²=0.90 may not be achievable without advanced techniques—strive for best possible). Error analysis: (1) Plot residuals vs predicted values, (2) Plot residuals vs each feature—identify problematic regions, (3) Find worst predictions—what makes these samples hard? (4) Analyze errors by feature ranges—does model fail on expensive houses? <strong>Boundaries:</strong> Prioritize model understanding over peak performance. Document what works and what doesn\'t. Include failed experiments—they\'re valuable learning. Compare all models fairly on same train/test split. <strong>Deliverable:</strong> Extend day37_real_dataset.ipynb with model training, comparison, and error analysis achieving best possible R² (target ≥0.70) with thorough analysis of successes and failures. <strong>Verification:</strong> Should achieve competitive R² on California Housing. Error analysis should reveal patterns and failure modes. Common pitfall: focusing only on metrics without understanding errors. Success check: Can you explain which model works best and why? What are failure modes? <strong>Resources:</strong> <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Andrew Ng: Model Evaluation</a>, <a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">Hands-On ML Chapter 2</a>' },
            { label: 'Write comprehensive analysis report with visualizations and insights', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day37_california_housing.png showing: (1) Correlation heatmap from EDA, (2) Best model: predicted vs actual scatter with R² annotation, (3) Residual plot for best model, (4) Model comparison bar chart (test R² for all models). Write docs/notes/day37_real_dataset_analysis.md (700-900 words) documenting complete analysis: (1) Data characteristics discovered in EDA (distributions, correlations, outliers), (2) Preprocessing decisions and their impact, (3) Model comparison results—which model performs best and why? (4) Error analysis findings—what patterns in failures? Which samples are hardest? (5) Lessons learned—what would improve performance? What didn\'t work and why? (6) Comparison with sklearn baseline—how close is your from-scratch implementation? (7) Computational considerations—time and memory trade-offs. This document showcases your ability to conduct complete ML analysis from data exploration to model evaluation. <strong>Boundaries:</strong> Make visualizations publication-quality and well-annotated. In notes, balance technical details with high-level insights. Include specific numbers (R² values, feature correlations) not just qualitative descriptions. Discuss both successes and failures honestly. <strong>Deliverable:</strong> High-quality multi-panel figure at 300 DPI and comprehensive analysis document demonstrating thorough understanding of real-world ML workflow. <strong>Verification:</strong> Notes should read like professional ML report. Visualizations should support all claims. Common pitfall: vague analysis without specific findings—be concrete and precise. Success check: Could you present this analysis to demonstrate your ML skills? <strong>Resources:</strong> <a href="https://developers.google.com/machine-learning/guides" target="_blank" rel="noopener">Google ML Guides</a>' }
          ],
          reflectionPrompt: 'What did applying ML to real data teach you that synthetic examples didn\'t?'
        },
        {
          globalDay: 38,
          week: 6,
          title: 'Foundations Capstone: PCA with Reconstruction Demo',
          priority: 'HIGH',
          tasks: [
            { label: 'Implement complete PCA class with reconstruction and explained variance', estMinutes: 90, details: '<strong>Action:</strong> Build production-quality PCA class with complete functionality. Implement: (1) <code>__init__(n_components=None, method="svd")</code> allowing SVD or eigendecomposition, (2) <code>fit(X)</code> computing PCs, explained variance, and mean (for centering), (3) <code>transform(X)</code> projecting data to PC space, (4) <code>fit_transform(X)</code> convenience method, (5) <code>inverse_transform(X_pca)</code> reconstructing original space from PC space, (6) Properties: <code>components_</code> (PC directions), <code>explained_variance_</code>, <code>explained_variance_ratio_</code>, <code>mean_</code>. Add utility methods: <code>get_n_components_for_variance(threshold)</code> returning k for desired variance retention (e.g., 95%). Include comprehensive docstrings, type hints, and input validation. Test against sklearn.decomposition.PCA for correctness. Add visualization method <code>plot_explained_variance()</code> showing cumulative variance vs components with elbow detection. <strong>Boundaries:</strong> Follow sklearn API conventions closely. Handle edge cases: n_components=None (keep all), n_components as float (variance threshold), numerical stability (clip tiny negative eigenvalues to zero). Add assertions for valid inputs. Code should be modular with clear separation of concerns. <strong>Deliverable:</strong> Create notebooks/foundations/capstone/day38_pca_complete.ipynb with complete PCA class, tests showing equivalence with sklearn, and usage examples on multiple datasets. <strong>Verification:</strong> Results should match sklearn within numerical precision. All methods should work correctly. Common pitfall: forgetting to store mean for inverse transform—centering must be reversible. Success check: Reconstruct original data from PCs: ||X - PCA.inverse_transform(PCA.transform(X))|| should be small. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank" rel="noopener">sklearn PCA API</a>, <a href="https://arxiv.org/abs/1404.1100" target="_blank" rel="noopener">PCA Tutorial (arxiv)</a>' },
            { label: 'Create compelling reconstruction demonstrations on image data', estMinutes: 75, details: '<strong>Action:</strong> Demonstrate PCA reconstruction on visual data to show information/compression trade-off. Use MNIST digits or Olivetti faces dataset from sklearn. Workflow: (1) Load image dataset and display sample images, (2) Apply PCA with varying components k ∈ {1, 2, 5, 10, 20, 50, 100, full}, (3) For each k, reconstruct images and compute MSE reconstruction error, (4) Create visualization grid: rows are different k values, columns are different sample images showing original vs reconstruction, (5) Plot reconstruction error vs k—should show elbow where diminishing returns start, (6) Plot cumulative explained variance—show how many components needed for 90%, 95%, 99% variance. Add interactive element if using Jupyter widgets: slider to adjust k and see reconstruction update. This demo viscerally demonstrates PCA as lossy compression—fewer components = lower quality but smaller size. Calculate compression ratio: original size vs (k components + mean). <strong>Boundaries:</strong> Make visualizations compelling—reconstruction quality changes should be obvious. Use grayscale images to keep computation tractable. Try both faces (smoother) and digits (sharper features) to see different PCA behavior. Document observations: which features are captured by top PCs? How many PCs needed for recognizable reconstruction? <strong>Deliverable:</strong> Extend day38_pca_complete.ipynb with image reconstruction demo showing clear quality/compression trade-off across multiple k values. <strong>Verification:</strong> Reconstructions should show clear degradation with fewer components. MSE should decrease monotonically with k. Common pitfall: not normalizing images properly—ensure pixel values in valid range after reconstruction. Success check: Top 50 components should reconstruct faces/digits recognizably; top 10 should show degradation but preserve structure. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/datasets/real_world.html#olivetti-faces-dataset" target="_blank" rel="noopener">Olivetti Faces Dataset</a>, <a href="https://www.youtube.com/watch?v=HMOI_lkzW08" target="_blank" rel="noopener">StatQuest: PCA</a>' },
            { label: 'Generate artifacts and write comprehensive PCA analysis notes', estMinutes: 60, details: '<strong>Action:</strong> Generate artifacts/day38_pca_reconstruction.png showing: (1) Grid of reconstructions with varying k (6 images × 5 k-values), (2) Reconstruction error plot with elbow highlighted, (3) Cumulative explained variance plot with 90%/95%/99% thresholds marked, (4) Comparison of original vs k=50 reconstruction for 4 diverse samples. Write docs/notes/day38_pca_analysis.md (700-900 words) covering: (1) PCA as dimensionality reduction and lossy compression—mathematical relationship between k and information retained, (2) How to choose k: scree plot elbow method, variance threshold (95% rule), downstream task performance, computational constraints, (3) Reconstruction quality analysis—what image features are lost with few components? Top PCs capture global structure (average face), later PCs capture details, (4) Practical PCA applications: visualization (t-SNE alternative), feature extraction, noise reduction, data compression, preprocessing for other algorithms, (5) Limitations: assumes linear relationships, sensitive to scaling, can\'t capture non-linear structure, (6) When to use PCA vs alternatives (autoencoders for non-linear, ICA for non-Gaussian, NMF for non-negative). <strong>Boundaries:</strong> Make reconstruction grid visually striking—clear quality progression. In notes, connect math to practical decisions. Discuss trade-offs: accuracy vs speed, interpretability vs performance. <strong>Deliverable:</strong> Compelling visualization at 300 DPI and comprehensive analysis notes positioning PCA in broader dimensionality reduction landscape. <strong>Verification:</strong> Visualizations should tell clear story about PCA trade-offs. Notes should guide practical PCA usage. Common pitfall: analysis without actionable insights—provide decision-making guidance. Success check: Could you use these materials to teach PCA and advise on component selection? <strong>Resources:</strong> <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book Ch 2</a>, <a href="https://distill.pub/2016/misread-tsne/" target="_blank" rel="noopener">How to Use t-SNE Effectively</a>' }
          ],
          reflectionPrompt: 'How does reconstruction error help you choose the right number of principal components?'
        },
        {
          globalDay: 39,
          week: 6,
          title: 'Foundations Capstone: Summary Document Creation',
          priority: 'HIGH',
          tasks: [
            { label: 'Write comprehensive foundations_summary.md covering all 6 weeks (Part 1)', estMinutes: 90, details: '<strong>Action:</strong> Begin comprehensive summary document covering Weeks 1-3. Create docs/capstone/foundations_summary.md with professional structure: executive summary, table of contents, and detailed sections. Week 1 (Linear Algebra): (1) Vectors: geometric and algebraic perspectives, operations (addition, scaling, dot product), (2) Matrices: as transformations, column perspective, operations (matrix-vector, matrix-matrix multiplication), (3) Determinants: area scaling interpretation, connection to invertibility, (4) Key insights: matrices transform space, determinants measure scaling, column space determines reach. Week 2 (Advanced Linear Algebra): (1) Eigenvalues/eigenvectors: directions unchanged by transformation, applications to PCA, (2) SVD: decomposition into rotation-scaling-rotation, connection to PCA, (3) Orthogonality: perpendicular vectors, orthonormal bases, projections, (4) Key insights: eigenvectors reveal transformation structure, SVD provides optimal low-rank approximation. Week 3 (Calculus & Probability Intro): (1) Derivatives: rates of change, gradients for optimization, (2) Partial derivatives: multivariable calculus for ML, (3) Basic probability: events, conditional probability, independence, (4) Key insights: derivatives enable optimization, probability models uncertainty. For each concept, include: definition, intuition, why it matters for ML, concrete example. Target 2-3 paragraphs per major concept. <strong>Boundaries:</strong> Write clearly for your future self—balance rigor with accessibility. Include equations where helpful but emphasize understanding over formalism. Use concrete examples throughout. Organize hierarchically with clear headings. <strong>Deliverable:</strong> Well-structured summary document covering Weeks 1-3 comprehensively (3-4 pages), meeting half the ≥5 page requirement. <strong>Verification:</strong> Should cover all major topics from Weeks 1-3 with clear explanations. Common pitfall: too much detail—focus on key concepts and insights. Success check: Could someone use this to review foundations quickly? <strong>Resources:</strong> Your weeks 1-3 notes, <a href="https://mml-book.github.io/" target="_blank" rel="noopener">Mathematics for Machine Learning book</a>' },
            { label: 'Complete foundations_summary.md covering Weeks 4-6 (Part 2)', estMinutes: 75, details: '<strong>Action:</strong> Complete summary document covering Weeks 4-6. Week 4 (Advanced Calculus & Statistics): (1) Convexity: guarantees for optimization, convex functions in ML, (2) Gradient descent: batch, mini-batch, SGD trade-offs, (3) Bayes\' theorem: updating beliefs with evidence, naive Bayes classifier, (4) Entropy: measuring uncertainty, connection to information theory, (5) Key insights: convexity ensures GD finds global minimum, Bayes enables probabilistic reasoning. Week 5 (ML Algorithms): (1) Linear regression: normal equation (exact) vs gradient descent (iterative), (2) Regularization: L2/Ridge prevents overfitting, bias-variance trade-off, (3) PCA: dimensionality reduction via variance maximization, eigendecomposition, (4) Statistical distributions: Gaussian, Bernoulli, Binomial; MLE for parameter estimation, (5) Hypothesis testing: t-tests, p-values, significance, (6) Key insights: regularization improves generalization, PCA finds principal variance directions. Week 6 (Integration): (1) End-to-end ML pipeline: EDA, preprocessing, training, evaluation, (2) Implementation best practices: clean code, validation, error analysis, (3) Real-world application lessons: data quality matters, model selection requires experimentation, (4) Key insights: ML is iterative experimentation guided by theory. Add "Connections" section showing how concepts integrate: matrices enable efficient computation, calculus enables optimization, probability enables uncertainty quantification, statistics enable rigorous evaluation. Conclude with "Next Steps" previewing Classical ML phase. <strong>Boundaries:</strong> Make Week 4-6 sections mirror Week 1-3 structure for consistency. Total document should be 5-7 pages (1500-2000 words). Include key equations but emphasize conceptual understanding. <strong>Deliverable:</strong> Complete 5-7 page foundations summary document covering all 6 weeks comprehensively. <strong>Verification:</strong> Document should be comprehensive yet concise reference for all foundation concepts. Common pitfall: redundancy with notes—this is high-level synthesis, not exhaustive documentation. Success check: Does this document effectively summarize 6 weeks of learning? <strong>Resources:</strong> Your weeks 4-6 notes, <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning Book</a>' },
            { label: 'Polish document with visualizations and create summary artifacts', estMinutes: 60, details: '<strong>Action:</strong> Enhance foundations_summary.md with visual aids and polish. Add: (1) Key visualizations: include 6-8 carefully selected figures from previous weeks (e.g., matrix transformations, GD convergence, PCA reconstruction, model comparison), (2) Summary tables: major algorithms with complexity/use-cases, distributions with properties, optimization methods with trade-offs, (3) Formula reference: key equations in clean format (matrices, gradients, loss functions, test statistics), (4) Concept dependency graph: show how concepts build on each other, (5) Resource appendix: best learning resources discovered for each topic. Polish writing: (1) Edit for clarity and concision, (2) Ensure consistent formatting and heading hierarchy, (3) Add cross-references between related concepts, (4) Check all equations render correctly, (5) Proofread for spelling/grammar. Generate artifacts/foundations_capstone_summary.png: single-page visual summary showing concept progression from Week 1 linear algebra through Week 6 integration, with key insights highlighted. This serves as overview/reference. <strong>Boundaries:</strong> Visualizations should enhance not clutter—select most informative figures. Keep formatting consistent and professional. Make it publication-quality reference document. <strong>Deliverable:</strong> Polished, comprehensive foundations_summary.md (5-7 pages) with integrated visualizations and single-page visual summary artifact at 300 DPI. <strong>Verification:</strong> Document should be polished, comprehensive, and highly usable as reference. Visual summary should capture essence of 6 weeks. Common pitfall: over-decoration—maintain professional technical document style. Success check: Would you be proud to share this document as demonstration of your learning? <strong>Resources:</strong> <a href="https://www.markdownguide.org/extended-syntax/" target="_blank" rel="noopener">Markdown Extended Syntax</a>' }
          ],
          reflectionPrompt: 'What are the most important connections between concepts you discovered while writing the summary?'
        },
        {
          globalDay: 40,
          week: 6,
          title: 'Foundations Capstone: Comprehensive Assessment Quiz',
          priority: 'HIGH',
          tasks: [
            { label: 'Create and complete 20-question foundations quiz covering all topics', estMinutes: 90, details: '<strong>Action:</strong> Create comprehensive assessment quiz testing understanding across all foundation topics. Question distribution: 5 Linear Algebra (vectors, matrices, eigenvalues, SVD), 4 Calculus (derivatives, gradients, optimization, convexity), 4 Probability/Statistics (Bayes, distributions, hypothesis tests, entropy), 4 ML Algorithms (linear regression, Ridge, PCA, model evaluation), 3 Implementation/Practice (debugging, API design, best practices). Question types: (1) Conceptual: "Explain why regularization improves generalization", (2) Computational: "Compute eigenvalues of [[3,1],[1,3]]", (3) Applied: "Given learning curves, diagnose overfitting vs underfitting", (4) Analysis: "Compare normal equation vs gradient descent for dataset with n=1000, d=50000". Make questions challenging but fair—test understanding not memorization. Include: multiple choice (select best answer), short answer (2-3 sentences), calculation problems, scenario analysis. Create in docs/capstone/foundations_quiz.md with questions first, answers at end for self-grading. Take quiz honestly under timed conditions (60 min for 20 questions). <strong>Boundaries:</strong> Questions should cover breadth of topics, not just recent weeks. Mix difficulty: 40% moderate, 40% challenging, 20% difficult. No trivial questions. Test both breadth (many topics) and depth (understanding key concepts thoroughly). Cover practical and theoretical aspects. <strong>Deliverable:</strong> Complete 20-question quiz in foundations_quiz.md with questions, your answers, correct answers, and scoring (aim for ≥16/20). <strong>Verification:</strong> Quiz should comprehensively test foundations knowledge. Questions should be clear and unambiguous. Common pitfall: questions testing memorization rather than understanding—focus on application and reasoning. Success check: Achieve ≥16/20 correct demonstrating strong foundations mastery. <strong>Resources:</strong> Your weeks 1-6 materials, <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Andrew Ng ML quizzes</a>' },
            { label: 'Analyze quiz results and create targeted remediation plan', estMinutes: 75, details: '<strong>Action:</strong> Thoroughly analyze quiz performance to identify knowledge gaps and create remediation plan. Analysis: (1) Categorize each question by topic and difficulty, (2) For incorrect answers: identify root cause (conceptual misunderstanding, computational error, forgot formula, misread question), (3) Identify patterns: which topics had most errors? Were errors in concepts or application? (4) Self-assessment: which questions felt uncertain even if correct? (5) Create topic-by-topic breakdown: e.g., "Linear Algebra: 4/5 correct; struggled with eigenvector interpretation". Remediation plan: For each weak area, specify: (1) Topic to review, (2) Specific gaps (what exactly is unclear?), (3) Resources to use (specific videos, textbook sections, exercises), (4) Practice activities (implement from scratch, work example problems, teach concept to someone), (5) Success criteria (how will you know you\'ve mastered it?), (6) Time allocation (how long to spend). Create prioritized list focusing on topics most critical for Classical ML phase. Be honest and specific—vague goals like "review calculus" don\'t help. <strong>Boundaries:</strong> Make analysis detailed and honest—identifying gaps is valuable, not shameful. Prioritize topics most important for upcoming work. Create actionable remediation plan with concrete activities, not just "read more". Set realistic time estimates—can\'t re-learn everything. <strong>Deliverable:</strong> Detailed quiz analysis and prioritized remediation plan in docs/capstone/quiz_analysis_remediation.md with specific gaps and concrete action items. <strong>Verification:</strong> Analysis should identify specific knowledge gaps. Remediation plan should be actionable with clear success criteria. Common pitfall: generic remediation like "study more"—be specific about what and how. Success check: Remediation plan addresses your specific gaps with concrete activities. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2019/03/19/fix-knowledge-gaps/" target="_blank" rel="noopener">How to Fix Knowledge Gaps</a>' },
            { label: 'Execute high-priority remediation and create concept connections document', estMinutes: 60, details: '<strong>Action:</strong> Execute remediation for 2-3 highest priority gaps identified in quiz analysis. For each gap: (1) Review relevant materials from original learning (notes, notebooks, videos), (2) Work through additional practice problems until concept clicks, (3) Implement related algorithm or solve computational problem demonstrating mastery, (4) Document your new understanding—what was the key insight that made it click? Additionally, create docs/capstone/concept_connections.md documenting deep concept connections discovered through foundations: (1) How linear algebra enables efficient computation (vectorization, matrix operations), (2) How calculus enables optimization (gradients point toward improvement, convexity guarantees convergence), (3) How probability enables uncertainty quantification (Bayesian inference, confidence intervals), (4) How statistics enable rigorous evaluation (hypothesis tests, effect sizes), (5) How these combine in ML algorithms (linear regression uses linear algebra for computation, calculus for optimization, statistics for evaluation). Include concrete examples from your implementations showing each connection. This metacognitive reflection cements understanding. <strong>Boundaries:</strong> Focus remediation on 2-3 items—quality over quantity. Really work until gaps are filled. For connections document, emphasize unexpected or non-obvious relationships. Use specific examples from your code/analysis. <strong>Deliverable:</strong> Remediation work products (re-implemented algorithms, solved problems) and concept connections document showing deep understanding of foundations integration. <strong>Verification:</strong> Remediation should address identified gaps effectively. Connections document should show synthesis beyond individual topics. Common pitfall: superficial remediation—work until you truly understand, not just recognize. Success check: Can you now answer quiz questions you missed? Connections document shows integrated understanding? <strong>Resources:</strong> <a href="https://barbaraoakley.com/books/learn-like-a-pro/" target="_blank" rel="noopener">Learning how to Learn</a>' }
          ],
          reflectionPrompt: 'What patterns in your quiz performance reveal about your learning strengths and areas for growth?'
        },
        {
          globalDay: 41,
          week: 6,
          title: 'Foundations Capstone: Integration & Portfolio Polish',
          priority: 'HIGH',
          tasks: [
            { label: 'Create integrated capstone notebook showcasing all foundation skills', estMinutes: 90, details: '<strong>Action:</strong> Build comprehensive showcase notebook integrating all foundation concepts in single compelling project. Project: "End-to-End Regression Analysis with Dimensionality Reduction". Workflow: (1) Introduction section explaining project goals and methods, (2) Data loading and EDA: use Boston Housing or similar dataset, comprehensive visualizations, correlation analysis, (3) Feature engineering: create polynomial features, interaction terms, (4) Preprocessing: handle outliers, feature scaling, train-test split, (5) Dimensionality reduction: apply your from-scratch PCA, analyze variance explained, visualize in 2D PC space, (6) Modeling: train multiple models (Linear Regression, Ridge with CV, PCA+Ridge), implement from scratch using your classes, (7) Evaluation: comprehensive metrics (R², RMSE, MAE), visualizations (predicted vs actual, residuals, learning curves), statistical tests comparing models, (8) Error analysis: identify failure modes, analyze residuals, (9) Conclusions: summarize findings, discuss what worked and limitations. Include comprehensive markdown narrative explaining every step, connecting to theoretical concepts from Weeks 1-6. Make it tutorial-quality with clear explanations, clean code, and professional visualizations. This showcases your ability to execute complete ML project using foundation concepts. <strong>Boundaries:</strong> Prioritize clarity and completeness over novelty. Use your from-scratch implementations where possible, sklearn for preprocessing only. Include theory connections: "We use SVD for PCA because it\'s more numerically stable than eigendecomposition". Make every visualization publication-quality. <strong>Deliverable:</strong> Create notebooks/foundations/capstone/day41_integrated_capstone.ipynb demonstrating all foundation skills in single polished project (aim for "portfolio-ready" quality). <strong>Verification:</strong> Notebook should run top-to-bottom without errors, include comprehensive analysis, and showcase technical depth. Common pitfall: treating as collection of tasks rather than cohesive narrative—tell a story. Success check: Would you be proud to show this to potential employers/collaborators? <strong>Resources:</strong> <a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">Hands-On ML example notebooks</a>' },
            { label: 'Polish all capstone artifacts and create comprehensive README', estMinutes: 75, details: '<strong>Action:</strong> Polish all Week 6 deliverables to portfolio quality and create comprehensive documentation. Polish tasks: (1) Review all capstone notebooks (Days 36-41) for: code quality (PEP 8, comments, docstrings), narrative quality (clear explanations, typos fixed, logical flow), visualization quality (labels, legends, colors, DPI), (2) Ensure consistency: naming conventions, style, structure across notebooks, (3) Add table of contents to longer notebooks, (4) Export key notebooks as PDF for offline sharing, (5) Test: restart kernel, run all cells, verify no errors. Create README: Write docs/capstone/README.md (800-1000 words) documenting entire capstone: (1) Overview: capstone goals and structure, (2) Deliverables: list of all artifacts with descriptions, (3) Key achievements: R² scores, implementation completeness, quiz score, (4) Technical highlights: from-scratch implementations (LinearRegression, PCA), real dataset analysis, comprehensive evaluation, (5) Lessons learned: most valuable skills gained, challenges overcome, (6) Repository structure: file organization guide, (7) How to run: environment setup, dependencies, execution instructions. Make this professional documentation of your capstone work. <strong>Boundaries:</strong> Be thorough in polishing—this represents 6 weeks of work. README should be detailed enough that someone else could understand your work. Include specific results/metrics demonstrating achievement. Use markdown formatting effectively (headings, lists, code blocks, links). <strong>Deliverable:</strong> All capstone artifacts polished to portfolio quality and comprehensive README documenting everything. <strong>Verification:</strong> All notebooks should run cleanly. README should comprehensively document capstone. Common pitfall: leaving rough edges—polish thoroughly. Success check: Is this capstone portfolio-ready? Could someone understand your work from README? <strong>Resources:</strong> <a href="https://www.makeareadme.com/" target="_blank" rel="noopener">How to write a README</a>' },
            { label: 'Reflect and create personal learning retrospective', estMinutes: 60, details: '<strong>Action:</strong> Create comprehensive retrospective document reflecting on 6-week foundations journey. Write docs/capstone/foundations_retrospective.md (700-900 words) covering: (1) Learning journey: key milestones, breakthroughs, challenges, (2) Technical growth: specific skills gained, before/after comparison, confidence in foundations topics, (3) Meta-learning insights: what study methods worked best? When did concepts click? How did your learning process evolve? (4) Unexpected discoveries: concepts you found fascinating or counterintuitive, connections between topics you didn\'t anticipate, (5) Challenges and solutions: most difficult topics, how you overcame struggles, resources that helped, (6) Time management: accuracy of time estimates, efficiency patterns, work habits that helped or hindered, (7) Looking forward: how strong is your foundation for Classical ML? What topics would benefit from ongoing practice? What learning strategies will you keep/change? (8) Advice to past self: what would you do differently knowing what you know now? Be honest and specific—this reflection solidifies learning and informs future phases. Include quantitative data where possible (hours spent, quiz score, R² achieved, etc.). <strong>Boundaries:</strong> Make it personal and genuine—this is for your benefit. Balance celebration of achievements with honest assessment of remaining gaps. Identify concrete patterns and actionable insights, not just feelings. Consider: social learning vs solo, morning vs evening study, video vs text, coding vs theory first. <strong>Deliverable:</strong> Thoughtful retrospective document capturing your 6-week foundations learning journey with specific insights and growth evidence. <strong>Verification:</strong> Should show deep reflection, not surface-level summaries. Should identify specific learning patterns and future improvements. Common pitfall: generic reflections—be specific and evidence-based. Success check: Does this document capture your growth? Will it help you learn better in future phases? <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2020/11/02/reflect-on-learning/" target="_blank" rel="noopener">How to Reflect on Learning</a>' }
          ],
          reflectionPrompt: 'Looking back at 6 weeks of foundations, what skill or insight will be most valuable going forward?'
        },
        {
          globalDay: 42,
          week: 6,
          title: 'Week 6 Review & Phase 1 Foundations Complete',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Comprehensive Phase 1 review: assess readiness for Classical ML', estMinutes: 75, details: '<strong>Action:</strong> Conduct final comprehensive review of entire Phase 1 (Weeks 1-6, 42 days) to assess Classical ML readiness. Review process: (1) Week-by-week: skim each week\'s notes and key notebooks, verify understanding of main concepts, identify any lingering confusion, (2) Cross-week connections: review your foundations_summary.md and concept_connections.md, verify you understand how topics integrate, (3) Implementation review: verify you can still implement key algorithms from scratch (linear regression, Ridge, PCA) with only API documentation, (4) Quiz retake: attempt questions you missed previously—are they now clear? (5) Readiness checklist: for each foundation topic, rate confidence 1-5 and identify if further practice needed. Create docs/reviews/phase1_complete.md with: (1) Topic-by-topic confidence ratings (1-5 scale), (2) Specific remaining gaps with plans to address, (3) Overall readiness assessment: are you ready for Classical ML? What prerequisites might need reinforcement during Phase 2 buffer? (4) Phase 1 statistics: total time invested, notebooks created, concepts mastered, quiz score, key achievements. Be thorough—this determines your Classical ML readiness. <strong>Boundaries:</strong> This is assessment, not re-learning—note gaps but don\'t try to fix everything now. Be honest about confidence levels—identifying gaps helps prevent future struggles. Review actively, not passively—test yourself with problems and implementations. Spend ~12 minutes per week reviewing (6 weeks × 12 min ≈ 75 min). <strong>Deliverable:</strong> Comprehensive Phase 1 completion review document with honest readiness assessment and statistical summary. <strong>Verification:</strong> Should accurately assess your foundation strength. Should identify specific topics needing maintenance. Common pitfall: overconfidence or underconfidence—calibrate honestly. Success check: Confidence ratings align with actual capability—can you implement what you claim to know? <strong>Resources:</strong> All your Phase 1 materials, <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Andrew Ng ML prerequisites</a>' },
            { label: 'Create Classical ML preview and learning strategy optimization', estMinutes: 60, details: '<strong>Action:</strong> Research Classical ML phase content and optimize learning strategy based on Phase 1 experience. Preview Classical ML: (1) Review Phase 3 curriculum: sklearn ecosystem, classification (logistic regression, SVM), ensemble methods (random forests, boosting), model selection (cross-validation, hyperparameter tuning), metrics (accuracy, precision, recall, AUC), (2) Identify foundation connections: how do concepts from Phase 1 enable these algorithms? Where will linear algebra, calculus, probability, statistics apply? (3) Prerequisites verification: do you have all necessary foundations? Any gaps to address in Phase 2 buffer? Learning strategy optimization: Based on Phase 1 experience, document: (1) What study methods worked best (video → implementation → notes? Or implementation → video for debugging?), (2) Optimal time of day and environment, (3) Balance of theory vs practice that works for you, (4) How to handle challenging concepts (ask for help when? Additional resources?), (5) Time estimation lessons (what takes longer than expected?), (6) Social learning opportunities (Discord usage, study partners?). Write docs/planning/phase2_strategy.md with Classical ML preview and optimized learning strategy going forward. <strong>Boundaries:</strong> Don\'t do deep dive into Classical ML content yet—high-level preview only. Focus strategy optimization on actionable patterns from Phase 1. Be specific: not "use more resources" but "when stuck >20 min on concept, watch Khan Academy explanation before continuing". Consider both what worked and what to change. <strong>Deliverable:</strong> Classical ML preview and optimized learning strategy document based on Phase 1 insights. <strong>Verification:</strong> Strategy should be informed by specific Phase 1 experiences. Should include concrete, actionable improvements. Common pitfall: generic advice—base strategy on your specific patterns. Success check: Does strategy address your specific learning patterns and challenges? <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/ultralearning/" target="_blank" rel="noopener">Ultralearning</a>, <a href="https://barbaraoakley.com/books/learn-like-a-pro/" target="_blank" rel="noopener">Learn Like a Pro</a>' },
            { label: 'Write Phase 1 completion log and celebrate achievements', estMinutes: 30, details: '<strong>Action:</strong> Create celebratory completion log documenting Phase 1 achievement. Write docs/logs/phase1_complete_log.md (500-700 words) including: (1) Achievement summary: 42 days completed, notebooks created, algorithms implemented from scratch, datasets analyzed, quiz score achieved, foundation topics mastered, (2) Highlight reel: top 5 most satisfying accomplishments (e.g., "Implemented PCA from SVD and saw it perfectly match sklearn", "Achieved R²=0.75 on California Housing from scratch"), (3) Challenges overcome: most difficult topics and how you conquered them, (4) Growth metrics: before vs after confidence in math/programming/ML, specific skills gained, (5) Favorite moments: breakthroughs, "aha!" insights, particularly elegant implementations, satisfying visualizations, (6) Gratitude: resources that helped most, people who supported your learning, (7) Looking forward: excitement for Classical ML, confidence in foundations, goals for next phase. Include quantitative achievements (time invested, code written, models trained) and qualitative growth (understanding deepened, confidence gained). Make this celebratory—you completed intensive 6-week foundations program! <strong>Boundaries:</strong> Make it positive and celebratory while being honest. Quantify achievements where possible. Reflect on growth—you know much more now than 6 weeks ago! Include both technical achievements and personal growth. Consider: you can now implement ML algorithms from mathematical first principles, understand research papers better, analyze real datasets competently. <strong>Deliverable:</strong> Celebratory Phase 1 completion log documenting achievement and growth. <strong>Verification:</strong> Should celebrate real achievements while acknowledging growth journey. Should convey genuine satisfaction with hard work paying off. Common pitfall: downplaying achievements—recognize how much you\'ve learned! Success check: Do you feel genuine pride reading this log? Does it capture your growth? <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2019/07/08/celebrate-success/" target="_blank" rel="noopener">Why Celebrating Matters</a>' }
          ],
          reflectionPrompt: 'What is your biggest achievement from Phase 1 foundations, and what does it mean for your ML journey?'
        },
      ]
    },
    {
      id: 'buffer-setup',
      title: 'Phase 2: Buffer & Structure Setup',
      description: 'Set up testing infrastructure, migrate to Deepnote, and establish weekly logging habits.',
      duration: '7 days (Week 7)',
      weeks: [7],
      days: [
        {
          globalDay: 43,
          week: 7,
          title: 'Pytest & Testing Infrastructure Setup',
          priority: 'HIGH',
          tasks: [
            { label: 'Install pytest, configure project structure, and understand testing principles', estMinutes: 60, details: '<strong>Action:</strong> Set up professional testing infrastructure for your ML project. Install pytest: <code>pip install pytest pytest-cov</code>. Create proper project structure: separate source code (<code>src/ml_foundations/</code>) from tests (<code>tests/</code>) and notebooks (<code>notebooks/</code>). Structure: <code>ml-project/ ├── src/ml_foundations/ │ ├── __init__.py │ ├── linear_models.py │ └── dimensionality_reduction.py ├── tests/ │ ├── __init__.py │ ├── test_linear_models.py │ └── test_pca.py ├── notebooks/ └── setup.py</code>. Understand testing principles: (1) Tests verify correctness, (2) Tests document expected behavior, (3) Tests enable refactoring with confidence, (4) Tests catch regressions early. Study pytest basics: test discovery (files starting with test_), test functions (starting with test_), assertions, fixtures, parametrization. Create setup.py for installable package. Study Test-Driven Development (TDD) mindset: red (failing test) → green (minimal code to pass) → refactor. <strong>Boundaries:</strong> Focus on project structure today—comprehensive tests come next. Use pytest conventions: tests/ mirrors src/ structure. Keep it simple initially—advanced features (fixtures, parametrize) come with practice. Review <a href="https://docs.pytest.org/en/stable/goodpractices.html" target="_blank" rel="noopener">pytest good practices</a>. <strong>Deliverable:</strong> Proper project structure with pytest installed, basic setup.py, empty test files created. Document in docs/setup/day43_testing_setup.md. <strong>Verification:</strong> <code>pytest --version</code> works, <code>pytest</code> discovers test files (even if empty), project is pip-installable in development mode (<code>pip install -e .</code>). Common pitfall: messy file organization—establish clean structure from start. Success check: Can you import your modules from both notebooks and tests? <strong>Resources:</strong> <a href="https://docs.pytest.org/en/stable/getting-started.html" target="_blank" rel="noopener">Pytest Getting Started</a>, <a href="https://realpython.com/pytest-python-testing/" target="_blank" rel="noopener">Real Python: Pytest Guide</a>, <a href="https://packaging.python.org/tutorials/packaging-projects/" target="_blank" rel="noopener">Python Packaging Tutorial</a>' },
            { label: 'Write comprehensive test suite for LinearRegression class', estMinutes: 90, details: '<strong>Action:</strong> Create comprehensive test suite for your from-scratch LinearRegression implementation. Move LinearRegression to <code>src/ml_foundations/linear_models.py</code> as a proper module. Write <code>tests/test_linear_models.py</code> with tests: (1) <code>test_fit_simple()</code>: fit on y=2x+3, verify coefficients within tolerance, (2) <code>test_predict()</code>: verify predictions match expected values, (3) <code>test_score()</code>: verify R² calculation correctness, (4) <code>test_normal_equation_vs_gd()</code>: both methods converge to same solution, (5) <code>test_sklearn_equivalence()</code>: results match sklearn.linear_model.LinearRegression, (6) <code>test_edge_cases()</code>: single feature, single sample, perfect fit, (7) <code>test_invalid_inputs()</code>: verify proper errors for wrong shapes/types. Use pytest fixtures for common test data: <code>@pytest.fixture def simple_data(): X = np.array([[1], [2], [3]]); y = np.array([3, 5, 7]); return X, y</code>. Use parametrize for testing multiple scenarios: <code>@pytest.mark.parametrize("method", ["normal_equation", "gradient_descent"])</code>. Implement TDD: write test first (red), make it pass (green), refactor for clarity. <strong>Boundaries:</strong> Write focused tests—each tests one thing. Use descriptive names: <code>test_prediction_accuracy_on_linear_data()</code> not <code>test1()</code>. Use appropriate tolerances for floating point comparisons: <code>np.allclose()</code> or <code>pytest.approx()</code>. Test both happy paths and error conditions. <strong>Deliverable:</strong> Complete test suite in tests/test_linear_models.py with ≥7 tests, all passing, demonstrating comprehensive coverage of LinearRegression functionality. <strong>Verification:</strong> <code>pytest tests/test_linear_models.py -v</code> shows all tests passing. Tests catch bugs if you intentionally break LinearRegression. Common pitfall: tests that don\'t actually verify correctness—use known ground truth. Success check: All tests pass; code coverage >80% for linear_models.py. <strong>Resources:</strong> <a href="https://docs.pytest.org/en/stable/how-to/fixtures.html" target="_blank" rel="noopener">Pytest Fixtures</a>, <a href="https://docs.pytest.org/en/stable/how-to/parametrize.html" target="_blank" rel="noopener">Pytest Parametrize</a>' },
            { label: 'Add coverage reporting and document testing workflow', estMinutes: 90, details: '<strong>Action:</strong> Set up code coverage tracking and document testing best practices. Generate coverage report: <code>pytest --cov=src/ml_foundations --cov-report=html --cov-report=term</code>. Review coverage report: identify untested code paths, aim for >80% coverage for core modules. Add coverage configuration in <code>setup.cfg</code> or <code>pyproject.toml</code>. Write missing tests to improve coverage—focus on important edge cases and error handling. Create docs/testing/testing_guide.md (600-800 words) documenting: (1) How to run tests: <code>pytest</code> (all), <code>pytest tests/test_linear_models.py</code> (specific file), <code>pytest -v</code> (verbose), (2) How to run with coverage: command and interpreting reports, (3) Testing best practices: one assertion per test when possible, descriptive names, fixtures for common setup, parametrize for similar tests, test public API not internals, (4) TDD workflow: write test first, implement minimal code to pass, refactor, (5) When to write tests: before implementing (TDD), after implementing (validation), when fixing bugs (regression tests), (6) Project testing strategy: what to test (core algorithms, edge cases, error handling), what not to test (trivial getters, third-party libraries). <strong>Boundaries:</strong> Don\'t obsess over 100% coverage—focus on important code paths. Testing trivial code wastes time. In guide, emphasize practical workflow over theory. Include concrete examples from your test suite. Make guide your reference for ongoing testing. <strong>Deliverable:</strong> Coverage report showing >80% for core modules and comprehensive testing guide documenting workflow and best practices. <strong>Verification:</strong> Coverage report is readable and highlights gaps. Guide is clear and actionable. Common pitfall: chasing 100% coverage including trivial code—prioritize important tests. Success check: Can you use your testing guide to maintain good testing habits going forward? <strong>Resources:</strong> <a href="https://pytest-cov.readthedocs.io/" target="_blank" rel="noopener">pytest-cov documentation</a>, <a href="https://martinfowler.com/bliki/TestCoverage.html" target="_blank" rel="noopener">Martin Fowler: Test Coverage</a>' }
          ],
          reflectionPrompt: 'Why is testing important for learning projects, not just production code?'
        },
        {
          globalDay: 44,
          week: 7,
          title: 'Code Formatting & Pre-commit Hooks',
          priority: 'HIGH',
          tasks: [
            { label: 'Install Black, isort, flake8 and understand code formatting philosophy', estMinutes: 90, details: '<strong>Action:</strong> Set up automatic code formatting toolchain for consistent, professional code style. Install tools: <code>pip install black isort flake8 pre-commit</code>. Understand each tool: (1) Black: opinionated code formatter, enforces consistent style automatically (line length, quotes, spacing), no configuration needed, "any color as long as it\'s black", (2) isort: sorts imports alphabetically and by type (stdlib, third-party, local), (3) flake8: linter catching style issues and potential bugs (unused imports, undefined variables, complexity), (4) pre-commit: framework for managing git hooks, runs checks before each commit. Configure Black in <code>pyproject.toml</code>: <code>[tool.black] line-length = 88 target-version = [\'py38\']</code>. Configure isort to work with Black: <code>[tool.isort] profile = "black"</code>. Create <code>.flake8</code> config file with reasonable settings. Run each tool manually on your code: observe formatting changes, fix any flake8 issues. Understand philosophy: automated formatting eliminates style debates, focuses code reviews on logic not style, maintains consistency across team/time. <strong>Boundaries:</strong> Accept Black\'s choices—don\'t fight the formatter. Use default settings unless you have strong reasons. Fix flake8 issues; don\'t just disable warnings. Study Black\'s philosophy—opinionated tools reduce decision fatigue. <strong>Deliverable:</strong> All formatting tools installed, configured, tested on your codebase. Document in docs/setup/day44_formatting.md with tool descriptions and configuration rationale. <strong>Verification:</strong> <code>black .</code> and <code>isort .</code> format code consistently. <code>flake8 .</code> shows no critical issues (warnings OK initially). Common pitfall: fighting Black\'s formatting—embrace the tool and move on. Success check: Code passes all formatters and linters. <strong>Resources:</strong> <a href="https://black.readthedocs.io/en/stable/" target="_blank" rel="noopener">Black Documentation</a>, <a href="https://pycqa.github.io/isort/" target="_blank" rel="noopener">isort Documentation</a>, <a href="https://flake8.pycqa.org/" target="_blank" rel="noopener">Flake8 Documentation</a>' },
            { label: 'Configure pre-commit hooks and integrate with git workflow', estMinutes: 75, details: '<strong>Action:</strong> Set up pre-commit hooks to automatically check code quality before every commit. Create <code>.pre-commit-config.yaml</code> in repository root: <code>repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v4.4.0 hooks: - id: trailing-whitespace - id: end-of-file-fixer - id: check-yaml - id: check-added-large-files - repo: https://github.com/psf/black rev: 23.3.0 hooks: - id: black - repo: https://github.com/pycqa/isort rev: 5.12.0 hooks: - id: isort - repo: https://github.com/pycqa/flake8 rev: 6.0.0 hooks: - id: flake8</code>. Install hooks: <code>pre-commit install</code>. Test hooks: <code>pre-commit run --all-files</code>. Make a commit to verify hooks run automatically. If hooks fail, fix issues and commit again. Understand workflow: (1) Stage changes with git add, (2) Attempt commit, (3) Pre-commit runs checks, (4) If checks fail, fix issues and try again, (5) If checks pass, commit succeeds. This ensures only clean, formatted code enters repository. Add hooks for pytest: <code>- repo: local hooks: - id: pytest id: pytest name: pytest entry: pytest language: system pass_filenames: false</code>. <strong>Boundaries:</strong> Start with basic hooks, add more as you become comfortable. Some hooks auto-fix (Black, isort), others require manual fixes (flake8, pytest). Don\'t bypass hooks with <code>--no-verify</code> except emergencies. Configure to run fast—skip slow tests in pre-commit, run those in CI. <strong>Deliverable:</strong> Working pre-commit configuration running formatters and linters automatically on every commit. Extend day44_formatting.md with pre-commit setup instructions. <strong>Verification:</strong> Attempting to commit unformatted code triggers hooks and blocks commit until fixed. <code>pre-commit run --all-files</code> passes on entire codebase. Common pitfall: overly strict hooks frustrating workflow—balance thoroughness with speed. Success check: Hooks run automatically on commit, improving code quality without manual effort. <strong>Resources:</strong> <a href="https://pre-commit.com/" target="_blank" rel="noopener">Pre-commit Documentation</a>, <a href="https://ljvmiranda921.github.io/notebook/2018/06/21/precommits-using-black-and-flake8/" target="_blank" rel="noopener">Guide: Pre-commit with Black and Flake8</a>' },
            { label: 'Set up GitHub Actions for CI and create formatting documentation', estMinutes: 60, details: '<strong>Action:</strong> Configure Continuous Integration (CI) with GitHub Actions to run checks on every push/PR. Create <code>.github/workflows/tests.yml</code>: <code>name: Tests on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: \'3.9\' - run: pip install -r requirements.txt - run: black --check . - run: isort --check-only . - run: flake8 . - run: pytest --cov=src</code>. Create <code>requirements.txt</code> listing all dependencies. Push to GitHub and verify workflow runs. Fix any CI failures. Understand CI benefits: catches issues early, ensures code quality across contributors, validates every change, provides confidence for merging. Write docs/development/formatting_guide.md (500-700 words) documenting: (1) Why formatting matters: consistency, readability, reduced bike-shedding, (2) Tools used: Black (formatter), isort (import sorter), flake8 (linter), pre-commit (automation), (3) How to use: commands for manual runs, automatic pre-commit behavior, (4) Fixing issues: common flake8 errors and solutions, handling pre-commit failures, (5) CI integration: what GitHub Actions checks, how to debug failures, (6) Best practices: commit frequently, run formatters before PR, don\'t bypass hooks, fix issues rather than disable checks. <strong>Boundaries:</strong> Keep CI fast (< 5 minutes ideally) so it doesn\'t block development. Consider caching dependencies. In documentation, provide practical examples of common scenarios and fixes. Make guide accessible to future you and collaborators. <strong>Deliverable:</strong> Working GitHub Actions CI, comprehensive formatting guide documenting tools and workflow. <strong>Verification:</strong> GitHub Actions runs successfully on push. Guide clearly explains formatting workflow. Common pitfall: complex CI taking too long—keep it simple and fast. Success check: Every push triggers CI checks; guide helps debug failures quickly. <strong>Resources:</strong> <a href="https://docs.github.com/en/actions/quickstart" target="_blank" rel="noopener">GitHub Actions Quickstart</a>, <a href="https://realpython.com/python-continuous-integration/" target="_blank" rel="noopener">Python CI Tutorial</a>' }
          ],
          reflectionPrompt: 'How do automated formatting tools improve code quality and reduce friction?'
        },
        {
          globalDay: 45,
          week: 7,
          title: 'Deepnote Migration & Cloud Environment',
          priority: 'HIGH',
          tasks: [
            { label: 'Set up Deepnote workspace and understand cloud notebook advantages', estMinutes: 90, details: '<strong>Action:</strong> Set up cloud development environment with Deepnote for collaborative, reproducible work. Sign up at <a href="https://deepnote.com" target="_blank" rel="noopener">deepnote.com</a> (free tier includes 750 hours/month). Create new project workspace. Understand advantages over local Jupyter: (1) Access from any device, (2) Pre-configured environment with common ML libraries, (3) Easy collaboration and sharing, (4) Version control integration, (5) Persistent cloud compute, (6) No local setup headaches. Explore Deepnote features: (1) Markdown + code cells like Jupyter, (2) Built-in git integration, (3) Environment variables for secrets, (4) Terminal access for package installation, (5) File browser and dataset uploads, (6) Sharing via link with access controls. Create requirements.txt for your project dependencies. Test importing your custom modules—either upload to workspace or install from git. Create test notebook demonstrating: data loading, your LinearRegression class, visualization, and markdown narrative. Compare experience with local Jupyter—note convenience and any limitations. <strong>Boundaries:</strong> Start with free tier to understand capabilities. Explore documentation and tutorials. Test that your from-scratch implementations work in Deepnote environment. Note any dependencies missing from base environment. Don\'t commit sensitive data or credentials—use environment variables. <strong>Deliverable:</strong> Working Deepnote workspace with test notebook successfully running your ML code. Document setup in docs/setup/day45_deepnote.md. <strong>Verification:</strong> Can access Deepnote from browser, run notebooks successfully, import your modules, save/load data. Common pitfall: treating cloud env like local—understand compute limits and persistence. Success check: Test notebook runs end-to-end in Deepnote showing all Phase 1 capabilities work. <strong>Resources:</strong> <a href="https://docs.deepnote.com/" target="_blank" rel="noopener">Deepnote Documentation</a>, <a href="https://docs.deepnote.com/integrations/github" target="_blank" rel="noopener">Deepnote GitHub Integration</a>' },
            { label: 'Migrate key notebooks to Deepnote and establish workflow', estMinutes: 75, details: '<strong>Action:</strong> Migrate important notebooks from Phase 1 to Deepnote and establish cloud-first workflow. Select 3-5 key notebooks to migrate: (1) Linear regression end-to-end, (2) PCA with reconstruction demo, (3) California Housing analysis, (4) Your integrated capstone notebook. For each: (1) Upload to Deepnote or clone from GitHub, (2) Verify all cells run successfully, (3) Fix any environment issues (missing packages, path problems), (4) Test data loading (upload datasets or use URLs), (5) Verify visualizations render properly, (6) Add README cell at top explaining notebook purpose. Create docs/setup/migration_checklist.md documenting: (1) Which notebooks migrated, (2) Environment setup steps (pip installs needed), (3) Data handling (where datasets stored, how to load), (4) Known issues and workarounds, (5) Differences from local environment, (6) When to use Deepnote vs local (Deepnote for collaboration/access anywhere, local for large-scale experiments). Establish workflow: primary development in Deepnote, sync to GitHub regularly, local only for special cases. <strong>Boundaries:</strong> Don\'t migrate everything—select representative examples. Test thoroughly to ensure reproducibility. Document any manual steps needed. Consider: Deepnote is collaborative, so make notebooks presentable. Use markdown cells for narrative. Organize files logically in workspace. <strong>Deliverable:</strong> 3-5 key notebooks running successfully in Deepnote, migration checklist documenting process and setup. <strong>Verification:</strong> Migrated notebooks run end-to-end without errors. All visualizations display correctly. Code produces expected results. Common pitfall: assuming identical to local—test thoroughly and document differences. Success check: Can you work on ML projects entirely in Deepnote going forward? <strong>Resources:</strong> <a href="https://docs.deepnote.com/collaboration/sharing-projects" target="_blank" rel="noopener">Deepnote Sharing Guide</a>' },
            { label: 'Create environment parity checklist and document best practices', estMinutes: 60, details: '<strong>Action:</strong> Create comprehensive environment parity checklist ensuring consistency across local and cloud. Write docs/setup/environment_parity.md (600-800 words) covering: (1) Dependencies: maintain requirements.txt with exact versions (<code>pip freeze > requirements.txt</code>), test installation in both environments, note any platform-specific issues, (2) Python version: use same version (3.8+ recommended) in both environments, (3) Data management: strategy for datasets (cloud storage, git LFS, or download scripts), avoid committing large files to git, document data sources and access methods, (4) Secrets management: use environment variables for API keys/credentials, never commit secrets to git, use Deepnote environment variables, (5) File paths: use relative paths not absolute, platform-independent path handling (pathlib), (6) Testing strategy: test critical notebooks in both environments periodically, automated tests should run in both. Create comparison table: Feature | Local | Deepnote | Notes. Add "Pre-flight Checklist" section: steps before starting new notebook in either environment (check dependencies, data access, git sync). Include troubleshooting section: common issues and solutions for environment mismatches. This document ensures your work is reproducible regardless of platform. <strong>Boundaries:</strong> Be thorough—environment issues are frustrating to debug. Document what you learn through trial and error. Make checklist actionable with specific commands and steps. Include examples from your migration experience. Consider: this supports collaboration and future-proofing your work. <strong>Deliverable:</strong> Comprehensive environment parity documentation with checklist, comparison table, and troubleshooting guide. <strong>Verification:</strong> Document accurately reflects both environments. Checklist is actionable and complete. Common pitfall: vague documentation—be specific with commands and file paths. Success check: Could someone else reproduce your environment using this documentation? <strong>Resources:</strong> <a href="https://12factor.net/" target="_blank" rel="noopener">The Twelve-Factor App</a>, <a href="https://realpython.com/python-virtual-environments-a-primer/" target="_blank" rel="noopener">Python Virtual Environments Guide</a>' }
          ],
          reflectionPrompt: 'How does cloud-based development with Deepnote change your ML workflow?'
        },
        {
          globalDay: 46,
          week: 7,
          title: 'Weekly Logging System & Progress Tracking',
          priority: 'HIGH',
          tasks: [
            { label: 'Design weekly logging template and establish logging cadence', estMinutes: 90, details: '<strong>Action:</strong> Create systematic weekly logging process for tracking progress, insights, and patterns throughout your ML journey. Design template in docs/templates/weekly_log_template.md including sections: (1) Week Overview: dates, phase, main topics, overall sentiment, (2) Accomplishments: specific deliverables (notebooks created, algorithms implemented, datasets analyzed), quantitative metrics (lines of code, test coverage, model R²), (3) Learning Highlights: key concepts mastered, "aha moments", connections discovered, (4) Challenges: difficult topics, time-consuming tasks, confusing concepts, how overcome, (5) Time Analysis: planned vs actual hours, time distribution across activities (theory, coding, debugging, documentation), efficiency observations, (6) Resources Used: most helpful materials (videos, docs, papers), new resources discovered, (7) Social Learning: Discord interactions, questions asked/answered, collaboration opportunities, (8) Looking Ahead: next week preview, preparation needed, concerns/excitement, (9) Meta-Learning: study habits that worked, improvements needed, energy/motivation patterns. Template should be comprehensive yet quick to fill (20-30 min max). Establish cadence: every Sunday evening, review week and fill template. This builds habit of reflection and progress tracking. <strong>Boundaries:</strong> Make template detailed enough for useful insights but not so burdensome it gets skipped. Balance quantitative (hours, metrics) with qualitative (feelings, insights). Focus on patterns and learning, not just task completion. Consider: weekly logs are data for understanding your learning process. Template should evolve based on what insights prove valuable. <strong>Deliverable:</strong> Comprehensive weekly log template and establishment of Sunday evening logging routine. Document in docs/setup/day46_weekly_logging.md. <strong>Verification:</strong> Template covers all important aspects of learning journey. Feels manageable to complete weekly. Common pitfall: overly complex template that becomes a chore—keep it useful not exhaustive. Success check: Can you complete the template for this week in 30 minutes with valuable insights? <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2020/11/02/reflect-on-learning/" target="_blank" rel="noopener">How to Reflect on Learning</a>, <a href="https://jamesclear.com/habit-stacking" target="_blank" rel="noopener">Habit Stacking</a>' },
            { label: 'Create Phase 1 retrospective using weekly logs and establish patterns', estMinutes: 75, details: '<strong>Action:</strong> Aggregate all Phase 1 weekly logs (Weeks 1-6) to identify patterns and insights. Create docs/retrospectives/phase1_aggregate_analysis.md analyzing: (1) Time patterns: total hours invested, distribution across weeks, correlation between estimated and actual time, activities consuming most/least time, (2) Learning velocity: topics mastered per week, difficulty trends over time, improvement in implementation speed, (3) Resource effectiveness: which types of resources worked best (videos, docs, coding practice), rating each major resource used, (4) Challenge patterns: recurring difficulties (math concepts, coding bugs, time management), how strategies evolved to handle challenges, (5) Energy/motivation: when did energy peak/dip, what affected motivation, how to maintain momentum, (6) Social learning: value of Discord, collaboration opportunities taken/missed, (7) Meta-patterns: optimal study times, effectiveness of different learning modes, efficiency improvements over time. Use your weekly logs as data—extract quotes, quantitative data, specific examples. Visualize: create charts showing time distribution, learning velocity, resource ratings. Identify: (1) What consistently worked (keep doing), (2) What didn\'t work (stop doing), (3) What to try (start doing). This analysis informs your approach to Classical ML phase. <strong>Boundaries:</strong> Be analytical and honest—identify real patterns, not just confirmation bias. Use specific evidence from logs. Quantify where possible. Don\'t judge yourself—learning has ups and downs, document them objectively. Consider: this is meta-learning—learning about how you learn best. <strong>Deliverable:</strong> Comprehensive Phase 1 learning pattern analysis with visualizations, identifying what works and what doesn\'t for your learning. <strong>Verification:</strong> Analysis reveals actionable insights about your learning process. Backed by specific evidence from logs. Common pitfall: superficial analysis without real patterns—dig deep into your data. Success check: Can you name 3 specific changes to your study approach based on this analysis? <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/ultralearning/" target="_blank" rel="noopener">Ultralearning: Meta-Learning</a>' },
            { label: 'Set up progress tracking dashboard and define success metrics', estMinutes: 60, details: '<strong>Action:</strong> Create quantitative progress tracking system for ongoing motivation and course correction. Design tracking spreadsheet or document (docs/tracking/progress_dashboard.md) with metrics: (1) Daily: hours studied, topics covered, tasks completed, (2) Weekly: notebooks created, tests written, algorithms implemented, R² scores achieved, quiz/assessment results, (3) Phase: cumulative hours, total deliverables, mastery ratings per topic, overall phase completion, (4) Long-term: phases completed, major milestones, portfolio pieces. Create visualization scripts or manual charts: (1) Cumulative hours over time (target line vs actual), (2) Weekly task completion rate, (3) Learning velocity (topics mastered per week), (4) Model performance improvement over time. Define success metrics for upcoming Classical ML phase: what does "success" look like? (1) Quantitative: X notebooks completed, Y tests passing, Z% coverage, model accuracy targets, (2) Qualitative: understanding concepts deeply, ability to implement from scratch, portfolio-ready projects. Set realistic targets based on Phase 1 experience. Include motivation system: celebrate milestones (every 50 hours, completed phase, achieved difficult goal). Write docs/setup/tracking_system.md (500-700 words) documenting: tracking methods, success metrics, how to maintain dashboard, how to use data for course correction, motivation strategies. <strong>Boundaries:</strong> Don\'t over-engineer—simple tracking is better than complex abandoned system. Focus on metrics that matter for learning, not just task completion. Balance quantitative tracking with qualitative reflection. Make updating dashboard quick (5-10 min/week max). <strong>Deliverable:</strong> Progress tracking dashboard with defined metrics, visualization approach, and documentation of tracking system. <strong>Verification:</strong> Dashboard is maintainable and provides motivating feedback. Metrics align with learning goals. Common pitfall: tracking everything and maintaining nothing—keep it simple and sustainable. Success check: Does tracking system motivate you? Can you sustain it long-term? <strong>Resources:</strong> <a href="https://jamesclear.com/measuring-progress" target="_blank" rel="noopener">James Clear: How to Measure Progress</a>, <a href="https://www.youtube.com/watch?v=mNeXuCYiE0U" target="_blank" rel="noopener">How to Track Goals</a>' }
          ],
          reflectionPrompt: 'What patterns in your learning have the weekly logs revealed?'
        },
        {
          globalDay: 47,
          week: 7,
          title: 'Repository Structure & Documentation Audit',
          priority: 'HIGH',
          tasks: [
            { label: 'Audit current repository structure and create organization plan', estMinutes: 90, details: '<strong>Action:</strong> Conduct comprehensive audit of your ML project repository and plan restructuring for clarity and scalability. Current state audit: (1) List all directories and their contents, (2) Identify disorganized areas (mixed concerns, unclear naming, scattered files), (3) Note missing documentation, (4) Check for consistency in naming conventions, (5) Assess if structure supports future growth. Design ideal structure following best practices: <code>ml-foundations/ ├── README.md (project overview) ├── LICENSE ├── .gitignore ├── requirements.txt ├── setup.py ├── pyproject.toml (tool configs) ├── src/ml_foundations/ (installable package) │ ├── __init__.py │ ├── linear_models.py │ ├── dimensionality_reduction.py │ └── utils.py ├── tests/ (mirrors src/) │ ├── test_linear_models.py │ └── test_dimensionality_reduction.py ├── notebooks/ (organized by phase/topic) │ ├── phase1_foundations/ │ ├── phase2_buffer_setup/ │ └── README.md (notebook index) ├── docs/ │ ├── setup/ (environment, tools) │ ├── notes/ (learning notes) │ ├── logs/ (weekly reflections) │ ├── capstone/ (Phase 1 capstone) │ └── README.md (docs overview) ├── data/ (with README) ├── artifacts/ (plots, models) ├── scripts/ (utility scripts) └── .github/workflows/ (CI/CD)</code>. Create reorganization plan with migration strategy (move files systematically, test after each change, update imports). <strong>Boundaries:</strong> Design for clarity (obvious where things go) and scalability (supports future phases). Use standard Python project conventions. Separate concerns: source code, tests, notebooks, documentation, data. Make navigation easy with README files at key levels. Consider: repository represents your work professionally—structure matters. <strong>Deliverable:</strong> Current state audit document and detailed reorganization plan in docs/setup/day47_repo_structure.md. <strong>Verification:</strong> Plan addresses current disorganization. New structure follows Python best practices. Migration strategy is safe and systematic. Common pitfall: over-engineering structure—keep it practical and intuitive. Success check: Would someone else find your repository organized and navigable? <strong>Resources:</strong> <a href="https://github.com/navdeep-G/samplemod" target="_blank" rel="noopener">Sample Python Project Structure</a>, <a href="https://packaging.python.org/tutorials/packaging-projects/" target="_blank" rel="noopener">Python Packaging Guide</a>, <a href="https://drivendata.github.io/cookiecutter-data-science/" target="_blank" rel="noopener">Cookiecutter Data Science</a>' },
            { label: 'Execute repository reorganization and update documentation', estMinutes: 75, details: '<strong>Action:</strong> Systematically reorganize repository following your plan. Migration process: (1) Create backup branch: <code>git checkout -b backup-pre-reorg</code>, (2) Work on new branch: <code>git checkout -b repo-reorg</code>, (3) Move files systematically: start with docs, then notebooks, then source code, (4) Update imports in code/tests after moving source files, (5) Update paths in notebooks, (6) Test after each major change: run pytest, try importing modules, execute key notebooks, (7) Update .gitignore to exclude artifacts and data (but commit .gitkeep in empty dirs), (8) Commit incrementally with clear messages: "Move source code to src/", "Reorganize notebooks by phase". Create/update README files: (1) Root README.md: project description, structure overview, setup instructions, usage examples, (2) notebooks/README.md: index of all notebooks with descriptions, (3) docs/README.md: documentation overview and navigation guide, (4) data/README.md: data sources, descriptions, how to obtain. Verify: (1) All tests pass in new structure, (2) Notebooks run correctly, (3) Package installable: <code>pip install -e .</code> works, (4) Documentation is findable and up-to-date. <strong>Boundaries:</strong> Work carefully—broken imports are frustrating. Test frequently. Commit incrementally so you can revert if needed. Use git mv to preserve history. Update CI/CD configs if paths changed. Don\'t rush—systematic beats fast but broken. <strong>Deliverable:</strong> Reorganized repository with updated structure, all tests passing, and current documentation. <strong>Verification:</strong> Repository follows new structure plan. All tests pass. Notebooks run. Package installs correctly. Common pitfall: breaking imports during reorganization—test frequently. Success check: Repository is now well-organized, navigable, and everything works. <strong>Resources:</strong> <a href="https://git-scm.com/docs/git-mv" target="_blank" rel="noopener">git mv documentation</a>, <a href="https://realpython.com/python-import/" target="_blank" rel="noopener">Python Imports Guide</a>' },
            { label: 'Update README, add LICENSE, and create CONTRIBUTING guide', estMinutes: 60, details: '<strong>Action:</strong> Polish repository with professional documentation making it portfolio-ready. Update README.md to comprehensive project documentation (800-1000 words): (1) Project Title and Description: what this repository contains (ML learning journey, from-scratch implementations), (2) Features: key capabilities (LinearRegression, Ridge, PCA, comprehensive notebooks), (3) Project Structure: directory layout with descriptions, (4) Installation: prerequisites, setup steps, virtual environment creation, dependency installation, (5) Usage: how to run notebooks, how to use your modules, example code snippets, (6) Testing: how to run pytest, coverage information, (7) Development: how to contribute, code style (Black/isort), pre-commit hooks, (8) Roadmap: phases completed and upcoming (Foundations complete, Classical ML next), (9) Learning Resources: key resources used, (10) Author and acknowledgments. Add LICENSE: choose appropriate license (MIT for permissive open source). Create CONTRIBUTING.md: guidelines for if you share repo or work with collaborators, includes: code style, testing requirements, commit conventions, issue/PR process. Add badges to README: build status (GitHub Actions), code coverage, license, Python version. This makes repository professional and shareable. <strong>Boundaries:</strong> Write README for external audience—explain clearly as if reader knows nothing about your project. Make installation/usage instructions concrete with exact commands. Choose permissive license unless you have reasons for restrictive. Consider: this documentation represents your work quality. Portfolio-ready means someone could understand and use your project from README alone. <strong>Deliverable:</strong> Comprehensive README, LICENSE file, CONTRIBUTING guide, making repository professional and portfolio-ready. <strong>Verification:</strong> README is complete, accurate, and well-formatted. Instructions can be followed by external user. License is appropriate. Common pitfall: vague or incomplete README—be thorough and concrete. Success check: Could someone clone your repo and get started using only the README? Would you be proud to share this publicly? <strong>Resources:</strong> <a href="https://www.makeareadme.com/" target="_blank" rel="noopener">How to Write a README</a>, <a href="https://choosealicense.com/" target="_blank" rel="noopener">Choose a License</a>, <a href="https://github.com/nayafia/contributing-template" target="_blank" rel="noopener">CONTRIBUTING Template</a>' }
          ],
          reflectionPrompt: 'How does good repository structure and documentation reflect on your work quality?'
        },
        {
          globalDay: 48,
          week: 7,
          title: 'Buffer Day: Catch-up & Light Learning',
          priority: 'HIGH',
          tasks: [
            { label: 'Complete any unfinished Phase 1 or Week 7 tasks', estMinutes: 90, details: '<strong>Action:</strong> Use buffer day to catch up on any incomplete or rushed work from Phase 1 or Week 7 setup tasks. Review task list: (1) Phase 1 (Days 1-42): identify any notebooks incomplete, implementations buggy, or understanding shaky, (2) Week 7 (Days 43-47): verify testing setup complete, formatting tools working, Deepnote migrated, weekly logging established, repo restructured. Prioritize by importance: (1) Critical: anything blocking Classical ML progress (shaky foundations, broken infrastructure), (2) High: incomplete major deliverables (capstone notebook, testing suite, documentation), (3) Medium: polish items (visualization quality, documentation completeness), (4) Low: nice-to-haves (additional tests, extra notes). Work systematically through priorities. For incomplete notebooks: finish code and narrative, ensure reproducibility. For infrastructure: verify all tools work correctly. For understanding gaps: review materials until clear. Don\'t rush—quality over speed. Track what you complete and what remains. This buffer prevents technical debt accumulation. <strong>Boundaries:</strong> Focus on important gaps, not perfection. Accept that some low-priority items may remain incomplete—that\'s okay. Don\'t start new work; finish existing work. Aim for "good enough" on polish items. Use time wisely: 90 minutes for catch-up, not procrastination. Consider: buffer days are for course correction and debt reduction, keeping you on track for long-term success. <strong>Deliverable:</strong> Completed or significantly progressed on high-priority incomplete items. Updated task tracking showing current status. <strong>Verification:</strong> Critical and high-priority items completed or on track. Infrastructure works correctly. Understanding gaps filled. Common pitfall: using buffer for new exploration instead of finishing existing work. Success check: Do you feel caught up and ready for Classical ML? <strong>Resources:</strong> Your task lists and weekly logs identifying gaps' },
            { label: 'Explore supplementary ML topics: interpretability, feature engineering, or real-world examples', estMinutes: 75, details: '<strong>Action:</strong> Use buffer time for light exploration of interesting ML topics not in core curriculum but valuable for broadening perspective. Choose 1-2 topics based on interest: (1) Model Interpretability: SHAP values, LIME, feature importance, partial dependence plots—understand why models make predictions, (2) Feature Engineering: domain-specific transforms, interaction terms, polynomial features, binning/discretization, encoding categorical variables—the art of crafting predictive features, (3) Real-world ML Workflow: data cleaning, handling missing values, outlier treatment, class imbalance, production deployment considerations, (4) AutoML: automated hyperparameter tuning, NAS (Neural Architecture Search), tools like TPOT or AutoKeras—future of ML?, (5) ML Ethics: fairness, bias, privacy, interpretability requirements, social impact—responsible ML practice. For chosen topics: (1) Watch 1-2 introductory videos (~20-30 min total), (2) Read blog posts or documentation overviews, (3) Try simple example if time permits, (4) Take brief notes on key concepts and why they matter. This is exploratory learning, not mastery—build awareness for future deep dives. <strong>Boundaries:</strong> Keep it light and interesting, not stressful. This is enrichment, not required curriculum. Don\'t go too deep—save that for dedicated learning time. Goal is exposure and motivation, not mastery. Choose topics that excite you. Consider: breadth now, depth later. Understanding the ML landscape helps you navigate it. <strong>Deliverable:</strong> Brief exploration notes (docs/exploration/buffer_day_exploration.md) documenting topics explored, key takeaways, and future learning interests. <strong>Verification:</strong> Notes capture interesting concepts and spark curiosity. Exploration felt enriching, not burdensome. Common pitfall: going too deep and creating new obligations—keep it light. Success check: Did exploration broaden your ML perspective enjoyably? <strong>Resources:</strong> <a href="https://christophm.github.io/interpretable-ml-book/" target="_blank" rel="noopener">Interpretable ML Book</a>, <a href="https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/" target="_blank" rel="noopener">Feature Engineering Book</a>, <a href="https://www.youtube.com/watch?v=NyzPxZm2BH0" target="_blank" rel="noopener">Google ML Crash Course</a>' },
            { label: 'Reflect and prepare mentally for Classical ML phase transition', estMinutes: 60, details: '<strong>Action:</strong> Use buffer day for reflection and mental preparation for transitioning from foundations to Classical ML. Reflection: Write docs/reflection/phase1_to_phase2_transition.md (500-700 words) covering: (1) Emotional journey: how do you feel completing foundations? Excited? Nervous? Confident? (2) Growth recognition: specific ways you\'ve grown (technical skills, study habits, confidence, persistence), compare yourself now to Day 1, (3) Foundation strength: honest assessment of your readiness for Classical ML, remaining uncertainties, confidence in different areas, (4) Study approach evolution: how has your learning process improved? What have you learned about learning? (5) Mindset shifts: how has your thinking about ML changed? What surprised you? What\'s clearer now? Mental preparation: (1) Review Classical ML phase overview: topics, duration, deliverables, (2) Set intentions: what do you want to achieve? How do you want to grow? (3) Identify potential challenges: what might be difficult? How will you handle struggles? (4) Renew commitment: why are you doing this? What motivates you? (5) Celebrate: acknowledge completing intensive 6-week foundations + 1-week setup! Define success for Classical ML: both outcomes (skills, projects) and process (consistent effort, deep understanding, enjoyable learning). <strong>Boundaries:</strong> Be honest and reflective, not performative. Recognize both strengths and areas for growth. Balance confidence with humility—you\'ve learned a lot but there\'s more ahead. Consider: transitions are opportunities to reset and optimize. Mental preparation prevents burnout and maintains motivation. <strong>Deliverable:</strong> Thoughtful transition reflection document showing self-awareness and readiness for next phase. <strong>Verification:</strong> Reflection shows genuine introspection. Mental preparation sets positive mindset for Classical ML. Common pitfall: skipping reflection as "not real work"—it\'s essential for sustained learning. Success check: Do you feel mentally ready and motivated for Classical ML? <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2019/11/11/beginning-ending/" target="_blank" rel="noopener">The Power of Transitions</a>, <a href="https://jamesclear.com/identity-based-habits" target="_blank" rel="noopener">Identity-Based Habits</a>' }
          ],
          reflectionPrompt: 'What unfinished business needs attention before moving to Classical ML?'
        },
        {
          globalDay: 49,
          week: 7,
          title: 'Week 7 Review & Classical ML Readiness',
          priority: 'HIGH',
          tasks: [
            { label: 'Review Week 7 infrastructure setup and verify all systems operational', estMinutes: 90, details: '<strong>Action:</strong> Comprehensive review of Week 7 infrastructure setup ensuring everything works correctly before Classical ML. Verification checklist: (1) Testing: pytest runs and passes on all tests, coverage >80% for core modules, tests catch bugs if code intentionally broken, (2) Formatting: Black/isort/flake8 installed and configured, pre-commit hooks run automatically on commit, GitHub Actions CI passes on recent commits, (3) Deepnote: workspace accessible, key notebooks migrated and running, environment parity documented, (4) Logging: weekly log template created, Phase 1 logs aggregated and analyzed, progress tracking dashboard established, (5) Repository: well-organized structure, comprehensive README, LICENSE and CONTRIBUTING files present, all documentation up-to-date. For each system, test end-to-end: don\'t just check it exists, verify it works. Fix any issues discovered. Document current state: create docs/reviews/week07_infrastructure_review.md listing: what works correctly, what needs fixing, what\'s optional for now. Create maintenance checklist: regular tasks to keep infrastructure healthy (weekly: run tests, log progress; monthly: update dependencies, review documentation). This ensures solid foundation for Classical ML phase. <strong>Boundaries:</strong> Be thorough—broken infrastructure disrupts learning. Test everything hands-on, don\'t assume. Fix critical issues now; note nice-to-haves for later. Document honestly—knowing what works and what doesn\'t prevents surprises. Consider: infrastructure enables efficient learning; investing in setup pays dividends. <strong>Deliverable:</strong> Verified, operational infrastructure with review document and maintenance checklist. <strong>Verification:</strong> All critical systems work correctly. Documentation accurate. Maintenance plan established. Common pitfall: cursory review without testing—verify thoroughly. Success check: Confidence that infrastructure won\'t block Classical ML progress. <strong>Resources:</strong> Your Week 7 setup documentation' },
            { label: 'Complete Classical ML readiness assessment and create phase preview', estMinutes: 75, details: '<strong>Action:</strong> Assess readiness for Classical ML and create detailed phase preview. Readiness assessment: Create docs/planning/classical_ml_readiness.md evaluating: (1) Foundation knowledge: rate confidence (1-5) in linear algebra, calculus, probability, statistics, Python, pandas/matplotlib, (2) Implementation skills: can you implement algorithms from scratch? Debug effectively? Write tests? (3) Infrastructure: testing, formatting, version control, cloud environment—all operational? (4) Study habits: effective learning strategies identified? Time management working? Motivation strong? (5) Overall readiness: scale 1-10, specific areas needing attention. Be honest—identifying gaps now prevents struggling later. For gaps, create brief remediation plan. Phase preview: Study Classical ML curriculum in detail. Create docs/planning/classical_ml_preview.md covering: (1) Overview: topics, duration (21 days), phases, (2) Week-by-week breakdown: Week 8 (sklearn, classification, logistic regression), Week 9 (ensemble methods, random forests, boosting), Week 10 (cross-validation, hyperparameter tuning, model selection), (3) Key concepts to learn: for each major topic, what you\'ll understand, implement, and apply, (4) Prerequisites check: do you have necessary foundations? Any review needed? (5) Success criteria: what does mastery look like for Classical ML? (6) Preparation tasks: anything to do before Day 50? (resources to bookmark, concepts to review), (7) Anticipated challenges: what might be difficult? How will you handle? (8) Excitement factors: what are you most looking forward to? <strong>Boundaries:</strong> Make readiness assessment honest and specific. Phase preview should be detailed enough to feel prepared but not overwhelming. Balance excitement with realism. Consider: preparation prevents poor performance. Understanding what\'s ahead enables better planning. <strong>Deliverable:</strong> Honest readiness assessment identifying any gaps and detailed Classical ML phase preview. <strong>Verification:</strong> Assessment is thorough and actionable. Preview provides clear picture of Classical ML phase. Common pitfall: overconfidence or underconfidence—calibrate honestly. Success check: Do you feel informed and ready (even if slightly nervous)? <strong>Resources:</strong> Phase 3 curriculum documentation, <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Andrew Ng ML Course</a>, <a href="https://scikit-learn.org/stable/tutorial/index.html" target="_blank" rel="noopener">sklearn Tutorials</a>' },
            { label: 'Write Phase 2 completion log and celebrate setup achievement', estMinutes: 60, details: '<strong>Action:</strong> Complete Phase 2 with celebratory reflection documenting infrastructure setup achievement. Write docs/logs/phase2_complete_log.md (500-700 words) celebrating: (1) Infrastructure built: testing framework (pytest, coverage), formatting pipeline (Black, isort, flake8, pre-commit, CI), cloud environment (Deepnote), logging system (templates, tracking), repository organization (clean structure, professional docs), (2) Systems operational: everything tested and working, ready to support Classical ML, (3) Skills gained: testing, code quality, DevOps basics, project organization, systematic learning, (4) Time investment: hours spent, efficiency improvements, (5) Challenges overcome: technical issues solved, habits established, (6) Looking forward: confidence in infrastructure, readiness for Classical ML, excitement for next phase, (7) Gratitude: resources that helped, progress made. Include quantitative achievements: lines of code formatted, tests written, coverage achieved, notebooks migrated, documents created. This is completion of setup phase—not as glamorous as ML but equally important. Phase 2 enables efficient Classical ML work. Celebrate building professional development environment! Create brief summary for weekly log following established template. Update progress tracking dashboard with Phase 2 completion. Consider: you now have professional-grade infrastructure supporting your ML journey. This is real achievement worth celebrating. <strong>Boundaries:</strong> Make it genuinely celebratory—you built significant infrastructure! Recognize that setup work enables future learning. Balance pride in achievement with readiness for next challenge. Be specific about what you accomplished. Consider: Phase 2 makes you more professional and efficient. <strong>Deliverable:</strong> Celebratory Phase 2 completion log and updated progress tracking showing two phases complete. <strong>Verification:</strong> Log shows genuine pride in infrastructure achievement. Progress tracking current and motivating. Common pitfall: downplaying infrastructure work—it\'s crucial for success! Success check: Do you feel proud of your infrastructure and ready for Classical ML? <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2019/07/08/celebrate-success/" target="_blank" rel="noopener">Importance of Celebration</a>' }
          ],
          reflectionPrompt: 'How does solid infrastructure support your learning, and what are you most excited about for Classical ML?'
        }
      ]
    },
    {
      id: 'classical-ml',
      title: 'Phase 3: Classical ML Fundamentals',
      description: 'Master sklearn, metrics, cross-validation, and learning curves.',
      duration: '21 days (Weeks 8-10)',
      weeks: [8, 9, 10],
      days: [
        {
          globalDay: 50,
          week: 8,
          title: 'Sklearn Introduction & First Classifier',
          priority: 'HIGH',
          tasks: [
            { label: 'DataCamp: Introduction to Machine Learning with scikit-learn Chapter 1', estMinutes: 90, details: '<strong>Action:</strong> Complete <a href="https://www.datacamp.com/courses/supervised-learning-with-scikit-learn" target="_blank" rel="noopener">DataCamp: Introduction to Machine Learning with scikit-learn Chapter 1</a>, focusing on the sklearn API pattern (fit, predict, score), supervised learning concepts, and classification vs regression. Take notes on estimator interface, hyperparameters, and the train-test split paradigm. Work through all exercises to understand how sklearn abstracts ML algorithms. <strong>Boundaries:</strong> Stop after Chapter 1—don\'t dive into ensemble methods or advanced topics yet. Focus on understanding the consistent sklearn API that makes all models interchangeable. <strong>Deliverable:</strong> Completed DataCamp chapter with notes on the sklearn design philosophy and basic workflow (import, instantiate, fit, predict). <strong>Verification:</strong> Can you explain the sklearn estimator pattern and why it\'s powerful? Common pitfall: treating sklearn as a black box—understand what fit() computes and what predict() returns. Success check: Can you train a simple classifier on toy data and evaluate it? Estimated time: 90 minutes. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/getting_started.html" target="_blank" rel="noopener">sklearn Getting Started</a>, <a href="https://scikit-learn.org/stable/tutorial/basic/tutorial.html" target="_blank" rel="noopener">sklearn Tutorial</a>' },
            { label: 'Create notebooks/classical_ml/day50_sklearn_intro.ipynb', estMinutes: 90, details: '<strong>Action:</strong> Create <code>notebooks/classical_ml/day50_sklearn_intro.ipynb</code> exploring the sklearn ecosystem. Structure: (1) Import sklearn and verify installation, (2) Load a toy dataset (load_iris or make_classification), (3) Explore dataset structure (.data, .target, .feature_names), (4) Demonstrate train_test_split with various split ratios, (5) Train a simple classifier (LogisticRegression or KNeighborsClassifier), (6) Evaluate with .score() method. Add markdown cells explaining each step, the sklearn API pattern, and why reproducible splits matter (random_state parameter). <strong>Boundaries:</strong> Keep it simple—use default hyperparameters, focus on workflow not model tuning. Include data exploration (shape, dtypes, class distribution) before modeling. <strong>Deliverable:</strong> A well-documented notebook demonstrating the end-to-end sklearn workflow with clear outputs and explanations. <strong>Verification:</strong> Notebook runs error-free, demonstrates fit-predict-score pattern, explains why train-test split prevents overfitting. Common pitfall: forgetting to set random_state, making results non-reproducible. Success check: Someone unfamiliar with sklearn should understand the basic workflow from your notebook. Estimated time: 90 minutes. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">train_test_split docs</a>, <a href="https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html" target="_blank" rel="noopener">Supervised Learning Tutorial</a>' },
            { label: 'Train first LogisticRegression on Iris dataset', estMinutes: 60, details: '<strong>Action:</strong> Train your first LogisticRegression model on the Iris dataset in the same notebook. Steps: (1) Load iris dataset with <code>from sklearn.datasets import load_iris</code>, (2) Examine the 4 features (sepal length/width, petal length/width) and 3 classes, (3) Split data 80/20 with stratification to preserve class balance, (4) Import and instantiate LogisticRegression with default params, (5) Fit on training data, (6) Predict on test set, (7) Calculate accuracy score. Add visualization of predictions vs actuals. <strong>Boundaries:</strong> Use default solver and regularization—don\'t tune hyperparameters yet. Focus on understanding what logistic regression computes (class probabilities via sigmoid). Work with all 150 samples, all 4 features, all 3 classes (multi-class classification). <strong>Deliverable:</strong> Trained LogisticRegression model achieving >90% accuracy on Iris test set with prediction analysis. <strong>Verification:</strong> Model accuracy >0.90, predictions align with ground truth for most samples. Common pitfall: forgetting to fit before predict—sklearn raises NotFittedError. Success check: Can you interpret the model\'s class probability predictions using predict_proba()? Estimated time: 60 minutes. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" target="_blank" rel="noopener">LogisticRegression docs</a>, <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html" target="_blank" rel="noopener">Iris Logistic Regression Example</a>' },
            { label: 'Visualize decision boundaries', estMinutes: 45, details: '<strong>Action:</strong> Visualize decision boundaries for your Iris classifier to understand how logistic regression partitions feature space. Implementation: (1) Select two features (petal length and petal width for clearest separation), (2) Create a meshgrid covering the feature space range, (3) Predict class for every point in the mesh, (4) Plot decision regions with contourf, (5) Overlay training points with scatter, color-coded by true class. Show how the model creates linear decision boundaries separating classes. Add 3D visualization if using 3 features. <strong>Boundaries:</strong> Start with 2D visualization (2 features) before attempting 3D. Use matplotlib\'s contourf for filled contours, scatter for data points. Include color bar and legend. Don\'t spend time on artistic styling—focus on clarity. <strong>Deliverable:</strong> Clear decision boundary plot showing how logistic regression separates Iris classes in feature space, saved as <code>artifacts/day50_decision_boundaries.png</code>. <strong>Verification:</strong> Plot clearly shows decision regions, data points, and class labels. Boundaries should be roughly linear. Common pitfall: meshgrid resolution too coarse—use at least 100 points per axis. Success check: Can you see where the model is confident vs uncertain? Estimated time: 45 minutes. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html" target="_blank" rel="noopener">Classifier Comparison Visualization</a>, <a href="https://matplotlib.org/stable/gallery/images_contours_and_fields/contourf_demo.html" target="_blank" rel="noopener">Contourf Demo</a>' },
            { label: 'Write docs/notes/day50_sklearn.md', estMinutes: 30, details: '<strong>Action:</strong> Write <code>docs/notes/day50_sklearn.md</code> summarizing sklearn fundamentals and your first classification experience. Structure: (1) The sklearn API philosophy (estimators, fit/predict pattern, consistency across models), (2) Why train-test split is essential (overfitting prevention, honest evaluation), (3) Logistic regression intuition (linear model + sigmoid for probabilities), (4) Decision boundaries concept (how models partition feature space), (5) Key takeaways and open questions. Aim for 500-700 words. Include code snippets showing the basic workflow pattern. <strong>Boundaries:</strong> Write in your own voice—don\'t copy docs. Focus on your understanding and insights from today. What surprised you? What was confusing? Explain concepts as if teaching a peer. <strong>Deliverable:</strong> Well-structured markdown document capturing sklearn fundamentals and first classification insights. <strong>Verification:</strong> Document explains sklearn API, train-test split rationale, and logistic regression basics clearly. No spelling errors, proper markdown formatting. Common pitfall: being too abstract—use concrete Iris example to ground explanations. Success check: Could you reference this document later to remember today\'s key insights? Estimated time: 30 minutes. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/developers/develop.html" target="_blank" rel="noopener">sklearn API Design</a>, <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html" target="_blank" rel="noopener">Python Data Science Handbook: sklearn</a>' }
          ],
          reflectionPrompt: 'How does sklearn abstract away the math we learned in foundations?'
        },
                {
          globalDay: 51,
          week: 8,
          title: 'Classification',
          priority: 'HIGH',
          tasks: [
            { label: 'Classification: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Classification through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Classification to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Classification in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Classification to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Classification is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Classification to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Classification: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Classification from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Classification: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Classification. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Classification after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Classification help?'
        },
        {
          globalDay: 52,
          week: 8,
          title: 'Regression',
          priority: 'HIGH',
          tasks: [
            { label: 'Regression: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Regression through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Regression to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Regression in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Regression to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Regression is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Regression to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Regression: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Regression from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Regression: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Regression. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Regression after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Regression help?'
        },
        {
          globalDay: 53,
          week: 8,
          title: 'Cross-validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Cross-validation: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Cross-validation through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Cross-validation to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Cross-validation in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Cross-validation to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Cross-validation is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Cross-validation to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Cross-validation: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Cross-validation from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Cross-validation: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Cross-validation. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Cross-validation after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Cross-validation help?'
        },
        {
          globalDay: 54,
          week: 8,
          title: 'Metrics',
          priority: 'HIGH',
          tasks: [
            { label: 'Metrics: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Metrics through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Metrics to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Metrics in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Metrics to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Metrics is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Metrics to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Metrics: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Metrics from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Metrics: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Metrics. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Metrics after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Metrics help?'
        },
        {
          globalDay: 55,
          week: 8,
          title: 'Learning curves',
          priority: 'HIGH',
          tasks: [
            { label: 'Learning curves: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Learning curves through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Learning curves to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Learning curves in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Learning curves to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Learning curves is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Learning curves to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Learning curves: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Learning curves from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Learning curves: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Learning curves. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Learning curves after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Learning curves help?'
        },
        {
          globalDay: 56,
          week: 8,
          title: 'Model selection',
          priority: 'HIGH',
          tasks: [
            { label: 'Model selection: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Model selection through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Model selection to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Model selection in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Model selection to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Model selection is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Model selection to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Model selection: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Model selection from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Model selection: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Model selection. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Model selection after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Model selection help?'
        },
        {
          globalDay: 57,
          week: 8,
          title: 'Week 8 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75, details: '<strong>Action:</strong> Conduct comprehensive review of Week 9 Classical ML material. Process: (1) Re-read all notes from this week, identifying key concepts and any remaining confusion, (2) Review all notebooks you created—can you follow the logic? Do results still make sense? (3) Revisit hardest concepts: re-watch relevant videos, re-read documentation, create additional examples if needed, (4) Test yourself: solve 2-3 problems from scratch without referring to previous solutions, (5) Create a one-page summary of the week\'s key insights and how they connect. <strong>Boundaries:</strong> Don\'t just skim—engage actively with the material. Focus on understanding, not memorization. If something is still unclear after review, mark it for deeper study or to ask mentors. Don\'t spend excessive time on material you\'ve mastered—focus on weak areas. Balance breadth (touching all topics) with depth (solidifying shaky concepts). <strong>Deliverable:</strong> Week 9 review document (<code>docs/notes/week9_review.md</code>) containing: (1) Summary of each day\'s key topics (2-3 sentences each), (2) Concepts that now make sense that didn\'t before, (3) Remaining questions or confusion (with plan to address), (4) 3-5 integration questions testing cross-topic understanding, (5) Self-assessment of mastery (1-5 scale) for each major topic with justification. <strong>Verification:</strong> Review document demonstrates genuine engagement and honest self-assessment. Can you explain this week\'s concepts to someone else? Can you solve problems combining multiple concepts from the week? Your self-assessment should identify both strengths and areas needing more work. Common pitfall: Passive re-reading without active problem-solving—you need to test your understanding. Another pitfall: overconfidence from recognizing concepts vs actually being able to use them. Success check: You should feel more confident about the week\'s material and have a clear plan for any remaining gaps. Estimated time: 75 minutes for thorough, active review. <strong>Resources:</strong> Your own notes and notebooks from the week, <a href="https://www.scotthyoung.com/blog/2020/05/04/study-better/" target="_blank" rel="noopener">Effective Study Techniques</a>, <a href="https://www.learningscientists.org/learning-scientists-podcast" target="_blank" rel="noopener">Learning Scientists Podcast</a>' },
            { label: 'Practice exercises', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Practice exercises. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Practice exercises after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' },
            { label: 'Write weekly log', estMinutes: 30, details: '<strong>Action:</strong> Write your Week 9 log documenting progress, challenges, and insights. Structure: (1) Overview: major topics covered, time spent, overall feeling about the week, (2) Achievements: what went well, breakthroughs, successful implementations, concepts that clicked, (3) Challenges: what was difficult, topics needing more work, technical issues encountered, time management struggles, (4) Key learnings: 3-5 major insights or "aha moments" from the week, (5) Looking ahead: goals for next week, specific concepts to reinforce, adjustments to study approach if needed. Be honest and reflective—this log is for you. <strong>Boundaries:</strong> Keep it concise but substantive—aim for 400-600 words. Focus on learning and growth, not just activities. Don\'t just list what you did—reflect on what you learned and how you\'re evolving as a learner. Be specific with examples rather than generic ("I struggled with cross-validation overfitting on the Wine dataset" not "some things were hard"). <strong>Deliverable:</strong> Weekly log document saved as <code>docs/logs/week9_log.md</code> with structured reflection on progress, challenges, learnings, and forward-looking goals. <strong>Verification:</strong> Log demonstrates genuine reflection and honest self-assessment. Contains specific examples and insights, not generic platitudes. Shows growth mindset—challenges viewed as learning opportunities. Includes actionable plans for improvement. Common pitfall: Writing perfunctory logs just to check a box—invest in reflection, it pays dividends in learning efficiency. Another pitfall: being too harsh or too easy on yourself—aim for honest, balanced assessment. Success check: Could you review this log in a month and remember the week clearly? Does it help you see your growth trajectory? Estimated time: 30 minutes for thoughtful reflection and writing. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2010/01/11/learn-faster-by-writing-twice/" target="_blank" rel="noopener">Learning Through Reflection</a>, <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Learning Principles</a>' }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 58,
          week: 9,
          title: 'Classification',
          priority: 'HIGH',
          tasks: [
            { label: 'Classification: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Classification through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Classification to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Classification in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Classification to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Classification is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Classification to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Classification: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Classification from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Classification: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Classification. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Classification after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Classification help?'
        },
        {
          globalDay: 59,
          week: 9,
          title: 'Regression',
          priority: 'HIGH',
          tasks: [
            { label: 'Regression: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Regression through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Regression to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Regression in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Regression to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Regression is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Regression to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Regression: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Regression from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Regression: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Regression. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Regression after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Regression help?'
        },
        {
          globalDay: 60,
          week: 9,
          title: 'Cross-validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Cross-validation: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Cross-validation through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Cross-validation to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Cross-validation in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Cross-validation to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Cross-validation is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Cross-validation to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Cross-validation: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Cross-validation from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Cross-validation: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Cross-validation. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Cross-validation after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Cross-validation help?'
        },
        {
          globalDay: 61,
          week: 9,
          title: 'Metrics',
          priority: 'HIGH',
          tasks: [
            { label: 'Metrics: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Metrics through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Metrics to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Metrics in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Metrics to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Metrics is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Metrics to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Metrics: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Metrics from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Metrics: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Metrics. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Metrics after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Metrics help?'
        },
        {
          globalDay: 62,
          week: 9,
          title: 'Learning curves',
          priority: 'HIGH',
          tasks: [
            { label: 'Learning curves: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Learning curves through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Learning curves to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Learning curves in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Learning curves to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Learning curves is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Learning curves to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Learning curves: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Learning curves from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Learning curves: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Learning curves. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Learning curves after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Learning curves help?'
        },
        {
          globalDay: 63,
          week: 9,
          title: 'Model selection',
          priority: 'HIGH',
          tasks: [
            { label: 'Model selection: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Model selection through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Model selection to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Model selection in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Model selection to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Model selection is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Model selection to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Model selection: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Model selection from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Model selection: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Model selection. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Model selection after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Model selection help?'
        },
        {
          globalDay: 64,
          week: 9,
          title: 'Week 9 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75, details: '<strong>Action:</strong> Conduct comprehensive review of Week 10 Classical ML material. Process: (1) Re-read all notes from this week, identifying key concepts and any remaining confusion, (2) Review all notebooks you created—can you follow the logic? Do results still make sense? (3) Revisit hardest concepts: re-watch relevant videos, re-read documentation, create additional examples if needed, (4) Test yourself: solve 2-3 problems from scratch without referring to previous solutions, (5) Create a one-page summary of the week\'s key insights and how they connect. <strong>Boundaries:</strong> Don\'t just skim—engage actively with the material. Focus on understanding, not memorization. If something is still unclear after review, mark it for deeper study or to ask mentors. Don\'t spend excessive time on material you\'ve mastered—focus on weak areas. Balance breadth (touching all topics) with depth (solidifying shaky concepts). <strong>Deliverable:</strong> Week 10 review document (<code>docs/notes/week10_review.md</code>) containing: (1) Summary of each day\'s key topics (2-3 sentences each), (2) Concepts that now make sense that didn\'t before, (3) Remaining questions or confusion (with plan to address), (4) 3-5 integration questions testing cross-topic understanding, (5) Self-assessment of mastery (1-5 scale) for each major topic with justification. <strong>Verification:</strong> Review document demonstrates genuine engagement and honest self-assessment. Can you explain this week\'s concepts to someone else? Can you solve problems combining multiple concepts from the week? Your self-assessment should identify both strengths and areas needing more work. Common pitfall: Passive re-reading without active problem-solving—you need to test your understanding. Another pitfall: overconfidence from recognizing concepts vs actually being able to use them. Success check: You should feel more confident about the week\'s material and have a clear plan for any remaining gaps. Estimated time: 75 minutes for thorough, active review. <strong>Resources:</strong> Your own notes and notebooks from the week, <a href="https://www.scotthyoung.com/blog/2020/05/04/study-better/" target="_blank" rel="noopener">Effective Study Techniques</a>, <a href="https://www.learningscientists.org/learning-scientists-podcast" target="_blank" rel="noopener">Learning Scientists Podcast</a>' },
            { label: 'Practice exercises', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Practice exercises. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Practice exercises after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' },
            { label: 'Write weekly log', estMinutes: 30, details: '<strong>Action:</strong> Write your Week 10 log documenting progress, challenges, and insights. Structure: (1) Overview: major topics covered, time spent, overall feeling about the week, (2) Achievements: what went well, breakthroughs, successful implementations, concepts that clicked, (3) Challenges: what was difficult, topics needing more work, technical issues encountered, time management struggles, (4) Key learnings: 3-5 major insights or "aha moments" from the week, (5) Looking ahead: goals for next week, specific concepts to reinforce, adjustments to study approach if needed. Be honest and reflective—this log is for you. <strong>Boundaries:</strong> Keep it concise but substantive—aim for 400-600 words. Focus on learning and growth, not just activities. Don\'t just list what you did—reflect on what you learned and how you\'re evolving as a learner. Be specific with examples rather than generic ("I struggled with cross-validation overfitting on the Wine dataset" not "some things were hard"). <strong>Deliverable:</strong> Weekly log document saved as <code>docs/logs/week10_log.md</code> with structured reflection on progress, challenges, learnings, and forward-looking goals. <strong>Verification:</strong> Log demonstrates genuine reflection and honest self-assessment. Contains specific examples and insights, not generic platitudes. Shows growth mindset—challenges viewed as learning opportunities. Includes actionable plans for improvement. Common pitfall: Writing perfunctory logs just to check a box—invest in reflection, it pays dividends in learning efficiency. Another pitfall: being too harsh or too easy on yourself—aim for honest, balanced assessment. Success check: Could you review this log in a month and remember the week clearly? Does it help you see your growth trajectory? Estimated time: 30 minutes for thoughtful reflection and writing. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2010/01/11/learn-faster-by-writing-twice/" target="_blank" rel="noopener">Learning Through Reflection</a>, <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Learning Principles</a>' }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 65,
          week: 10,
          title: 'Classification',
          priority: 'HIGH',
          tasks: [
            { label: 'Classification: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Classification through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Classification to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Classification in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Classification to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Classification is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Classification to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Classification: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Classification from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Classification: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Classification. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Classification after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Classification help?'
        },
        {
          globalDay: 66,
          week: 10,
          title: 'Regression',
          priority: 'HIGH',
          tasks: [
            { label: 'Regression: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Regression through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Regression to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Regression in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Regression to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Regression is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Regression to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Regression: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Regression from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Regression: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Regression. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Regression after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Regression help?'
        },
        {
          globalDay: 67,
          week: 10,
          title: 'Cross-validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Cross-validation: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Cross-validation through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Cross-validation to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Cross-validation in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Cross-validation to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Cross-validation is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Cross-validation to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Cross-validation: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Cross-validation from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Cross-validation: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Cross-validation. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Cross-validation after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Cross-validation help?'
        },
        {
          globalDay: 68,
          week: 10,
          title: 'Metrics',
          priority: 'HIGH',
          tasks: [
            { label: 'Metrics: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Metrics through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Metrics to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Metrics in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Metrics to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Metrics is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Metrics to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Metrics: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Metrics from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Metrics: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Metrics. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Metrics after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Metrics help?'
        },
        {
          globalDay: 69,
          week: 10,
          title: 'Learning curves',
          priority: 'HIGH',
          tasks: [
            { label: 'Learning curves: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Learning curves through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Learning curves to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Learning curves in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Learning curves to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Learning curves is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Learning curves to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Learning curves: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Learning curves from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Learning curves: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Learning curves. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Learning curves after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Learning curves help?'
        },
        {
          globalDay: 70,
          week: 10,
          title: 'Model selection',
          priority: 'HIGH',
          tasks: [
            { label: 'Model selection: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Model selection through official documentation, tutorial videos, and worked examples. Focus on understanding algorithm intuition, when to use each method, strengths/weaknesses, and connections to mathematical foundations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Model selection to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Model selection in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Model selection to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Model selection is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Model selection to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">sklearn User Guide</a>, <a href="https://hastie.su.domains/ElemStatLearn/" target="_blank" rel="noopener">Elements of Statistical Learning</a>' },
            { label: 'Model selection: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Model selection from scratch following best practices and the scikit-learn API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/classical_ml/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following scikit-learn conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run all cells error-free, check model accuracy against baselines, verify predictions make sense (no NaNs, correct shapes, reasonable values). Compare against sklearn documentation examples. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">sklearn API Reference</a>, <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples Gallery</a>' },
            { label: 'Model selection: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Model selection. Work through at least 5-8 problems or experiments of varying difficulty covering different datasets (classification and regression), various hyperparameter settings, comparing multiple algorithms, analyzing failure cases. Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for model performance metrics (accuracy, precision, recall, F1, AUC), proper cross-validation on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Model selection after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Model selection help?'
        }
      ]
    },
    {
      id: 'deep-learning',
      title: 'Phase 4: Deep Learning Core',
      description: 'PyTorch fundamentals, MLP/CNN architectures, training stability, and CIFAR-10.',
      duration: '49 days (Weeks 11-17)',
      weeks: [11, 12, 13, 14, 15, 16, 17],
      days: [
        {
          globalDay: 71,
          week: 11,
          title: 'PyTorch Tensors & Autograd',
          priority: 'HIGH',
          tasks: [
            { label: 'Complete PyTorch 60-minute blitz tutorial', estMinutes: 90, details: '<strong>Action:</strong> Complete the <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">PyTorch 60-minute blitz tutorial</a>, focusing on tensor fundamentals, autograd basics, and neural network building blocks. Work through all sections: What is PyTorch?, Autograd, Neural Networks, and Training a Classifier. Take notes on tensor operations, requires_grad flag, backward() computation, nn.Module structure, and optimizer step pattern. Run all code examples and experiment with modifications. <strong>Boundaries:</strong> Complete the full tutorial but don\'t diverge into advanced topics. Focus on understanding PyTorch\'s eager execution model (vs TensorFlow\'s graph model), dynamic computational graphs, and the manual training loop structure. <strong>Deliverable:</strong> Completed tutorial with running code examples and notes on PyTorch fundamentals (tensors, autograd, nn.Module, DataLoader, optimizer pattern). <strong>Verification:</strong> Can you explain the difference between NumPy arrays and PyTorch tensors? Can you describe what happens when you call loss.backward()? Common pitfall: skipping autograd concepts—they\'re foundational for understanding how gradients flow. Success check: Can you write a minimal training loop from scratch? Estimated time: 90 minutes. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://pytorch.org/docs/stable/tensors.html" target="_blank" rel="noopener">Tensor Documentation</a>' },
            { label: 'Create notebooks/deep_learning/day71_pytorch_intro.ipynb', estMinutes: 90, details: '<strong>Action:</strong> Create <code>notebooks/deep_learning/day71_pytorch_intro.ipynb</code> exploring PyTorch tensor operations and GPU acceleration. Structure: (1) Import torch and check version, (2) Create tensors from lists, NumPy arrays, and using factory functions (zeros, ones, randn), (3) Demonstrate tensor operations (arithmetic, indexing, reshaping, broadcasting), (4) Show tensor-NumPy interoperability, (5) Explore device management (CPU vs CUDA), (6) Demonstrate moving tensors to GPU if available and timing operations, (7) Show in-place operations (underscore suffix). Compare performance of CPU vs GPU for large matrix multiplication. <strong>Boundaries:</strong> Focus on core tensor operations needed for deep learning—don\'t explore every tensor method. Include examples of common shapes: batch_size × channels × height × width. Verify CUDA availability but don\'t require it. <strong>Deliverable:</strong> Comprehensive notebook demonstrating PyTorch tensor fundamentals with performance comparisons and clear explanations. <strong>Verification:</strong> Notebook runs on both CPU and GPU (if available), demonstrates key tensor operations, shows performance benefits of GPU. Common pitfall: forgetting .to(device) when mixing CPU and GPU tensors—raises errors. Success check: Can you create, manipulate, and move tensors confidently? Estimated time: 90 minutes. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html" target="_blank" rel="noopener">Tensor Tutorial</a>, <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">torch Package Documentation</a>' },
            { label: 'Experiment with torch.autograd on simple functions', estMinutes: 60, details: '<strong>Action:</strong> Experiment with <code>torch.autograd</code> on simple mathematical functions to understand automatic differentiation. Examples: (1) Define scalar function f(x) = x², compute df/dx = 2x using autograd, (2) Multi-variable function f(x,y) = x²y + y³, compute partial derivatives, (3) Vector function and Jacobian, (4) Chain rule demonstration with composed functions, (5) Explore .backward(), .grad attribute, and gradient accumulation. Verify gradients match analytical derivatives you compute by hand. Visualize how gradients change with input values. <strong>Boundaries:</strong> Start with simple scalar functions before vectors. Always verify autograd output against hand-calculated derivatives. Use requires_grad=True explicitly. Understand when gradients accumulate vs reset (zero_grad). <strong>Deliverable:</strong> Notebook section with 4-5 autograd examples, comparing analytical vs automatic gradients, demonstrating gradient flow through computations. <strong>Verification:</strong> All autograd results match hand-calculated derivatives within floating-point precision. Can explain what computational graph autograd builds. Common pitfall: forgetting to zero gradients between backward() calls—they accumulate! Success check: Can you explain how autograd enables efficient neural network training? Estimated time: 60 minutes. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" target="_blank" rel="noopener">Autograd Tutorial</a>, <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener">Autograd Documentation</a>' },
            { label: 'Implement gradient descent manually with autograd', estMinutes: 75, details: '<strong>Action:</strong> Implement gradient descent manually using autograd to minimize a simple function. Problem: Find minimum of f(x,y) = (x-3)² + (y+2)² using gradient descent. Implementation: (1) Initialize parameters as tensors with requires_grad=True, (2) Set learning rate (try 0.1), (3) Loop for N iterations: compute function value, call backward(), manually update parameters with gradient step (x = x - lr * x.grad), zero gradients, (4) Track and plot function value vs iteration, (5) Visualize optimization trajectory on contour plot. Demonstrate convergence to true minimum (x=3, y=-2). Try different learning rates and observe behavior. <strong>Boundaries:</strong> Implement the update step manually (don\'t use torch.optim yet). Understand each component: loss computation, backward pass, gradient descent update, gradient zeroing. Start with simple quadratic—it\'s convex with unique minimum. <strong>Deliverable:</strong> Working gradient descent implementation finding function minimum, with convergence plot and trajectory visualization. <strong>Verification:</strong> Algorithm converges to correct minimum within tolerance (|x-3| < 0.01, |y+2| < 0.01) within 100 iterations. Function value decreases monotonically (if lr not too large). Common pitfall: forgetting to detach parameters from graph during update—use .data or .detach(). Success check: Can you explain why learning rate matters and what happens if it\'s too large/small? Estimated time: 75 minutes. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html" target="_blank" rel="noopener">Autograd Example</a>, <a href="https://pytorch.org/docs/stable/notes/autograd.html" target="_blank" rel="noopener">Autograd Mechanics</a>' },
            { label: 'Write docs/notes/day71_pytorch.md', estMinutes: 30, details: '<strong>Action:</strong> Write <code>docs/notes/day71_pytorch.md</code> summarizing PyTorch fundamentals and autograd magic. Structure: (1) PyTorch vs NumPy: when to use each, key differences (GPU support, autograd, dynamic graphs), (2) Tensor basics: creation, operations, device management, (3) Autograd mechanics: computational graphs, backward pass, gradient computation, (4) Manual gradient descent: the training loop building block, (5) Why autograd is revolutionary for deep learning (eliminates hand-coded derivatives, enables experimentation), (6) Key insights and open questions. Aim for 600-800 words. <strong>Boundaries:</strong> Focus on conceptual understanding not API reference. Explain why these tools matter for building neural networks. Use analogies where helpful (computational graph like recipe, backward like backtracking). <strong>Deliverable:</strong> Clear, insightful markdown document explaining PyTorch fundamentals and their significance for deep learning. <strong>Verification:</strong> Document explains tensors, autograd, and gradient descent clearly with examples. Proper markdown formatting, no spelling errors. Common pitfall: listing features without explaining why they matter—connect to your learning goals. Success check: Could this document help you remember PyTorch fundamentals weeks later? Estimated time: 30 minutes. <strong>Resources:</strong> <a href="https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/" target="_blank" rel="noopener">PyTorch Computational Graphs</a>, <a href="https://pytorch.org/docs/stable/notes/extending.html" target="_blank" rel="noopener">Extending PyTorch</a>' }
          ],
          reflectionPrompt: 'How does autograd differ from manual gradient computation?'
        },
                {
          globalDay: 72,
          week: 11,
          title: 'PyTorch',
          priority: 'HIGH',
          tasks: [
            { label: 'PyTorch: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of PyTorch through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting PyTorch to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining PyTorch in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain PyTorch to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what PyTorch is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect PyTorch to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'PyTorch: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement PyTorch from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'PyTorch: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of PyTorch. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with PyTorch after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does PyTorch help?'
        },
        {
          globalDay: 73,
          week: 11,
          title: 'Autograd',
          priority: 'HIGH',
          tasks: [
            { label: 'Autograd: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Autograd through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Autograd to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Autograd in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Autograd to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Autograd is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Autograd to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Autograd: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Autograd from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Autograd: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Autograd. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Autograd after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Autograd help?'
        },
        {
          globalDay: 74,
          week: 11,
          title: 'MLP',
          priority: 'HIGH',
          tasks: [
            { label: 'MLP: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of MLP through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting MLP to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining MLP in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain MLP to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what MLP is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect MLP to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'MLP: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement MLP from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'MLP: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of MLP. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with MLP after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does MLP help?'
        },
        {
          globalDay: 75,
          week: 11,
          title: 'CNN',
          priority: 'HIGH',
          tasks: [
            { label: 'CNN: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of CNN through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting CNN to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining CNN in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain CNN to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what CNN is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect CNN to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'CNN: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement CNN from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'CNN: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of CNN. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with CNN after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does CNN help?'
        },
        {
          globalDay: 76,
          week: 11,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Training through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Training to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Training in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Training to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Training is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Training to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Training: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Training from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Training: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Training. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Training after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 77,
          week: 11,
          title: 'Optimization',
          priority: 'HIGH',
          tasks: [
            { label: 'Optimization: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Optimization through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Optimization to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Optimization in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Optimization to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Optimization is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Optimization to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Optimization: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Optimization from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Optimization: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Optimization. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Optimization after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Optimization help?'
        },
        {
          globalDay: 78,
          week: 11,
          title: 'Week 11 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75, details: '<strong>Action:</strong> Conduct comprehensive review of Week 12 Deep Learning material. Process: (1) Re-read all notes from this week, identifying key concepts and any remaining confusion, (2) Review all notebooks you created—can you follow the logic? Do results still make sense? (3) Revisit hardest concepts: re-watch relevant videos, re-read documentation, create additional examples if needed, (4) Test yourself: solve 2-3 problems from scratch without referring to previous solutions, (5) Create a one-page summary of the week\'s key insights and how they connect. <strong>Boundaries:</strong> Don\'t just skim—engage actively with the material. Focus on understanding, not memorization. If something is still unclear after review, mark it for deeper study or to ask mentors. Don\'t spend excessive time on material you\'ve mastered—focus on weak areas. Balance breadth (touching all topics) with depth (solidifying shaky concepts). <strong>Deliverable:</strong> Week 12 review document (<code>docs/notes/week12_review.md</code>) containing: (1) Summary of each day\'s key topics (2-3 sentences each), (2) Concepts that now make sense that didn\'t before, (3) Remaining questions or confusion (with plan to address), (4) 3-5 integration questions testing cross-topic understanding, (5) Self-assessment of mastery (1-5 scale) for each major topic with justification. <strong>Verification:</strong> Review document demonstrates genuine engagement and honest self-assessment. Can you explain this week\'s concepts to someone else? Can you solve problems combining multiple concepts from the week? Your self-assessment should identify both strengths and areas needing more work. Common pitfall: Passive re-reading without active problem-solving—you need to test your understanding. Another pitfall: overconfidence from recognizing concepts vs actually being able to use them. Success check: You should feel more confident about the week\'s material and have a clear plan for any remaining gaps. Estimated time: 75 minutes for thorough, active review. <strong>Resources:</strong> Your own notes and notebooks from the week, <a href="https://www.scotthyoung.com/blog/2020/05/04/study-better/" target="_blank" rel="noopener">Effective Study Techniques</a>, <a href="https://www.learningscientists.org/learning-scientists-podcast" target="_blank" rel="noopener">Learning Scientists Podcast</a>' },
            { label: 'Practice exercises', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Practice exercises. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Practice exercises after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' },
            { label: 'Write weekly log', estMinutes: 30, details: '<strong>Action:</strong> Write your Week 12 log documenting progress, challenges, and insights. Structure: (1) Overview: major topics covered, time spent, overall feeling about the week, (2) Achievements: what went well, breakthroughs, successful implementations, concepts that clicked, (3) Challenges: what was difficult, topics needing more work, technical issues encountered, time management struggles, (4) Key learnings: 3-5 major insights or "aha moments" from the week, (5) Looking ahead: goals for next week, specific concepts to reinforce, adjustments to study approach if needed. Be honest and reflective—this log is for you. <strong>Boundaries:</strong> Keep it concise but substantive—aim for 400-600 words. Focus on learning and growth, not just activities. Don\'t just list what you did—reflect on what you learned and how you\'re evolving as a learner. Be specific with examples rather than generic ("I struggled with cross-validation overfitting on the Wine dataset" not "some things were hard"). <strong>Deliverable:</strong> Weekly log document saved as <code>docs/logs/week12_log.md</code> with structured reflection on progress, challenges, learnings, and forward-looking goals. <strong>Verification:</strong> Log demonstrates genuine reflection and honest self-assessment. Contains specific examples and insights, not generic platitudes. Shows growth mindset—challenges viewed as learning opportunities. Includes actionable plans for improvement. Common pitfall: Writing perfunctory logs just to check a box—invest in reflection, it pays dividends in learning efficiency. Another pitfall: being too harsh or too easy on yourself—aim for honest, balanced assessment. Success check: Could you review this log in a month and remember the week clearly? Does it help you see your growth trajectory? Estimated time: 30 minutes for thoughtful reflection and writing. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2010/01/11/learn-faster-by-writing-twice/" target="_blank" rel="noopener">Learning Through Reflection</a>, <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Learning Principles</a>' }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 79,
          week: 12,
          title: 'PyTorch',
          priority: 'HIGH',
          tasks: [
            { label: 'PyTorch: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of PyTorch through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting PyTorch to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining PyTorch in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain PyTorch to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what PyTorch is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect PyTorch to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'PyTorch: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement PyTorch from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'PyTorch: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of PyTorch. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with PyTorch after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does PyTorch help?'
        },
        {
          globalDay: 80,
          week: 12,
          title: 'Autograd',
          priority: 'HIGH',
          tasks: [
            { label: 'Autograd: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Autograd through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Autograd to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Autograd in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Autograd to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Autograd is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Autograd to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Autograd: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Autograd from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Autograd: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Autograd. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Autograd after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Autograd help?'
        },
        {
          globalDay: 81,
          week: 12,
          title: 'MLP',
          priority: 'HIGH',
          tasks: [
            { label: 'MLP: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of MLP through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting MLP to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining MLP in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain MLP to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what MLP is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect MLP to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'MLP: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement MLP from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'MLP: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of MLP. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with MLP after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does MLP help?'
        },
        {
          globalDay: 82,
          week: 12,
          title: 'CNN',
          priority: 'HIGH',
          tasks: [
            { label: 'CNN: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of CNN through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting CNN to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining CNN in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain CNN to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what CNN is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect CNN to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'CNN: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement CNN from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'CNN: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of CNN. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with CNN after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does CNN help?'
        },
        {
          globalDay: 83,
          week: 12,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Training through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Training to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Training in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Training to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Training is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Training to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Training: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Training from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Training: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Training. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Training after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 84,
          week: 12,
          title: 'Optimization',
          priority: 'HIGH',
          tasks: [
            { label: 'Optimization: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Optimization through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Optimization to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Optimization in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Optimization to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Optimization is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Optimization to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Optimization: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Optimization from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Optimization: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Optimization. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Optimization after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Optimization help?'
        },
        {
          globalDay: 85,
          week: 12,
          title: 'Week 12 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75, details: '<strong>Action:</strong> Conduct comprehensive review of Week 13 Deep Learning material. Process: (1) Re-read all notes from this week, identifying key concepts and any remaining confusion, (2) Review all notebooks you created—can you follow the logic? Do results still make sense? (3) Revisit hardest concepts: re-watch relevant videos, re-read documentation, create additional examples if needed, (4) Test yourself: solve 2-3 problems from scratch without referring to previous solutions, (5) Create a one-page summary of the week\'s key insights and how they connect. <strong>Boundaries:</strong> Don\'t just skim—engage actively with the material. Focus on understanding, not memorization. If something is still unclear after review, mark it for deeper study or to ask mentors. Don\'t spend excessive time on material you\'ve mastered—focus on weak areas. Balance breadth (touching all topics) with depth (solidifying shaky concepts). <strong>Deliverable:</strong> Week 13 review document (<code>docs/notes/week13_review.md</code>) containing: (1) Summary of each day\'s key topics (2-3 sentences each), (2) Concepts that now make sense that didn\'t before, (3) Remaining questions or confusion (with plan to address), (4) 3-5 integration questions testing cross-topic understanding, (5) Self-assessment of mastery (1-5 scale) for each major topic with justification. <strong>Verification:</strong> Review document demonstrates genuine engagement and honest self-assessment. Can you explain this week\'s concepts to someone else? Can you solve problems combining multiple concepts from the week? Your self-assessment should identify both strengths and areas needing more work. Common pitfall: Passive re-reading without active problem-solving—you need to test your understanding. Another pitfall: overconfidence from recognizing concepts vs actually being able to use them. Success check: You should feel more confident about the week\'s material and have a clear plan for any remaining gaps. Estimated time: 75 minutes for thorough, active review. <strong>Resources:</strong> Your own notes and notebooks from the week, <a href="https://www.scotthyoung.com/blog/2020/05/04/study-better/" target="_blank" rel="noopener">Effective Study Techniques</a>, <a href="https://www.learningscientists.org/learning-scientists-podcast" target="_blank" rel="noopener">Learning Scientists Podcast</a>' },
            { label: 'Practice exercises', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Practice exercises. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Practice exercises after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' },
            { label: 'Write weekly log', estMinutes: 30, details: '<strong>Action:</strong> Write your Week 13 log documenting progress, challenges, and insights. Structure: (1) Overview: major topics covered, time spent, overall feeling about the week, (2) Achievements: what went well, breakthroughs, successful implementations, concepts that clicked, (3) Challenges: what was difficult, topics needing more work, technical issues encountered, time management struggles, (4) Key learnings: 3-5 major insights or "aha moments" from the week, (5) Looking ahead: goals for next week, specific concepts to reinforce, adjustments to study approach if needed. Be honest and reflective—this log is for you. <strong>Boundaries:</strong> Keep it concise but substantive—aim for 400-600 words. Focus on learning and growth, not just activities. Don\'t just list what you did—reflect on what you learned and how you\'re evolving as a learner. Be specific with examples rather than generic ("I struggled with cross-validation overfitting on the Wine dataset" not "some things were hard"). <strong>Deliverable:</strong> Weekly log document saved as <code>docs/logs/week13_log.md</code> with structured reflection on progress, challenges, learnings, and forward-looking goals. <strong>Verification:</strong> Log demonstrates genuine reflection and honest self-assessment. Contains specific examples and insights, not generic platitudes. Shows growth mindset—challenges viewed as learning opportunities. Includes actionable plans for improvement. Common pitfall: Writing perfunctory logs just to check a box—invest in reflection, it pays dividends in learning efficiency. Another pitfall: being too harsh or too easy on yourself—aim for honest, balanced assessment. Success check: Could you review this log in a month and remember the week clearly? Does it help you see your growth trajectory? Estimated time: 30 minutes for thoughtful reflection and writing. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2010/01/11/learn-faster-by-writing-twice/" target="_blank" rel="noopener">Learning Through Reflection</a>, <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Learning Principles</a>' }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 86,
          week: 13,
          title: 'PyTorch',
          priority: 'HIGH',
          tasks: [
            { label: 'PyTorch: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of PyTorch through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting PyTorch to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining PyTorch in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain PyTorch to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what PyTorch is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect PyTorch to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'PyTorch: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement PyTorch from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'PyTorch: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of PyTorch. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with PyTorch after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does PyTorch help?'
        },
        {
          globalDay: 87,
          week: 13,
          title: 'Autograd',
          priority: 'HIGH',
          tasks: [
            { label: 'Autograd: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Autograd through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Autograd to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Autograd in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Autograd to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Autograd is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Autograd to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Autograd: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Autograd from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Autograd: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Autograd. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Autograd after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Autograd help?'
        },
        {
          globalDay: 88,
          week: 13,
          title: 'MLP',
          priority: 'HIGH',
          tasks: [
            { label: 'MLP: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of MLP through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting MLP to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining MLP in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain MLP to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what MLP is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect MLP to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'MLP: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement MLP from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'MLP: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of MLP. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with MLP after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does MLP help?'
        },
        {
          globalDay: 89,
          week: 13,
          title: 'CNN',
          priority: 'HIGH',
          tasks: [
            { label: 'CNN: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of CNN through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting CNN to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining CNN in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain CNN to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what CNN is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect CNN to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'CNN: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement CNN from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'CNN: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of CNN. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with CNN after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does CNN help?'
        },
        {
          globalDay: 90,
          week: 13,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Training through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Training to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Training in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Training to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Training is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Training to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Training: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Training from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Training: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Training. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Training after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 91,
          week: 13,
          title: 'Optimization',
          priority: 'HIGH',
          tasks: [
            { label: 'Optimization: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Optimization through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Optimization to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Optimization in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Optimization to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Optimization is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Optimization to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Optimization: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Optimization from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Optimization: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Optimization. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Optimization after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Optimization help?'
        },
        {
          globalDay: 92,
          week: 13,
          title: 'Week 13 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75, details: '<strong>Action:</strong> Conduct comprehensive review of Week 14 Deep Learning material. Process: (1) Re-read all notes from this week, identifying key concepts and any remaining confusion, (2) Review all notebooks you created—can you follow the logic? Do results still make sense? (3) Revisit hardest concepts: re-watch relevant videos, re-read documentation, create additional examples if needed, (4) Test yourself: solve 2-3 problems from scratch without referring to previous solutions, (5) Create a one-page summary of the week\'s key insights and how they connect. <strong>Boundaries:</strong> Don\'t just skim—engage actively with the material. Focus on understanding, not memorization. If something is still unclear after review, mark it for deeper study or to ask mentors. Don\'t spend excessive time on material you\'ve mastered—focus on weak areas. Balance breadth (touching all topics) with depth (solidifying shaky concepts). <strong>Deliverable:</strong> Week 14 review document (<code>docs/notes/week14_review.md</code>) containing: (1) Summary of each day\'s key topics (2-3 sentences each), (2) Concepts that now make sense that didn\'t before, (3) Remaining questions or confusion (with plan to address), (4) 3-5 integration questions testing cross-topic understanding, (5) Self-assessment of mastery (1-5 scale) for each major topic with justification. <strong>Verification:</strong> Review document demonstrates genuine engagement and honest self-assessment. Can you explain this week\'s concepts to someone else? Can you solve problems combining multiple concepts from the week? Your self-assessment should identify both strengths and areas needing more work. Common pitfall: Passive re-reading without active problem-solving—you need to test your understanding. Another pitfall: overconfidence from recognizing concepts vs actually being able to use them. Success check: You should feel more confident about the week\'s material and have a clear plan for any remaining gaps. Estimated time: 75 minutes for thorough, active review. <strong>Resources:</strong> Your own notes and notebooks from the week, <a href="https://www.scotthyoung.com/blog/2020/05/04/study-better/" target="_blank" rel="noopener">Effective Study Techniques</a>, <a href="https://www.learningscientists.org/learning-scientists-podcast" target="_blank" rel="noopener">Learning Scientists Podcast</a>' },
            { label: 'Practice exercises', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Practice exercises. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Practice exercises after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' },
            { label: 'Write weekly log', estMinutes: 30, details: '<strong>Action:</strong> Write your Week 14 log documenting progress, challenges, and insights. Structure: (1) Overview: major topics covered, time spent, overall feeling about the week, (2) Achievements: what went well, breakthroughs, successful implementations, concepts that clicked, (3) Challenges: what was difficult, topics needing more work, technical issues encountered, time management struggles, (4) Key learnings: 3-5 major insights or "aha moments" from the week, (5) Looking ahead: goals for next week, specific concepts to reinforce, adjustments to study approach if needed. Be honest and reflective—this log is for you. <strong>Boundaries:</strong> Keep it concise but substantive—aim for 400-600 words. Focus on learning and growth, not just activities. Don\'t just list what you did—reflect on what you learned and how you\'re evolving as a learner. Be specific with examples rather than generic ("I struggled with cross-validation overfitting on the Wine dataset" not "some things were hard"). <strong>Deliverable:</strong> Weekly log document saved as <code>docs/logs/week14_log.md</code> with structured reflection on progress, challenges, learnings, and forward-looking goals. <strong>Verification:</strong> Log demonstrates genuine reflection and honest self-assessment. Contains specific examples and insights, not generic platitudes. Shows growth mindset—challenges viewed as learning opportunities. Includes actionable plans for improvement. Common pitfall: Writing perfunctory logs just to check a box—invest in reflection, it pays dividends in learning efficiency. Another pitfall: being too harsh or too easy on yourself—aim for honest, balanced assessment. Success check: Could you review this log in a month and remember the week clearly? Does it help you see your growth trajectory? Estimated time: 30 minutes for thoughtful reflection and writing. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2010/01/11/learn-faster-by-writing-twice/" target="_blank" rel="noopener">Learning Through Reflection</a>, <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Learning Principles</a>' }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 93,
          week: 14,
          title: 'PyTorch',
          priority: 'HIGH',
          tasks: [
            { label: 'PyTorch: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of PyTorch through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting PyTorch to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining PyTorch in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain PyTorch to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what PyTorch is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect PyTorch to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'PyTorch: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement PyTorch from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'PyTorch: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of PyTorch. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with PyTorch after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does PyTorch help?'
        },
        {
          globalDay: 94,
          week: 14,
          title: 'Autograd',
          priority: 'HIGH',
          tasks: [
            { label: 'Autograd: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Autograd through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Autograd to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Autograd in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Autograd to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Autograd is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Autograd to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Autograd: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Autograd from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Autograd: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Autograd. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Autograd after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Autograd help?'
        },
        {
          globalDay: 95,
          week: 14,
          title: 'MLP',
          priority: 'HIGH',
          tasks: [
            { label: 'MLP: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of MLP through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting MLP to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining MLP in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain MLP to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what MLP is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect MLP to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'MLP: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement MLP from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'MLP: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of MLP. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with MLP after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does MLP help?'
        },
        {
          globalDay: 96,
          week: 14,
          title: 'CNN',
          priority: 'HIGH',
          tasks: [
            { label: 'CNN: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of CNN through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting CNN to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining CNN in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain CNN to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what CNN is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect CNN to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'CNN: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement CNN from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'CNN: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of CNN. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with CNN after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does CNN help?'
        },
        {
          globalDay: 97,
          week: 14,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Training through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Training to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Training in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Training to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Training is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Training to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Training: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Training from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Training: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Training. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Training after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 98,
          week: 14,
          title: 'Optimization',
          priority: 'HIGH',
          tasks: [
            { label: 'Optimization: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Optimization through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Optimization to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Optimization in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Optimization to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Optimization is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Optimization to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Optimization: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Optimization from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Optimization: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Optimization. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Optimization after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Optimization help?'
        },
        {
          globalDay: 99,
          week: 14,
          title: 'Week 14 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75, details: '<strong>Action:</strong> Conduct comprehensive review of Week 15 Deep Learning material. Process: (1) Re-read all notes from this week, identifying key concepts and any remaining confusion, (2) Review all notebooks you created—can you follow the logic? Do results still make sense? (3) Revisit hardest concepts: re-watch relevant videos, re-read documentation, create additional examples if needed, (4) Test yourself: solve 2-3 problems from scratch without referring to previous solutions, (5) Create a one-page summary of the week\'s key insights and how they connect. <strong>Boundaries:</strong> Don\'t just skim—engage actively with the material. Focus on understanding, not memorization. If something is still unclear after review, mark it for deeper study or to ask mentors. Don\'t spend excessive time on material you\'ve mastered—focus on weak areas. Balance breadth (touching all topics) with depth (solidifying shaky concepts). <strong>Deliverable:</strong> Week 15 review document (<code>docs/notes/week15_review.md</code>) containing: (1) Summary of each day\'s key topics (2-3 sentences each), (2) Concepts that now make sense that didn\'t before, (3) Remaining questions or confusion (with plan to address), (4) 3-5 integration questions testing cross-topic understanding, (5) Self-assessment of mastery (1-5 scale) for each major topic with justification. <strong>Verification:</strong> Review document demonstrates genuine engagement and honest self-assessment. Can you explain this week\'s concepts to someone else? Can you solve problems combining multiple concepts from the week? Your self-assessment should identify both strengths and areas needing more work. Common pitfall: Passive re-reading without active problem-solving—you need to test your understanding. Another pitfall: overconfidence from recognizing concepts vs actually being able to use them. Success check: You should feel more confident about the week\'s material and have a clear plan for any remaining gaps. Estimated time: 75 minutes for thorough, active review. <strong>Resources:</strong> Your own notes and notebooks from the week, <a href="https://www.scotthyoung.com/blog/2020/05/04/study-better/" target="_blank" rel="noopener">Effective Study Techniques</a>, <a href="https://www.learningscientists.org/learning-scientists-podcast" target="_blank" rel="noopener">Learning Scientists Podcast</a>' },
            { label: 'Practice exercises', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Practice exercises. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Practice exercises after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' },
            { label: 'Write weekly log', estMinutes: 30, details: '<strong>Action:</strong> Write your Week 15 log documenting progress, challenges, and insights. Structure: (1) Overview: major topics covered, time spent, overall feeling about the week, (2) Achievements: what went well, breakthroughs, successful implementations, concepts that clicked, (3) Challenges: what was difficult, topics needing more work, technical issues encountered, time management struggles, (4) Key learnings: 3-5 major insights or "aha moments" from the week, (5) Looking ahead: goals for next week, specific concepts to reinforce, adjustments to study approach if needed. Be honest and reflective—this log is for you. <strong>Boundaries:</strong> Keep it concise but substantive—aim for 400-600 words. Focus on learning and growth, not just activities. Don\'t just list what you did—reflect on what you learned and how you\'re evolving as a learner. Be specific with examples rather than generic ("I struggled with cross-validation overfitting on the Wine dataset" not "some things were hard"). <strong>Deliverable:</strong> Weekly log document saved as <code>docs/logs/week15_log.md</code> with structured reflection on progress, challenges, learnings, and forward-looking goals. <strong>Verification:</strong> Log demonstrates genuine reflection and honest self-assessment. Contains specific examples and insights, not generic platitudes. Shows growth mindset—challenges viewed as learning opportunities. Includes actionable plans for improvement. Common pitfall: Writing perfunctory logs just to check a box—invest in reflection, it pays dividends in learning efficiency. Another pitfall: being too harsh or too easy on yourself—aim for honest, balanced assessment. Success check: Could you review this log in a month and remember the week clearly? Does it help you see your growth trajectory? Estimated time: 30 minutes for thoughtful reflection and writing. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2010/01/11/learn-faster-by-writing-twice/" target="_blank" rel="noopener">Learning Through Reflection</a>, <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Learning Principles</a>' }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 100,
          week: 15,
          title: 'PyTorch',
          priority: 'HIGH',
          tasks: [
            { label: 'PyTorch: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of PyTorch through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting PyTorch to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining PyTorch in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain PyTorch to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what PyTorch is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect PyTorch to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'PyTorch: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement PyTorch from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'PyTorch: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of PyTorch. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with PyTorch after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does PyTorch help?'
        },
        {
          globalDay: 101,
          week: 15,
          title: 'Autograd',
          priority: 'HIGH',
          tasks: [
            { label: 'Autograd: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Autograd through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Autograd to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Autograd in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Autograd to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Autograd is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Autograd to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Autograd: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Autograd from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Autograd: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Autograd. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Autograd after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Autograd help?'
        },
        {
          globalDay: 102,
          week: 15,
          title: 'MLP',
          priority: 'HIGH',
          tasks: [
            { label: 'MLP: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of MLP through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting MLP to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining MLP in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain MLP to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what MLP is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect MLP to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'MLP: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement MLP from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'MLP: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of MLP. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with MLP after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does MLP help?'
        },
        {
          globalDay: 103,
          week: 15,
          title: 'CNN',
          priority: 'HIGH',
          tasks: [
            { label: 'CNN: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of CNN through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting CNN to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining CNN in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain CNN to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what CNN is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect CNN to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'CNN: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement CNN from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'CNN: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of CNN. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with CNN after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does CNN help?'
        },
        {
          globalDay: 104,
          week: 15,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Training through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Training to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Training in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Training to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Training is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Training to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Training: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Training from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Training: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Training. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Training after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 105,
          week: 15,
          title: 'Optimization',
          priority: 'HIGH',
          tasks: [
            { label: 'Optimization: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Optimization through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Optimization to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Optimization in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Optimization to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Optimization is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Optimization to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Optimization: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Optimization from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Optimization: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Optimization. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Optimization after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Optimization help?'
        },
        {
          globalDay: 106,
          week: 15,
          title: 'Week 15 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75, details: '<strong>Action:</strong> Conduct comprehensive review of Week 16 Deep Learning material. Process: (1) Re-read all notes from this week, identifying key concepts and any remaining confusion, (2) Review all notebooks you created—can you follow the logic? Do results still make sense? (3) Revisit hardest concepts: re-watch relevant videos, re-read documentation, create additional examples if needed, (4) Test yourself: solve 2-3 problems from scratch without referring to previous solutions, (5) Create a one-page summary of the week\'s key insights and how they connect. <strong>Boundaries:</strong> Don\'t just skim—engage actively with the material. Focus on understanding, not memorization. If something is still unclear after review, mark it for deeper study or to ask mentors. Don\'t spend excessive time on material you\'ve mastered—focus on weak areas. Balance breadth (touching all topics) with depth (solidifying shaky concepts). <strong>Deliverable:</strong> Week 16 review document (<code>docs/notes/week16_review.md</code>) containing: (1) Summary of each day\'s key topics (2-3 sentences each), (2) Concepts that now make sense that didn\'t before, (3) Remaining questions or confusion (with plan to address), (4) 3-5 integration questions testing cross-topic understanding, (5) Self-assessment of mastery (1-5 scale) for each major topic with justification. <strong>Verification:</strong> Review document demonstrates genuine engagement and honest self-assessment. Can you explain this week\'s concepts to someone else? Can you solve problems combining multiple concepts from the week? Your self-assessment should identify both strengths and areas needing more work. Common pitfall: Passive re-reading without active problem-solving—you need to test your understanding. Another pitfall: overconfidence from recognizing concepts vs actually being able to use them. Success check: You should feel more confident about the week\'s material and have a clear plan for any remaining gaps. Estimated time: 75 minutes for thorough, active review. <strong>Resources:</strong> Your own notes and notebooks from the week, <a href="https://www.scotthyoung.com/blog/2020/05/04/study-better/" target="_blank" rel="noopener">Effective Study Techniques</a>, <a href="https://www.learningscientists.org/learning-scientists-podcast" target="_blank" rel="noopener">Learning Scientists Podcast</a>' },
            { label: 'Practice exercises', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Practice exercises. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Practice exercises after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' },
            { label: 'Write weekly log', estMinutes: 30, details: '<strong>Action:</strong> Write your Week 16 log documenting progress, challenges, and insights. Structure: (1) Overview: major topics covered, time spent, overall feeling about the week, (2) Achievements: what went well, breakthroughs, successful implementations, concepts that clicked, (3) Challenges: what was difficult, topics needing more work, technical issues encountered, time management struggles, (4) Key learnings: 3-5 major insights or "aha moments" from the week, (5) Looking ahead: goals for next week, specific concepts to reinforce, adjustments to study approach if needed. Be honest and reflective—this log is for you. <strong>Boundaries:</strong> Keep it concise but substantive—aim for 400-600 words. Focus on learning and growth, not just activities. Don\'t just list what you did—reflect on what you learned and how you\'re evolving as a learner. Be specific with examples rather than generic ("I struggled with cross-validation overfitting on the Wine dataset" not "some things were hard"). <strong>Deliverable:</strong> Weekly log document saved as <code>docs/logs/week16_log.md</code> with structured reflection on progress, challenges, learnings, and forward-looking goals. <strong>Verification:</strong> Log demonstrates genuine reflection and honest self-assessment. Contains specific examples and insights, not generic platitudes. Shows growth mindset—challenges viewed as learning opportunities. Includes actionable plans for improvement. Common pitfall: Writing perfunctory logs just to check a box—invest in reflection, it pays dividends in learning efficiency. Another pitfall: being too harsh or too easy on yourself—aim for honest, balanced assessment. Success check: Could you review this log in a month and remember the week clearly? Does it help you see your growth trajectory? Estimated time: 30 minutes for thoughtful reflection and writing. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2010/01/11/learn-faster-by-writing-twice/" target="_blank" rel="noopener">Learning Through Reflection</a>, <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Learning Principles</a>' }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 107,
          week: 16,
          title: 'PyTorch',
          priority: 'HIGH',
          tasks: [
            { label: 'PyTorch: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of PyTorch through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting PyTorch to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining PyTorch in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain PyTorch to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what PyTorch is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect PyTorch to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'PyTorch: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement PyTorch from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'PyTorch: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of PyTorch. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with PyTorch after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does PyTorch help?'
        },
        {
          globalDay: 108,
          week: 16,
          title: 'Autograd',
          priority: 'HIGH',
          tasks: [
            { label: 'Autograd: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Autograd through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Autograd to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Autograd in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Autograd to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Autograd is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Autograd to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Autograd: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Autograd from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Autograd: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Autograd. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Autograd after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Autograd help?'
        },
        {
          globalDay: 109,
          week: 16,
          title: 'MLP',
          priority: 'HIGH',
          tasks: [
            { label: 'MLP: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of MLP through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting MLP to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining MLP in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain MLP to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what MLP is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect MLP to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'MLP: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement MLP from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'MLP: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of MLP. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with MLP after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does MLP help?'
        },
        {
          globalDay: 110,
          week: 16,
          title: 'CNN',
          priority: 'HIGH',
          tasks: [
            { label: 'CNN: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of CNN through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting CNN to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining CNN in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain CNN to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what CNN is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect CNN to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'CNN: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement CNN from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'CNN: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of CNN. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with CNN after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does CNN help?'
        },
        {
          globalDay: 111,
          week: 16,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Training through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Training to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Training in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Training to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Training is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Training to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Training: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Training from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Training: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Training. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Training after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 112,
          week: 16,
          title: 'Optimization',
          priority: 'HIGH',
          tasks: [
            { label: 'Optimization: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Optimization through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Optimization to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Optimization in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Optimization to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Optimization is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Optimization to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Optimization: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Optimization from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Optimization: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Optimization. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Optimization after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Optimization help?'
        },
        {
          globalDay: 113,
          week: 16,
          title: 'Week 16 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75, details: '<strong>Action:</strong> Conduct comprehensive review of Week 17 Deep Learning material. Process: (1) Re-read all notes from this week, identifying key concepts and any remaining confusion, (2) Review all notebooks you created—can you follow the logic? Do results still make sense? (3) Revisit hardest concepts: re-watch relevant videos, re-read documentation, create additional examples if needed, (4) Test yourself: solve 2-3 problems from scratch without referring to previous solutions, (5) Create a one-page summary of the week\'s key insights and how they connect. <strong>Boundaries:</strong> Don\'t just skim—engage actively with the material. Focus on understanding, not memorization. If something is still unclear after review, mark it for deeper study or to ask mentors. Don\'t spend excessive time on material you\'ve mastered—focus on weak areas. Balance breadth (touching all topics) with depth (solidifying shaky concepts). <strong>Deliverable:</strong> Week 17 review document (<code>docs/notes/week17_review.md</code>) containing: (1) Summary of each day\'s key topics (2-3 sentences each), (2) Concepts that now make sense that didn\'t before, (3) Remaining questions or confusion (with plan to address), (4) 3-5 integration questions testing cross-topic understanding, (5) Self-assessment of mastery (1-5 scale) for each major topic with justification. <strong>Verification:</strong> Review document demonstrates genuine engagement and honest self-assessment. Can you explain this week\'s concepts to someone else? Can you solve problems combining multiple concepts from the week? Your self-assessment should identify both strengths and areas needing more work. Common pitfall: Passive re-reading without active problem-solving—you need to test your understanding. Another pitfall: overconfidence from recognizing concepts vs actually being able to use them. Success check: You should feel more confident about the week\'s material and have a clear plan for any remaining gaps. Estimated time: 75 minutes for thorough, active review. <strong>Resources:</strong> Your own notes and notebooks from the week, <a href="https://www.scotthyoung.com/blog/2020/05/04/study-better/" target="_blank" rel="noopener">Effective Study Techniques</a>, <a href="https://www.learningscientists.org/learning-scientists-podcast" target="_blank" rel="noopener">Learning Scientists Podcast</a>' },
            { label: 'Practice exercises', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Practice exercises. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Practice exercises after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' },
            { label: 'Write weekly log', estMinutes: 30, details: '<strong>Action:</strong> Write your Week 17 log documenting progress, challenges, and insights. Structure: (1) Overview: major topics covered, time spent, overall feeling about the week, (2) Achievements: what went well, breakthroughs, successful implementations, concepts that clicked, (3) Challenges: what was difficult, topics needing more work, technical issues encountered, time management struggles, (4) Key learnings: 3-5 major insights or "aha moments" from the week, (5) Looking ahead: goals for next week, specific concepts to reinforce, adjustments to study approach if needed. Be honest and reflective—this log is for you. <strong>Boundaries:</strong> Keep it concise but substantive—aim for 400-600 words. Focus on learning and growth, not just activities. Don\'t just list what you did—reflect on what you learned and how you\'re evolving as a learner. Be specific with examples rather than generic ("I struggled with cross-validation overfitting on the Wine dataset" not "some things were hard"). <strong>Deliverable:</strong> Weekly log document saved as <code>docs/logs/week17_log.md</code> with structured reflection on progress, challenges, learnings, and forward-looking goals. <strong>Verification:</strong> Log demonstrates genuine reflection and honest self-assessment. Contains specific examples and insights, not generic platitudes. Shows growth mindset—challenges viewed as learning opportunities. Includes actionable plans for improvement. Common pitfall: Writing perfunctory logs just to check a box—invest in reflection, it pays dividends in learning efficiency. Another pitfall: being too harsh or too easy on yourself—aim for honest, balanced assessment. Success check: Could you review this log in a month and remember the week clearly? Does it help you see your growth trajectory? Estimated time: 30 minutes for thoughtful reflection and writing. <strong>Resources:</strong> <a href="https://www.scotthyoung.com/blog/2010/01/11/learn-faster-by-writing-twice/" target="_blank" rel="noopener">Learning Through Reflection</a>, <a href="https://fs.blog/learning/" target="_blank" rel="noopener">Learning Principles</a>' }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 114,
          week: 17,
          title: 'PyTorch',
          priority: 'HIGH',
          tasks: [
            { label: 'PyTorch: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of PyTorch through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting PyTorch to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining PyTorch in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain PyTorch to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what PyTorch is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect PyTorch to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'PyTorch: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement PyTorch from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'PyTorch: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of PyTorch. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with PyTorch after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does PyTorch help?'
        },
        {
          globalDay: 115,
          week: 17,
          title: 'Autograd',
          priority: 'HIGH',
          tasks: [
            { label: 'Autograd: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Autograd through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Autograd to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Autograd in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Autograd to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Autograd is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Autograd to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Autograd: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Autograd from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Autograd: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Autograd. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Autograd after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Autograd help?'
        },
        {
          globalDay: 116,
          week: 17,
          title: 'MLP',
          priority: 'HIGH',
          tasks: [
            { label: 'MLP: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of MLP through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting MLP to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining MLP in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain MLP to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what MLP is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect MLP to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'MLP: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement MLP from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'MLP: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of MLP. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with MLP after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does MLP help?'
        },
        {
          globalDay: 117,
          week: 17,
          title: 'CNN',
          priority: 'HIGH',
          tasks: [
            { label: 'CNN: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of CNN through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting CNN to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining CNN in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain CNN to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what CNN is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect CNN to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'CNN: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement CNN from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'CNN: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of CNN. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with CNN after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does CNN help?'
        },
        {
          globalDay: 118,
          week: 17,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Training through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Training to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Training in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Training to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Training is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Training to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Training: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Training from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Training: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Training. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Training after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 119,
          week: 17,
          title: 'Optimization',
          priority: 'HIGH',
          tasks: [
            { label: 'Optimization: Core concepts', estMinutes: 90, details: '<strong>Action:</strong> Study the core concepts of Optimization through official documentation, tutorial videos, and worked examples. Focus on understanding architecture design, forward/backward pass mechanics, gradient flow, and training stability considerations. Take detailed notes on key definitions, formulas, architectural choices, and intuitions. Create concept maps connecting Optimization to previously learned material. Use active learning—don\'t just read passively; implement small examples as you learn, experiment with parameters, and test your understanding. <strong>Boundaries:</strong> Focus on conceptual understanding before full implementation. Don\'t skip foundational concepts to rush ahead. Spend time visualizing and drawing diagrams—mental models are crucial. Stop when you have clear intuition—detailed implementation comes in the next task. Don\'t get distracted by advanced variants or optimizations yet. <strong>Deliverable:</strong> Create comprehensive notes document or concept map explaining Optimization in your own words. Include: (1) Core definitions and terminology with examples, (2) Visual diagrams illustrating key concepts and data flow, (3) Connections to prior topics and prerequisite knowledge, (4) 3-5 self-test questions with detailed answers, (5) Summary of when to use this approach vs alternatives. <strong>Verification:</strong> Can you explain Optimization to someone else without looking at your notes? Can you identify potential pitfalls or common misconceptions? Test yourself by covering your notes and explaining the concept aloud or writing a summary. Your explanation should include both what Optimization is and why it matters for ML systems. Common pitfall: Passive reading without active engagement leads to shallow understanding. Success check: You should be able to connect Optimization to broader learning objectives and explain why it matters. Can you predict what problems might arise in practice? Estimated time: 90 minutes including active note-taking, diagramming, and self-testing. <strong>Resources:</strong> <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch Documentation</a>, <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a>' },
            { label: 'Optimization: Implementation', estMinutes: 75, details: '<strong>Action:</strong> Implement Optimization from scratch following best practices and the PyTorch API patterns. Start with a minimal working version, then iteratively add features and improvements. Write clean, well-documented code with type hints where appropriate and comprehensive docstrings. Include inline comments explaining non-obvious logic, especially for mathematical operations or architectural choices. Structure your notebook clearly: imports, data loading, model definition, training, evaluation, visualization. <strong>Boundaries:</strong> Focus on correctness before optimization. Write tests or verification code as you go to verify each component works as expected. Don\'t copy-paste large code blocks—type everything yourself to build muscle memory and deep understanding. Keep functions small and focused on single responsibilities. Use meaningful variable names (not x, y, z for everything). <strong>Deliverable:</strong> Create a well-structured implementation in notebooks/deep_learning/day{day}_{topic_slug}.ipynb with: (1) Clear function/class definitions following PyTorch conventions, (2) Comprehensive docstrings with parameter descriptions and examples, (3) Verification code checking outputs at each stage, (4) A demonstration section showing the implementation in action on appropriate dataset, (5) Visualization of results (plots, metrics, confusion matrices as applicable). <strong>Verification:</strong> Run training loop successfully with loss decreasing, check gradients aren\'t NaN/exploding, verify model outputs have correct shapes and reasonable values. Use torch.no_grad() for validation, ensure reproducibility with seeds. Common pitfall: Writing code without testing incrementally, leading to hard-to-debug errors. Another pitfall: poor variable names making code unreadable. Success check: Your implementation should produce expected outputs for at least 5 diverse test cases. Code should be clean enough that you\'d be proud to show it in a code review or add to your portfolio. Estimated time: 75 minutes including implementation, testing, documentation, and verification. <strong>Resources:</strong> <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch Tutorials</a>, <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">torch.nn Documentation</a>' },
            { label: 'Optimization: Practice', estMinutes: 60, details: '<strong>Action:</strong> Complete practice exercises to solidify your understanding of Optimization. Work through at least 5-8 problems or experiments of varying difficulty covering different architectures, hyperparameter variations (learning rate, batch size, hidden dimensions), training techniques, debugging common issues (vanishing/exploding gradients, overfitting). Start with simpler problems to build confidence, then tackle more challenging ones. Time yourself on some problems to build fluency. Document your process, results, and insights for each experiment. <strong>Boundaries:</strong> Attempt each problem independently before checking solutions or references. If stuck for more than 15-20 minutes on implementation details, review the relevant concept from previous tasks then retry with fresh perspective. Don\'t just verify your answer is correct—make sure you understand WHY it\'s correct, what assumptions you made, and what the exercise was designed to teach you. <strong>Deliverable:</strong> A collection of solved problems or experiments with your working shown in notebook format. For each problem: (1) Write out your approach and hypotheses, (2) Show implementation with step-by-step work, (3) Visualize results appropriately (plots, tables, metrics), (4) Verify outcomes and analyze any surprises, (5) Note what concept it tested, (6) Identify any patterns, tricks, or insights you learned. <strong>Verification:</strong> Aim for training loss convergence, validation accuracy improvement, stable gradients, reasonable training time on practice problems. If results are poor, debug systematically: check data preprocessing, verify model architecture, examine loss curves, inspect predictions. For any mistakes, understand exactly where your reasoning or implementation went wrong. Can you solve similar problems quickly and confidently now? Common pitfall: Looking at solutions too quickly instead of struggling productively—the struggle builds deeper understanding. Another pitfall: not varying parameters enough to see their effects. Success check: You should feel significantly more confident with Optimization after practice. You should be able to approach new problems of similar difficulty without help, and debug issues efficiently. Estimated time: 60 minutes for thorough practice with multiple examples. <strong>Resources:</strong> <a href="https://scikit-learn.org/stable/auto_examples/index.html" target="_blank" rel="noopener">sklearn Examples</a>, <a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener">PyTorch Basics</a>, <a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers with Code</a>' }
          ],
          reflectionPrompt: 'How does Optimization help?'
        }
      ]
    },
    {
      id: 'nlp-warmup',
      title: 'Phase 5: Buffer & NLP Warmup',
      description: 'Light week with character-level dataset exploration and optional RNN.',
      duration: '7 days (Week 18)',
      weeks: [18],
      days: [
        {
          globalDay: 120,
          week: 18,
          title: 'Character-Level Dataset Prep',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Download tiny text corpus (Shakespeare or similar)', estMinutes: 15 },
            { label: 'Create notebooks/nlp/day120_char_data.ipynb', estMinutes: 90 },
            { label: 'Build character tokenizer and vocab', estMinutes: 60 },
            { label: 'Create train/val splits', estMinutes: 30 },
            { label: 'Write docs/notes/day120_char_tokenization.md', estMinutes: 30 }
          ],
          reflectionPrompt: 'How does character-level differ from word-level tokenization?'
        },
                {
          globalDay: 121,
          week: 18,
          title: 'Char data',
          priority: 'HIGH',
          tasks: [
            { label: 'Char data: Core concepts', estMinutes: 90 },
            { label: 'Char data: Implementation', estMinutes: 75 },
            { label: 'Char data: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Char data help?'
        },
        {
          globalDay: 122,
          week: 18,
          title: 'RNN',
          priority: 'HIGH',
          tasks: [
            { label: 'RNN: Core concepts', estMinutes: 90 },
            { label: 'RNN: Implementation', estMinutes: 75 },
            { label: 'RNN: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does RNN help?'
        },
        {
          globalDay: 123,
          week: 18,
          title: 'Sequences',
          priority: 'HIGH',
          tasks: [
            { label: 'Sequences: Core concepts', estMinutes: 90 },
            { label: 'Sequences: Implementation', estMinutes: 75 },
            { label: 'Sequences: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Sequences help?'
        },
        {
          globalDay: 124,
          week: 18,
          title: 'Text gen',
          priority: 'HIGH',
          tasks: [
            { label: 'Text gen: Core concepts', estMinutes: 90 },
            { label: 'Text gen: Implementation', estMinutes: 75 },
            { label: 'Text gen: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Text gen help?'
        },
        {
          globalDay: 125,
          week: 18,
          title: 'Eval',
          priority: 'HIGH',
          tasks: [
            { label: 'Eval: Core concepts', estMinutes: 90 },
            { label: 'Eval: Implementation', estMinutes: 75 },
            { label: 'Eval: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Eval help?'
        },
        {
          globalDay: 126,
          week: 18,
          title: 'Buffer',
          priority: 'HIGH',
          tasks: [
            { label: 'Buffer: Core concepts', estMinutes: 90 },
            { label: 'Buffer: Implementation', estMinutes: 75 },
            { label: 'Buffer: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Buffer help?'
        }
      ]
    },
    {
      id: 'transformers',
      title: 'Phase 6: Transformer Fundamentals',
      description: 'Attention mechanism, Multi-Head Attention, positional encoding, and Pre-LN blocks.',
      duration: '28 days (Weeks 19-22)',
      weeks: [19, 20, 21, 22],
      days: [
        {
          globalDay: 127,
          week: 19,
          title: 'Attention Mechanism Introduction',
          priority: 'HIGH',
          tasks: [
            { label: 'Read "Attention Is All You Need" paper - pass 1 (skim)', estMinutes: 60 },
            { label: 'Watch Yannic Kilcher explanation video', estMinutes: 45, resourceLinks: ['https://www.youtube.com/watch?v=iDulhoQ2pro'] },
            { label: 'Create notebooks/transformers/day127_attention_intro.ipynb', estMinutes: 120 },
            { label: 'Implement scaled dot-product attention from scratch', estMinutes: 90 },
            { label: 'Write docs/notes/day127_attention.md', estMinutes: 30 }
          ],
          reflectionPrompt: 'Why is scaling by sqrt(d_k) necessary?'
        },
                {
          globalDay: 128,
          week: 19,
          title: 'Attention',
          priority: 'HIGH',
          tasks: [
            { label: 'Attention: Core concepts', estMinutes: 90 },
            { label: 'Attention: Implementation', estMinutes: 75 },
            { label: 'Attention: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Attention help?'
        },
        {
          globalDay: 129,
          week: 19,
          title: 'MHA',
          priority: 'HIGH',
          tasks: [
            { label: 'MHA: Core concepts', estMinutes: 90 },
            { label: 'MHA: Implementation', estMinutes: 75 },
            { label: 'MHA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does MHA help?'
        },
        {
          globalDay: 130,
          week: 19,
          title: 'Positional encoding',
          priority: 'HIGH',
          tasks: [
            { label: 'Positional encoding: Core concepts', estMinutes: 90 },
            { label: 'Positional encoding: Implementation', estMinutes: 75 },
            { label: 'Positional encoding: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Positional encoding help?'
        },
        {
          globalDay: 131,
          week: 19,
          title: 'LayerNorm',
          priority: 'HIGH',
          tasks: [
            { label: 'LayerNorm: Core concepts', estMinutes: 90 },
            { label: 'LayerNorm: Implementation', estMinutes: 75 },
            { label: 'LayerNorm: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does LayerNorm help?'
        },
        {
          globalDay: 132,
          week: 19,
          title: 'Transformer blocks',
          priority: 'HIGH',
          tasks: [
            { label: 'Transformer blocks: Core concepts', estMinutes: 90 },
            { label: 'Transformer blocks: Implementation', estMinutes: 75 },
            { label: 'Transformer blocks: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Transformer blocks help?'
        },
        {
          globalDay: 133,
          week: 19,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        },
        {
          globalDay: 134,
          week: 19,
          title: 'Week 19 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 135,
          week: 20,
          title: 'Attention',
          priority: 'HIGH',
          tasks: [
            { label: 'Attention: Core concepts', estMinutes: 90 },
            { label: 'Attention: Implementation', estMinutes: 75 },
            { label: 'Attention: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Attention help?'
        },
        {
          globalDay: 136,
          week: 20,
          title: 'MHA',
          priority: 'HIGH',
          tasks: [
            { label: 'MHA: Core concepts', estMinutes: 90 },
            { label: 'MHA: Implementation', estMinutes: 75 },
            { label: 'MHA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does MHA help?'
        },
        {
          globalDay: 137,
          week: 20,
          title: 'Positional encoding',
          priority: 'HIGH',
          tasks: [
            { label: 'Positional encoding: Core concepts', estMinutes: 90 },
            { label: 'Positional encoding: Implementation', estMinutes: 75 },
            { label: 'Positional encoding: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Positional encoding help?'
        },
        {
          globalDay: 138,
          week: 20,
          title: 'LayerNorm',
          priority: 'HIGH',
          tasks: [
            { label: 'LayerNorm: Core concepts', estMinutes: 90 },
            { label: 'LayerNorm: Implementation', estMinutes: 75 },
            { label: 'LayerNorm: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does LayerNorm help?'
        },
        {
          globalDay: 139,
          week: 20,
          title: 'Transformer blocks',
          priority: 'HIGH',
          tasks: [
            { label: 'Transformer blocks: Core concepts', estMinutes: 90 },
            { label: 'Transformer blocks: Implementation', estMinutes: 75 },
            { label: 'Transformer blocks: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Transformer blocks help?'
        },
        {
          globalDay: 140,
          week: 20,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        },
        {
          globalDay: 141,
          week: 20,
          title: 'Week 20 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 142,
          week: 21,
          title: 'Attention',
          priority: 'HIGH',
          tasks: [
            { label: 'Attention: Core concepts', estMinutes: 90 },
            { label: 'Attention: Implementation', estMinutes: 75 },
            { label: 'Attention: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Attention help?'
        },
        {
          globalDay: 143,
          week: 21,
          title: 'MHA',
          priority: 'HIGH',
          tasks: [
            { label: 'MHA: Core concepts', estMinutes: 90 },
            { label: 'MHA: Implementation', estMinutes: 75 },
            { label: 'MHA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does MHA help?'
        },
        {
          globalDay: 144,
          week: 21,
          title: 'Positional encoding',
          priority: 'HIGH',
          tasks: [
            { label: 'Positional encoding: Core concepts', estMinutes: 90 },
            { label: 'Positional encoding: Implementation', estMinutes: 75 },
            { label: 'Positional encoding: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Positional encoding help?'
        },
        {
          globalDay: 145,
          week: 21,
          title: 'LayerNorm',
          priority: 'HIGH',
          tasks: [
            { label: 'LayerNorm: Core concepts', estMinutes: 90 },
            { label: 'LayerNorm: Implementation', estMinutes: 75 },
            { label: 'LayerNorm: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does LayerNorm help?'
        },
        {
          globalDay: 146,
          week: 21,
          title: 'Transformer blocks',
          priority: 'HIGH',
          tasks: [
            { label: 'Transformer blocks: Core concepts', estMinutes: 90 },
            { label: 'Transformer blocks: Implementation', estMinutes: 75 },
            { label: 'Transformer blocks: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Transformer blocks help?'
        },
        {
          globalDay: 147,
          week: 21,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        },
        {
          globalDay: 148,
          week: 21,
          title: 'Week 21 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 149,
          week: 22,
          title: 'Attention',
          priority: 'HIGH',
          tasks: [
            { label: 'Attention: Core concepts', estMinutes: 90 },
            { label: 'Attention: Implementation', estMinutes: 75 },
            { label: 'Attention: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Attention help?'
        },
        {
          globalDay: 150,
          week: 22,
          title: 'MHA',
          priority: 'HIGH',
          tasks: [
            { label: 'MHA: Core concepts', estMinutes: 90 },
            { label: 'MHA: Implementation', estMinutes: 75 },
            { label: 'MHA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does MHA help?'
        },
        {
          globalDay: 151,
          week: 22,
          title: 'Positional encoding',
          priority: 'HIGH',
          tasks: [
            { label: 'Positional encoding: Core concepts', estMinutes: 90 },
            { label: 'Positional encoding: Implementation', estMinutes: 75 },
            { label: 'Positional encoding: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Positional encoding help?'
        },
        {
          globalDay: 152,
          week: 22,
          title: 'LayerNorm',
          priority: 'HIGH',
          tasks: [
            { label: 'LayerNorm: Core concepts', estMinutes: 90 },
            { label: 'LayerNorm: Implementation', estMinutes: 75 },
            { label: 'LayerNorm: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does LayerNorm help?'
        },
        {
          globalDay: 153,
          week: 22,
          title: 'Transformer blocks',
          priority: 'HIGH',
          tasks: [
            { label: 'Transformer blocks: Core concepts', estMinutes: 90 },
            { label: 'Transformer blocks: Implementation', estMinutes: 75 },
            { label: 'Transformer blocks: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Transformer blocks help?'
        },
        {
          globalDay: 154,
          week: 22,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        }
      ]
    },
    {
      id: 'gpt-from-scratch',
      title: 'Phase 7: GPT from Scratch (Character-Level)',
      description: 'Full training loop with gradient accumulation, mixed precision, and sampling strategies.',
      duration: '42 days (Weeks 23-28)',
      weeks: [23, 24, 25, 26, 27, 28],
      days: [
        {
          globalDay: 155,
          week: 23,
          title: 'GPT Architecture Design',
          priority: 'HIGH',
          tasks: [
            { label: 'Read GPT-2 paper - pass 1', estMinutes: 75, resourceLinks: ['https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf'] },
            { label: 'Study nanoGPT architecture', estMinutes: 90, resourceLinks: ['https://github.com/karpathy/nanoGPT'] },
            { label: 'Create notebooks/gpt/day155_gpt_design.ipynb', estMinutes: 120 },
            { label: 'Design GPT config (n_layers, n_heads, d_model, vocab_size)', estMinutes: 60 },
            { label: 'Write docs/notes/day155_gpt_arch.md', estMinutes: 45 }
          ],
          reflectionPrompt: 'What are the key differences between GPT and BERT architectures?'
        },
                {
          globalDay: 156,
          week: 23,
          title: 'Architecture',
          priority: 'HIGH',
          tasks: [
            { label: 'Architecture: Core concepts', estMinutes: 90 },
            { label: 'Architecture: Implementation', estMinutes: 75 },
            { label: 'Architecture: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Architecture help?'
        },
        {
          globalDay: 157,
          week: 23,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90 },
            { label: 'Training: Implementation', estMinutes: 75 },
            { label: 'Training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 158,
          week: 23,
          title: 'Sampling',
          priority: 'HIGH',
          tasks: [
            { label: 'Sampling: Core concepts', estMinutes: 90 },
            { label: 'Sampling: Implementation', estMinutes: 75 },
            { label: 'Sampling: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Sampling help?'
        },
        {
          globalDay: 159,
          week: 23,
          title: 'Mixed precision',
          priority: 'HIGH',
          tasks: [
            { label: 'Mixed precision: Core concepts', estMinutes: 90 },
            { label: 'Mixed precision: Implementation', estMinutes: 75 },
            { label: 'Mixed precision: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Mixed precision help?'
        },
        {
          globalDay: 160,
          week: 23,
          title: 'Char-level',
          priority: 'HIGH',
          tasks: [
            { label: 'Char-level: Core concepts', estMinutes: 90 },
            { label: 'Char-level: Implementation', estMinutes: 75 },
            { label: 'Char-level: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Char-level help?'
        },
        {
          globalDay: 161,
          week: 23,
          title: 'Validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Validation: Core concepts', estMinutes: 90 },
            { label: 'Validation: Implementation', estMinutes: 75 },
            { label: 'Validation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Validation help?'
        },
        {
          globalDay: 162,
          week: 23,
          title: 'Week 23 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 163,
          week: 24,
          title: 'Architecture',
          priority: 'HIGH',
          tasks: [
            { label: 'Architecture: Core concepts', estMinutes: 90 },
            { label: 'Architecture: Implementation', estMinutes: 75 },
            { label: 'Architecture: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Architecture help?'
        },
        {
          globalDay: 164,
          week: 24,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90 },
            { label: 'Training: Implementation', estMinutes: 75 },
            { label: 'Training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 165,
          week: 24,
          title: 'Sampling',
          priority: 'HIGH',
          tasks: [
            { label: 'Sampling: Core concepts', estMinutes: 90 },
            { label: 'Sampling: Implementation', estMinutes: 75 },
            { label: 'Sampling: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Sampling help?'
        },
        {
          globalDay: 166,
          week: 24,
          title: 'Mixed precision',
          priority: 'HIGH',
          tasks: [
            { label: 'Mixed precision: Core concepts', estMinutes: 90 },
            { label: 'Mixed precision: Implementation', estMinutes: 75 },
            { label: 'Mixed precision: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Mixed precision help?'
        },
        {
          globalDay: 167,
          week: 24,
          title: 'Char-level',
          priority: 'HIGH',
          tasks: [
            { label: 'Char-level: Core concepts', estMinutes: 90 },
            { label: 'Char-level: Implementation', estMinutes: 75 },
            { label: 'Char-level: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Char-level help?'
        },
        {
          globalDay: 168,
          week: 24,
          title: 'Validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Validation: Core concepts', estMinutes: 90 },
            { label: 'Validation: Implementation', estMinutes: 75 },
            { label: 'Validation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Validation help?'
        },
        {
          globalDay: 169,
          week: 24,
          title: 'Week 24 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 170,
          week: 25,
          title: 'Architecture',
          priority: 'HIGH',
          tasks: [
            { label: 'Architecture: Core concepts', estMinutes: 90 },
            { label: 'Architecture: Implementation', estMinutes: 75 },
            { label: 'Architecture: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Architecture help?'
        },
        {
          globalDay: 171,
          week: 25,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90 },
            { label: 'Training: Implementation', estMinutes: 75 },
            { label: 'Training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 172,
          week: 25,
          title: 'Sampling',
          priority: 'HIGH',
          tasks: [
            { label: 'Sampling: Core concepts', estMinutes: 90 },
            { label: 'Sampling: Implementation', estMinutes: 75 },
            { label: 'Sampling: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Sampling help?'
        },
        {
          globalDay: 173,
          week: 25,
          title: 'Mixed precision',
          priority: 'HIGH',
          tasks: [
            { label: 'Mixed precision: Core concepts', estMinutes: 90 },
            { label: 'Mixed precision: Implementation', estMinutes: 75 },
            { label: 'Mixed precision: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Mixed precision help?'
        },
        {
          globalDay: 174,
          week: 25,
          title: 'Char-level',
          priority: 'HIGH',
          tasks: [
            { label: 'Char-level: Core concepts', estMinutes: 90 },
            { label: 'Char-level: Implementation', estMinutes: 75 },
            { label: 'Char-level: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Char-level help?'
        },
        {
          globalDay: 175,
          week: 25,
          title: 'Validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Validation: Core concepts', estMinutes: 90 },
            { label: 'Validation: Implementation', estMinutes: 75 },
            { label: 'Validation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Validation help?'
        },
        {
          globalDay: 176,
          week: 25,
          title: 'Week 25 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 177,
          week: 26,
          title: 'Architecture',
          priority: 'HIGH',
          tasks: [
            { label: 'Architecture: Core concepts', estMinutes: 90 },
            { label: 'Architecture: Implementation', estMinutes: 75 },
            { label: 'Architecture: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Architecture help?'
        },
        {
          globalDay: 178,
          week: 26,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90 },
            { label: 'Training: Implementation', estMinutes: 75 },
            { label: 'Training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 179,
          week: 26,
          title: 'Sampling',
          priority: 'HIGH',
          tasks: [
            { label: 'Sampling: Core concepts', estMinutes: 90 },
            { label: 'Sampling: Implementation', estMinutes: 75 },
            { label: 'Sampling: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Sampling help?'
        },
        {
          globalDay: 180,
          week: 26,
          title: 'Mixed precision',
          priority: 'HIGH',
          tasks: [
            { label: 'Mixed precision: Core concepts', estMinutes: 90 },
            { label: 'Mixed precision: Implementation', estMinutes: 75 },
            { label: 'Mixed precision: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Mixed precision help?'
        },
        {
          globalDay: 181,
          week: 26,
          title: 'Char-level',
          priority: 'HIGH',
          tasks: [
            { label: 'Char-level: Core concepts', estMinutes: 90 },
            { label: 'Char-level: Implementation', estMinutes: 75 },
            { label: 'Char-level: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Char-level help?'
        },
        {
          globalDay: 182,
          week: 26,
          title: 'Validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Validation: Core concepts', estMinutes: 90 },
            { label: 'Validation: Implementation', estMinutes: 75 },
            { label: 'Validation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Validation help?'
        },
        {
          globalDay: 183,
          week: 26,
          title: 'Week 26 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 184,
          week: 27,
          title: 'Architecture',
          priority: 'HIGH',
          tasks: [
            { label: 'Architecture: Core concepts', estMinutes: 90 },
            { label: 'Architecture: Implementation', estMinutes: 75 },
            { label: 'Architecture: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Architecture help?'
        },
        {
          globalDay: 185,
          week: 27,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90 },
            { label: 'Training: Implementation', estMinutes: 75 },
            { label: 'Training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 186,
          week: 27,
          title: 'Sampling',
          priority: 'HIGH',
          tasks: [
            { label: 'Sampling: Core concepts', estMinutes: 90 },
            { label: 'Sampling: Implementation', estMinutes: 75 },
            { label: 'Sampling: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Sampling help?'
        },
        {
          globalDay: 187,
          week: 27,
          title: 'Mixed precision',
          priority: 'HIGH',
          tasks: [
            { label: 'Mixed precision: Core concepts', estMinutes: 90 },
            { label: 'Mixed precision: Implementation', estMinutes: 75 },
            { label: 'Mixed precision: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Mixed precision help?'
        },
        {
          globalDay: 188,
          week: 27,
          title: 'Char-level',
          priority: 'HIGH',
          tasks: [
            { label: 'Char-level: Core concepts', estMinutes: 90 },
            { label: 'Char-level: Implementation', estMinutes: 75 },
            { label: 'Char-level: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Char-level help?'
        },
        {
          globalDay: 189,
          week: 27,
          title: 'Validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Validation: Core concepts', estMinutes: 90 },
            { label: 'Validation: Implementation', estMinutes: 75 },
            { label: 'Validation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Validation help?'
        },
        {
          globalDay: 190,
          week: 27,
          title: 'Week 27 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 191,
          week: 28,
          title: 'Architecture',
          priority: 'HIGH',
          tasks: [
            { label: 'Architecture: Core concepts', estMinutes: 90 },
            { label: 'Architecture: Implementation', estMinutes: 75 },
            { label: 'Architecture: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Architecture help?'
        },
        {
          globalDay: 192,
          week: 28,
          title: 'Training',
          priority: 'HIGH',
          tasks: [
            { label: 'Training: Core concepts', estMinutes: 90 },
            { label: 'Training: Implementation', estMinutes: 75 },
            { label: 'Training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Training help?'
        },
        {
          globalDay: 193,
          week: 28,
          title: 'Sampling',
          priority: 'HIGH',
          tasks: [
            { label: 'Sampling: Core concepts', estMinutes: 90 },
            { label: 'Sampling: Implementation', estMinutes: 75 },
            { label: 'Sampling: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Sampling help?'
        },
        {
          globalDay: 194,
          week: 28,
          title: 'Mixed precision',
          priority: 'HIGH',
          tasks: [
            { label: 'Mixed precision: Core concepts', estMinutes: 90 },
            { label: 'Mixed precision: Implementation', estMinutes: 75 },
            { label: 'Mixed precision: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Mixed precision help?'
        },
        {
          globalDay: 195,
          week: 28,
          title: 'Char-level',
          priority: 'HIGH',
          tasks: [
            { label: 'Char-level: Core concepts', estMinutes: 90 },
            { label: 'Char-level: Implementation', estMinutes: 75 },
            { label: 'Char-level: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Char-level help?'
        },
        {
          globalDay: 196,
          week: 28,
          title: 'Validation',
          priority: 'HIGH',
          tasks: [
            { label: 'Validation: Core concepts', estMinutes: 90 },
            { label: 'Validation: Implementation', estMinutes: 75 },
            { label: 'Validation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Validation help?'
        }
      ]
    },
    {
      id: 'tokenizer-scaling',
      title: 'Phase 8: BPE Tokenizer + Data Curation + Scaling Laws',
      description: 'Train BPE tokenizer with 16k vocab, curate datasets, and run small-scale scaling experiments.',
      duration: '35 days (Weeks 29-33)',
      weeks: [29, 30, 31, 32, 33],
      days: [
        {
          globalDay: 197,
          week: 29,
          title: 'BPE Algorithm Study',
          priority: 'HIGH',
          tasks: [
            { label: 'Read "Neural Machine Translation of Rare Words with Subword Units"', estMinutes: 60 },
            { label: 'Study tokenizers library documentation', estMinutes: 45, resourceLinks: ['https://huggingface.co/docs/tokenizers/'] },
            { label: 'Create notebooks/tokenizer/day197_bpe_study.ipynb', estMinutes: 90 },
            { label: 'Implement toy BPE on small corpus (100 merges)', estMinutes: 120 },
            { label: 'Write docs/notes/day197_bpe.md', estMinutes: 30 }
          ],
          reflectionPrompt: 'Why does BPE handle rare words better than word-level tokenization?'
        },
                {
          globalDay: 198,
          week: 29,
          title: 'BPE',
          priority: 'HIGH',
          tasks: [
            { label: 'BPE: Core concepts', estMinutes: 90 },
            { label: 'BPE: Implementation', estMinutes: 75 },
            { label: 'BPE: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does BPE help?'
        },
        {
          globalDay: 199,
          week: 29,
          title: 'Data curation',
          priority: 'HIGH',
          tasks: [
            { label: 'Data curation: Core concepts', estMinutes: 90 },
            { label: 'Data curation: Implementation', estMinutes: 75 },
            { label: 'Data curation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data curation help?'
        },
        {
          globalDay: 200,
          week: 29,
          title: 'Scaling laws',
          priority: 'HIGH',
          tasks: [
            { label: 'Scaling laws: Core concepts', estMinutes: 90 },
            { label: 'Scaling laws: Implementation', estMinutes: 75 },
            { label: 'Scaling laws: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Scaling laws help?'
        },
        {
          globalDay: 201,
          week: 29,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        },
        {
          globalDay: 202,
          week: 29,
          title: 'Compute tracking',
          priority: 'HIGH',
          tasks: [
            { label: 'Compute tracking: Core concepts', estMinutes: 90 },
            { label: 'Compute tracking: Implementation', estMinutes: 75 },
            { label: 'Compute tracking: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Compute tracking help?'
        },
        {
          globalDay: 203,
          week: 29,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        },
        {
          globalDay: 204,
          week: 29,
          title: 'Week 29 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 205,
          week: 30,
          title: 'Data curation',
          priority: 'HIGH',
          tasks: [
            { label: 'Data curation: Core concepts', estMinutes: 90 },
            { label: 'Data curation: Implementation', estMinutes: 75 },
            { label: 'Data curation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data curation help?'
        },
        {
          globalDay: 206,
          week: 30,
          title: 'Scaling laws',
          priority: 'HIGH',
          tasks: [
            { label: 'Scaling laws: Core concepts', estMinutes: 90 },
            { label: 'Scaling laws: Implementation', estMinutes: 75 },
            { label: 'Scaling laws: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Scaling laws help?'
        },
        {
          globalDay: 207,
          week: 30,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        },
        {
          globalDay: 208,
          week: 30,
          title: 'Compute tracking',
          priority: 'HIGH',
          tasks: [
            { label: 'Compute tracking: Core concepts', estMinutes: 90 },
            { label: 'Compute tracking: Implementation', estMinutes: 75 },
            { label: 'Compute tracking: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Compute tracking help?'
        },
        {
          globalDay: 209,
          week: 30,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        },
        {
          globalDay: 210,
          week: 30,
          title: 'BPE',
          priority: 'HIGH',
          tasks: [
            { label: 'BPE: Core concepts', estMinutes: 90 },
            { label: 'BPE: Implementation', estMinutes: 75 },
            { label: 'BPE: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does BPE help?'
        },
        {
          globalDay: 211,
          week: 30,
          title: 'Week 30 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 212,
          week: 31,
          title: 'Scaling laws',
          priority: 'HIGH',
          tasks: [
            { label: 'Scaling laws: Core concepts', estMinutes: 90 },
            { label: 'Scaling laws: Implementation', estMinutes: 75 },
            { label: 'Scaling laws: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Scaling laws help?'
        },
        {
          globalDay: 213,
          week: 31,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        },
        {
          globalDay: 214,
          week: 31,
          title: 'Compute tracking',
          priority: 'HIGH',
          tasks: [
            { label: 'Compute tracking: Core concepts', estMinutes: 90 },
            { label: 'Compute tracking: Implementation', estMinutes: 75 },
            { label: 'Compute tracking: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Compute tracking help?'
        },
        {
          globalDay: 215,
          week: 31,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        },
        {
          globalDay: 216,
          week: 31,
          title: 'BPE',
          priority: 'HIGH',
          tasks: [
            { label: 'BPE: Core concepts', estMinutes: 90 },
            { label: 'BPE: Implementation', estMinutes: 75 },
            { label: 'BPE: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does BPE help?'
        },
        {
          globalDay: 217,
          week: 31,
          title: 'Data curation',
          priority: 'HIGH',
          tasks: [
            { label: 'Data curation: Core concepts', estMinutes: 90 },
            { label: 'Data curation: Implementation', estMinutes: 75 },
            { label: 'Data curation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data curation help?'
        },
        {
          globalDay: 218,
          week: 31,
          title: 'Week 31 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 219,
          week: 32,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        },
        {
          globalDay: 220,
          week: 32,
          title: 'Compute tracking',
          priority: 'HIGH',
          tasks: [
            { label: 'Compute tracking: Core concepts', estMinutes: 90 },
            { label: 'Compute tracking: Implementation', estMinutes: 75 },
            { label: 'Compute tracking: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Compute tracking help?'
        },
        {
          globalDay: 221,
          week: 32,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        },
        {
          globalDay: 222,
          week: 32,
          title: 'BPE',
          priority: 'HIGH',
          tasks: [
            { label: 'BPE: Core concepts', estMinutes: 90 },
            { label: 'BPE: Implementation', estMinutes: 75 },
            { label: 'BPE: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does BPE help?'
        },
        {
          globalDay: 223,
          week: 32,
          title: 'Data curation',
          priority: 'HIGH',
          tasks: [
            { label: 'Data curation: Core concepts', estMinutes: 90 },
            { label: 'Data curation: Implementation', estMinutes: 75 },
            { label: 'Data curation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data curation help?'
        },
        {
          globalDay: 224,
          week: 32,
          title: 'Scaling laws',
          priority: 'HIGH',
          tasks: [
            { label: 'Scaling laws: Core concepts', estMinutes: 90 },
            { label: 'Scaling laws: Implementation', estMinutes: 75 },
            { label: 'Scaling laws: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Scaling laws help?'
        },
        {
          globalDay: 225,
          week: 32,
          title: 'Week 32 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 226,
          week: 33,
          title: 'Compute tracking',
          priority: 'HIGH',
          tasks: [
            { label: 'Compute tracking: Core concepts', estMinutes: 90 },
            { label: 'Compute tracking: Implementation', estMinutes: 75 },
            { label: 'Compute tracking: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Compute tracking help?'
        },
        {
          globalDay: 227,
          week: 33,
          title: 'Papers',
          priority: 'HIGH',
          tasks: [
            { label: 'Papers: Core concepts', estMinutes: 90 },
            { label: 'Papers: Implementation', estMinutes: 75 },
            { label: 'Papers: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Papers help?'
        },
        {
          globalDay: 228,
          week: 33,
          title: 'BPE',
          priority: 'HIGH',
          tasks: [
            { label: 'BPE: Core concepts', estMinutes: 90 },
            { label: 'BPE: Implementation', estMinutes: 75 },
            { label: 'BPE: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does BPE help?'
        },
        {
          globalDay: 229,
          week: 33,
          title: 'Data curation',
          priority: 'HIGH',
          tasks: [
            { label: 'Data curation: Core concepts', estMinutes: 90 },
            { label: 'Data curation: Implementation', estMinutes: 75 },
            { label: 'Data curation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data curation help?'
        },
        {
          globalDay: 230,
          week: 33,
          title: 'Scaling laws',
          priority: 'HIGH',
          tasks: [
            { label: 'Scaling laws: Core concepts', estMinutes: 90 },
            { label: 'Scaling laws: Implementation', estMinutes: 75 },
            { label: 'Scaling laws: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Scaling laws help?'
        },
        {
          globalDay: 231,
          week: 33,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        }
      ]
    },
    {
      id: 'serving-safety',
      title: 'Phase 9: Ethics, Safety & MVP Serving',
      description: 'Safety filters, red-teaming, FastAPI, Docker, HTTPS, and minimal UI.',
      duration: '21 days (Weeks 34-36)',
      weeks: [34, 35, 36],
      days: [
        {
          globalDay: 232,
          week: 34,
          title: 'AI Safety Fundamentals',
          priority: 'HIGH',
          tasks: [
            { label: 'Read "Concrete Problems in AI Safety" paper', estMinutes: 90 },
            { label: 'Create docs/safety.md document', estMinutes: 60 },
            { label: 'List potential failure modes for your model', estMinutes: 45 },
            { label: 'Design safety filter architecture', estMinutes: 60 },
            { label: 'Write docs/notes/day232_safety.md', estMinutes: 30 }
          ],
          reflectionPrompt: 'What are the most critical safety concerns for a student LLM project?'
        },
                {
          globalDay: 233,
          week: 34,
          title: 'Safety',
          priority: 'HIGH',
          tasks: [
            { label: 'Safety: Core concepts', estMinutes: 90 },
            { label: 'Safety: Implementation', estMinutes: 75 },
            { label: 'Safety: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Safety help?'
        },
        {
          globalDay: 234,
          week: 34,
          title: 'FastAPI',
          priority: 'HIGH',
          tasks: [
            { label: 'FastAPI: Core concepts', estMinutes: 90 },
            { label: 'FastAPI: Implementation', estMinutes: 75 },
            { label: 'FastAPI: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does FastAPI help?'
        },
        {
          globalDay: 235,
          week: 34,
          title: 'Docker',
          priority: 'HIGH',
          tasks: [
            { label: 'Docker: Core concepts', estMinutes: 90 },
            { label: 'Docker: Implementation', estMinutes: 75 },
            { label: 'Docker: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Docker help?'
        },
        {
          globalDay: 236,
          week: 34,
          title: 'HTTPS',
          priority: 'HIGH',
          tasks: [
            { label: 'HTTPS: Core concepts', estMinutes: 90 },
            { label: 'HTTPS: Implementation', estMinutes: 75 },
            { label: 'HTTPS: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does HTTPS help?'
        },
        {
          globalDay: 237,
          week: 34,
          title: 'Monitoring',
          priority: 'HIGH',
          tasks: [
            { label: 'Monitoring: Core concepts', estMinutes: 90 },
            { label: 'Monitoring: Implementation', estMinutes: 75 },
            { label: 'Monitoring: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Monitoring help?'
        },
        {
          globalDay: 238,
          week: 34,
          title: 'UI',
          priority: 'HIGH',
          tasks: [
            { label: 'UI: Core concepts', estMinutes: 90 },
            { label: 'UI: Implementation', estMinutes: 75 },
            { label: 'UI: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does UI help?'
        },
        {
          globalDay: 239,
          week: 34,
          title: 'Week 34 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 240,
          week: 35,
          title: 'Safety',
          priority: 'HIGH',
          tasks: [
            { label: 'Safety: Core concepts', estMinutes: 90 },
            { label: 'Safety: Implementation', estMinutes: 75 },
            { label: 'Safety: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Safety help?'
        },
        {
          globalDay: 241,
          week: 35,
          title: 'FastAPI',
          priority: 'HIGH',
          tasks: [
            { label: 'FastAPI: Core concepts', estMinutes: 90 },
            { label: 'FastAPI: Implementation', estMinutes: 75 },
            { label: 'FastAPI: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does FastAPI help?'
        },
        {
          globalDay: 242,
          week: 35,
          title: 'Docker',
          priority: 'HIGH',
          tasks: [
            { label: 'Docker: Core concepts', estMinutes: 90 },
            { label: 'Docker: Implementation', estMinutes: 75 },
            { label: 'Docker: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Docker help?'
        },
        {
          globalDay: 243,
          week: 35,
          title: 'HTTPS',
          priority: 'HIGH',
          tasks: [
            { label: 'HTTPS: Core concepts', estMinutes: 90 },
            { label: 'HTTPS: Implementation', estMinutes: 75 },
            { label: 'HTTPS: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does HTTPS help?'
        },
        {
          globalDay: 244,
          week: 35,
          title: 'Monitoring',
          priority: 'HIGH',
          tasks: [
            { label: 'Monitoring: Core concepts', estMinutes: 90 },
            { label: 'Monitoring: Implementation', estMinutes: 75 },
            { label: 'Monitoring: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Monitoring help?'
        },
        {
          globalDay: 245,
          week: 35,
          title: 'UI',
          priority: 'HIGH',
          tasks: [
            { label: 'UI: Core concepts', estMinutes: 90 },
            { label: 'UI: Implementation', estMinutes: 75 },
            { label: 'UI: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does UI help?'
        },
        {
          globalDay: 246,
          week: 35,
          title: 'Week 35 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 247,
          week: 36,
          title: 'Safety',
          priority: 'HIGH',
          tasks: [
            { label: 'Safety: Core concepts', estMinutes: 90 },
            { label: 'Safety: Implementation', estMinutes: 75 },
            { label: 'Safety: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Safety help?'
        },
        {
          globalDay: 248,
          week: 36,
          title: 'FastAPI',
          priority: 'HIGH',
          tasks: [
            { label: 'FastAPI: Core concepts', estMinutes: 90 },
            { label: 'FastAPI: Implementation', estMinutes: 75 },
            { label: 'FastAPI: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does FastAPI help?'
        },
        {
          globalDay: 249,
          week: 36,
          title: 'Docker',
          priority: 'HIGH',
          tasks: [
            { label: 'Docker: Core concepts', estMinutes: 90 },
            { label: 'Docker: Implementation', estMinutes: 75 },
            { label: 'Docker: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Docker help?'
        },
        {
          globalDay: 250,
          week: 36,
          title: 'HTTPS',
          priority: 'HIGH',
          tasks: [
            { label: 'HTTPS: Core concepts', estMinutes: 90 },
            { label: 'HTTPS: Implementation', estMinutes: 75 },
            { label: 'HTTPS: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does HTTPS help?'
        },
        {
          globalDay: 251,
          week: 36,
          title: 'Monitoring',
          priority: 'HIGH',
          tasks: [
            { label: 'Monitoring: Core concepts', estMinutes: 90 },
            { label: 'Monitoring: Implementation', estMinutes: 75 },
            { label: 'Monitoring: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Monitoring help?'
        },
        {
          globalDay: 252,
          week: 36,
          title: 'UI',
          priority: 'HIGH',
          tasks: [
            { label: 'UI: Core concepts', estMinutes: 90 },
            { label: 'UI: Implementation', estMinutes: 75 },
            { label: 'UI: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does UI help?'
        }
      ]
    },
    {
      id: 'peft-optimization',
      title: 'Phase 10: PEFT & Inference Optimization',
      description: 'LoRA/QLoRA fine-tuning, quantization (8/4-bit), and KV-cache implementation.',
      duration: '28 days (Weeks 37-40)',
      weeks: [37, 38, 39, 40],
      days: [
        {
          globalDay: 253,
          week: 37,
          title: 'LoRA Paper Study & Theory',
          priority: 'HIGH',
          tasks: [
            { label: 'Read "LoRA: Low-Rank Adaptation" paper - 3 passes', estMinutes: 120, resourceLinks: ['https://arxiv.org/abs/2106.09685'] },
            { label: 'Create notebooks/peft/day253_lora_theory.ipynb', estMinutes: 90 },
            { label: 'Derive math for low-rank updates (W + AB)', estMinutes: 75 },
            { label: 'Estimate parameter reduction for different ranks', estMinutes: 45 },
            { label: 'Write docs/notes/day253_lora.md', estMinutes: 45 }
          ],
          reflectionPrompt: 'Why does low-rank adaptation work so well for fine-tuning?'
        },
                {
          globalDay: 254,
          week: 37,
          title: 'LoRA',
          priority: 'HIGH',
          tasks: [
            { label: 'LoRA: Core concepts', estMinutes: 90 },
            { label: 'LoRA: Implementation', estMinutes: 75 },
            { label: 'LoRA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does LoRA help?'
        },
        {
          globalDay: 255,
          week: 37,
          title: 'QLoRA',
          priority: 'HIGH',
          tasks: [
            { label: 'QLoRA: Core concepts', estMinutes: 90 },
            { label: 'QLoRA: Implementation', estMinutes: 75 },
            { label: 'QLoRA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does QLoRA help?'
        },
        {
          globalDay: 256,
          week: 37,
          title: 'Quantization',
          priority: 'HIGH',
          tasks: [
            { label: 'Quantization: Core concepts', estMinutes: 90 },
            { label: 'Quantization: Implementation', estMinutes: 75 },
            { label: 'Quantization: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Quantization help?'
        },
        {
          globalDay: 257,
          week: 37,
          title: 'KV-cache',
          priority: 'HIGH',
          tasks: [
            { label: 'KV-cache: Core concepts', estMinutes: 90 },
            { label: 'KV-cache: Implementation', estMinutes: 75 },
            { label: 'KV-cache: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does KV-cache help?'
        },
        {
          globalDay: 258,
          week: 37,
          title: 'Inference opt',
          priority: 'HIGH',
          tasks: [
            { label: 'Inference opt: Core concepts', estMinutes: 90 },
            { label: 'Inference opt: Implementation', estMinutes: 75 },
            { label: 'Inference opt: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Inference opt help?'
        },
        {
          globalDay: 259,
          week: 37,
          title: 'Fine-tuning',
          priority: 'HIGH',
          tasks: [
            { label: 'Fine-tuning: Core concepts', estMinutes: 90 },
            { label: 'Fine-tuning: Implementation', estMinutes: 75 },
            { label: 'Fine-tuning: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Fine-tuning help?'
        },
        {
          globalDay: 260,
          week: 37,
          title: 'Week 37 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 261,
          week: 38,
          title: 'LoRA',
          priority: 'HIGH',
          tasks: [
            { label: 'LoRA: Core concepts', estMinutes: 90 },
            { label: 'LoRA: Implementation', estMinutes: 75 },
            { label: 'LoRA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does LoRA help?'
        },
        {
          globalDay: 262,
          week: 38,
          title: 'QLoRA',
          priority: 'HIGH',
          tasks: [
            { label: 'QLoRA: Core concepts', estMinutes: 90 },
            { label: 'QLoRA: Implementation', estMinutes: 75 },
            { label: 'QLoRA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does QLoRA help?'
        },
        {
          globalDay: 263,
          week: 38,
          title: 'Quantization',
          priority: 'HIGH',
          tasks: [
            { label: 'Quantization: Core concepts', estMinutes: 90 },
            { label: 'Quantization: Implementation', estMinutes: 75 },
            { label: 'Quantization: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Quantization help?'
        },
        {
          globalDay: 264,
          week: 38,
          title: 'KV-cache',
          priority: 'HIGH',
          tasks: [
            { label: 'KV-cache: Core concepts', estMinutes: 90 },
            { label: 'KV-cache: Implementation', estMinutes: 75 },
            { label: 'KV-cache: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does KV-cache help?'
        },
        {
          globalDay: 265,
          week: 38,
          title: 'Inference opt',
          priority: 'HIGH',
          tasks: [
            { label: 'Inference opt: Core concepts', estMinutes: 90 },
            { label: 'Inference opt: Implementation', estMinutes: 75 },
            { label: 'Inference opt: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Inference opt help?'
        },
        {
          globalDay: 266,
          week: 38,
          title: 'Fine-tuning',
          priority: 'HIGH',
          tasks: [
            { label: 'Fine-tuning: Core concepts', estMinutes: 90 },
            { label: 'Fine-tuning: Implementation', estMinutes: 75 },
            { label: 'Fine-tuning: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Fine-tuning help?'
        },
        {
          globalDay: 267,
          week: 38,
          title: 'Week 38 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 268,
          week: 39,
          title: 'LoRA',
          priority: 'HIGH',
          tasks: [
            { label: 'LoRA: Core concepts', estMinutes: 90 },
            { label: 'LoRA: Implementation', estMinutes: 75 },
            { label: 'LoRA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does LoRA help?'
        },
        {
          globalDay: 269,
          week: 39,
          title: 'QLoRA',
          priority: 'HIGH',
          tasks: [
            { label: 'QLoRA: Core concepts', estMinutes: 90 },
            { label: 'QLoRA: Implementation', estMinutes: 75 },
            { label: 'QLoRA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does QLoRA help?'
        },
        {
          globalDay: 270,
          week: 39,
          title: 'Quantization',
          priority: 'HIGH',
          tasks: [
            { label: 'Quantization: Core concepts', estMinutes: 90 },
            { label: 'Quantization: Implementation', estMinutes: 75 },
            { label: 'Quantization: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Quantization help?'
        },
        {
          globalDay: 271,
          week: 39,
          title: 'KV-cache',
          priority: 'HIGH',
          tasks: [
            { label: 'KV-cache: Core concepts', estMinutes: 90 },
            { label: 'KV-cache: Implementation', estMinutes: 75 },
            { label: 'KV-cache: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does KV-cache help?'
        },
        {
          globalDay: 272,
          week: 39,
          title: 'Inference opt',
          priority: 'HIGH',
          tasks: [
            { label: 'Inference opt: Core concepts', estMinutes: 90 },
            { label: 'Inference opt: Implementation', estMinutes: 75 },
            { label: 'Inference opt: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Inference opt help?'
        },
        {
          globalDay: 273,
          week: 39,
          title: 'Fine-tuning',
          priority: 'HIGH',
          tasks: [
            { label: 'Fine-tuning: Core concepts', estMinutes: 90 },
            { label: 'Fine-tuning: Implementation', estMinutes: 75 },
            { label: 'Fine-tuning: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Fine-tuning help?'
        },
        {
          globalDay: 274,
          week: 39,
          title: 'Week 39 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 275,
          week: 40,
          title: 'LoRA',
          priority: 'HIGH',
          tasks: [
            { label: 'LoRA: Core concepts', estMinutes: 90 },
            { label: 'LoRA: Implementation', estMinutes: 75 },
            { label: 'LoRA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does LoRA help?'
        },
        {
          globalDay: 276,
          week: 40,
          title: 'QLoRA',
          priority: 'HIGH',
          tasks: [
            { label: 'QLoRA: Core concepts', estMinutes: 90 },
            { label: 'QLoRA: Implementation', estMinutes: 75 },
            { label: 'QLoRA: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does QLoRA help?'
        },
        {
          globalDay: 277,
          week: 40,
          title: 'Quantization',
          priority: 'HIGH',
          tasks: [
            { label: 'Quantization: Core concepts', estMinutes: 90 },
            { label: 'Quantization: Implementation', estMinutes: 75 },
            { label: 'Quantization: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Quantization help?'
        },
        {
          globalDay: 278,
          week: 40,
          title: 'KV-cache',
          priority: 'HIGH',
          tasks: [
            { label: 'KV-cache: Core concepts', estMinutes: 90 },
            { label: 'KV-cache: Implementation', estMinutes: 75 },
            { label: 'KV-cache: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does KV-cache help?'
        },
        {
          globalDay: 279,
          week: 40,
          title: 'Inference opt',
          priority: 'HIGH',
          tasks: [
            { label: 'Inference opt: Core concepts', estMinutes: 90 },
            { label: 'Inference opt: Implementation', estMinutes: 75 },
            { label: 'Inference opt: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Inference opt help?'
        },
        {
          globalDay: 280,
          week: 40,
          title: 'Fine-tuning',
          priority: 'HIGH',
          tasks: [
            { label: 'Fine-tuning: Core concepts', estMinutes: 90 },
            { label: 'Fine-tuning: Implementation', estMinutes: 75 },
            { label: 'Fine-tuning: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Fine-tuning help?'
        }
      ]
    },
    {
      id: 'buffer-refactor',
      title: 'Phase 11: Buffer & Refactoring',
      description: 'Cleanup, refactoring, and testing for tokenizer/sampling.',
      duration: '7 days (Week 41)',
      weeks: [41],
      days: [
        {
          globalDay: 281,
          week: 41,
          title: 'Code Refactoring Day',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review all src/ modules for code smells', estMinutes: 90 },
            { label: 'Refactor duplicated code into utils', estMinutes: 120 },
            { label: 'Add type hints to key functions', estMinutes: 60 },
            { label: 'Run black formatter on all files', estMinutes: 15 },
            { label: 'Update README with current status', estMinutes: 30 }
          ],
          reflectionPrompt: 'What patterns emerged that could be abstracted?'
        },
                {
          globalDay: 282,
          week: 41,
          title: 'Code cleanup',
          priority: 'HIGH',
          tasks: [
            { label: 'Code cleanup: Core concepts', estMinutes: 90 },
            { label: 'Code cleanup: Implementation', estMinutes: 75 },
            { label: 'Code cleanup: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Code cleanup help?'
        },
        {
          globalDay: 283,
          week: 41,
          title: 'Testing',
          priority: 'HIGH',
          tasks: [
            { label: 'Testing: Core concepts', estMinutes: 90 },
            { label: 'Testing: Implementation', estMinutes: 75 },
            { label: 'Testing: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Testing help?'
        },
        {
          globalDay: 284,
          week: 41,
          title: 'Type hints',
          priority: 'HIGH',
          tasks: [
            { label: 'Type hints: Core concepts', estMinutes: 90 },
            { label: 'Type hints: Implementation', estMinutes: 75 },
            { label: 'Type hints: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Type hints help?'
        },
        {
          globalDay: 285,
          week: 41,
          title: 'Documentation',
          priority: 'HIGH',
          tasks: [
            { label: 'Documentation: Core concepts', estMinutes: 90 },
            { label: 'Documentation: Implementation', estMinutes: 75 },
            { label: 'Documentation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Documentation help?'
        },
        {
          globalDay: 286,
          week: 41,
          title: 'Buffer',
          priority: 'HIGH',
          tasks: [
            { label: 'Buffer: Core concepts', estMinutes: 90 },
            { label: 'Buffer: Implementation', estMinutes: 75 },
            { label: 'Buffer: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Buffer help?'
        },
        {
          globalDay: 287,
          week: 41,
          title: 'Review',
          priority: 'HIGH',
          tasks: [
            { label: 'Review: Core concepts', estMinutes: 90 },
            { label: 'Review: Implementation', estMinutes: 75 },
            { label: 'Review: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Review help?'
        }
      ]
    },
    {
      id: 'mlops',
      title: 'Phase 12: MLOps Essentials',
      description: 'Pytest, black, CI, YAML configs, JSONL logging, cleanup scripts.',
      duration: '21 days (Weeks 42-44)',
      weeks: [42, 43, 44],
      days: [
        {
          globalDay: 288,
          week: 42,
          title: 'Comprehensive Testing Setup',
          priority: 'HIGH',
          tasks: [
            { label: 'Write tests/test_tokenizer.py with 10+ test cases', estMinutes: 120 },
            { label: 'Write tests/test_attention.py', estMinutes: 90 },
            { label: 'Write tests/test_model.py', estMinutes: 90 },
            { label: 'Achieve >80% code coverage', estMinutes: 60 },
            { label: 'Document testing strategy in docs/mlops/testing.md', estMinutes: 30 }
          ],
          reflectionPrompt: 'Which components are hardest to test? Why?'
        },
                {
          globalDay: 289,
          week: 42,
          title: 'Pytest',
          priority: 'HIGH',
          tasks: [
            { label: 'Pytest: Core concepts', estMinutes: 90 },
            { label: 'Pytest: Implementation', estMinutes: 75 },
            { label: 'Pytest: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Pytest help?'
        },
        {
          globalDay: 290,
          week: 42,
          title: 'Black',
          priority: 'HIGH',
          tasks: [
            { label: 'Black: Core concepts', estMinutes: 90 },
            { label: 'Black: Implementation', estMinutes: 75 },
            { label: 'Black: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Black help?'
        },
        {
          globalDay: 291,
          week: 42,
          title: 'CI/CD',
          priority: 'HIGH',
          tasks: [
            { label: 'CI/CD: Core concepts', estMinutes: 90 },
            { label: 'CI/CD: Implementation', estMinutes: 75 },
            { label: 'CI/CD: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does CI/CD help?'
        },
        {
          globalDay: 292,
          week: 42,
          title: 'YAML',
          priority: 'HIGH',
          tasks: [
            { label: 'YAML: Core concepts', estMinutes: 90 },
            { label: 'YAML: Implementation', estMinutes: 75 },
            { label: 'YAML: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does YAML help?'
        },
        {
          globalDay: 293,
          week: 42,
          title: 'Logging',
          priority: 'HIGH',
          tasks: [
            { label: 'Logging: Core concepts', estMinutes: 90 },
            { label: 'Logging: Implementation', estMinutes: 75 },
            { label: 'Logging: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Logging help?'
        },
        {
          globalDay: 294,
          week: 42,
          title: 'Monitoring',
          priority: 'HIGH',
          tasks: [
            { label: 'Monitoring: Core concepts', estMinutes: 90 },
            { label: 'Monitoring: Implementation', estMinutes: 75 },
            { label: 'Monitoring: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Monitoring help?'
        },
        {
          globalDay: 295,
          week: 42,
          title: 'Week 42 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 296,
          week: 43,
          title: 'Pytest',
          priority: 'HIGH',
          tasks: [
            { label: 'Pytest: Core concepts', estMinutes: 90 },
            { label: 'Pytest: Implementation', estMinutes: 75 },
            { label: 'Pytest: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Pytest help?'
        },
        {
          globalDay: 297,
          week: 43,
          title: 'Black',
          priority: 'HIGH',
          tasks: [
            { label: 'Black: Core concepts', estMinutes: 90 },
            { label: 'Black: Implementation', estMinutes: 75 },
            { label: 'Black: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Black help?'
        },
        {
          globalDay: 298,
          week: 43,
          title: 'CI/CD',
          priority: 'HIGH',
          tasks: [
            { label: 'CI/CD: Core concepts', estMinutes: 90 },
            { label: 'CI/CD: Implementation', estMinutes: 75 },
            { label: 'CI/CD: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does CI/CD help?'
        },
        {
          globalDay: 299,
          week: 43,
          title: 'YAML',
          priority: 'HIGH',
          tasks: [
            { label: 'YAML: Core concepts', estMinutes: 90 },
            { label: 'YAML: Implementation', estMinutes: 75 },
            { label: 'YAML: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does YAML help?'
        },
        {
          globalDay: 300,
          week: 43,
          title: 'Logging',
          priority: 'HIGH',
          tasks: [
            { label: 'Logging: Core concepts', estMinutes: 90 },
            { label: 'Logging: Implementation', estMinutes: 75 },
            { label: 'Logging: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Logging help?'
        },
        {
          globalDay: 301,
          week: 43,
          title: 'Monitoring',
          priority: 'HIGH',
          tasks: [
            { label: 'Monitoring: Core concepts', estMinutes: 90 },
            { label: 'Monitoring: Implementation', estMinutes: 75 },
            { label: 'Monitoring: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Monitoring help?'
        },
        {
          globalDay: 302,
          week: 43,
          title: 'Week 43 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 303,
          week: 44,
          title: 'Pytest',
          priority: 'HIGH',
          tasks: [
            { label: 'Pytest: Core concepts', estMinutes: 90 },
            { label: 'Pytest: Implementation', estMinutes: 75 },
            { label: 'Pytest: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Pytest help?'
        },
        {
          globalDay: 304,
          week: 44,
          title: 'Black',
          priority: 'HIGH',
          tasks: [
            { label: 'Black: Core concepts', estMinutes: 90 },
            { label: 'Black: Implementation', estMinutes: 75 },
            { label: 'Black: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Black help?'
        },
        {
          globalDay: 305,
          week: 44,
          title: 'CI/CD',
          priority: 'HIGH',
          tasks: [
            { label: 'CI/CD: Core concepts', estMinutes: 90 },
            { label: 'CI/CD: Implementation', estMinutes: 75 },
            { label: 'CI/CD: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does CI/CD help?'
        },
        {
          globalDay: 306,
          week: 44,
          title: 'YAML',
          priority: 'HIGH',
          tasks: [
            { label: 'YAML: Core concepts', estMinutes: 90 },
            { label: 'YAML: Implementation', estMinutes: 75 },
            { label: 'YAML: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does YAML help?'
        },
        {
          globalDay: 307,
          week: 44,
          title: 'Logging',
          priority: 'HIGH',
          tasks: [
            { label: 'Logging: Core concepts', estMinutes: 90 },
            { label: 'Logging: Implementation', estMinutes: 75 },
            { label: 'Logging: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Logging help?'
        },
        {
          globalDay: 308,
          week: 44,
          title: 'Monitoring',
          priority: 'HIGH',
          tasks: [
            { label: 'Monitoring: Core concepts', estMinutes: 90 },
            { label: 'Monitoring: Implementation', estMinutes: 75 },
            { label: 'Monitoring: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Monitoring help?'
        }
      ]
    },
    {
      id: 'capstone',
      title: 'Phase 13: Capstone Project Build & Iterate',
      description: 'Choose Math Study Assistant OR Hebrew-English Code Assistant and build.',
      duration: '28 days (Weeks 45-48)',
      weeks: [45, 46, 47, 48],
      days: [
        {
          globalDay: 309,
          week: 45,
          title: 'Capstone Project Selection & Planning',
          priority: 'HIGH',
          tasks: [
            { label: 'Review both capstone options (Math Assistant vs Code Assistant)', estMinutes: 45 },
            { label: 'Choose ONE capstone project', estMinutes: 30 },
            { label: 'Write capstone proposal in docs/capstone/proposal.md', estMinutes: 90 },
            { label: 'Define success metrics and evaluation plan', estMinutes: 60 },
            { label: 'Create project timeline (weeks 45-48)', estMinutes: 45 }
          ],
          reflectionPrompt: 'Why did you choose this capstone? What excites you most?'
        },
                {
          globalDay: 310,
          week: 45,
          title: 'Planning',
          priority: 'HIGH',
          tasks: [
            { label: 'Planning: Core concepts', estMinutes: 90 },
            { label: 'Planning: Implementation', estMinutes: 75 },
            { label: 'Planning: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Planning help?'
        },
        {
          globalDay: 311,
          week: 45,
          title: 'Data collection',
          priority: 'HIGH',
          tasks: [
            { label: 'Data collection: Core concepts', estMinutes: 90 },
            { label: 'Data collection: Implementation', estMinutes: 75 },
            { label: 'Data collection: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data collection help?'
        },
        {
          globalDay: 312,
          week: 45,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        },
        {
          globalDay: 313,
          week: 45,
          title: 'Evaluation',
          priority: 'HIGH',
          tasks: [
            { label: 'Evaluation: Core concepts', estMinutes: 90 },
            { label: 'Evaluation: Implementation', estMinutes: 75 },
            { label: 'Evaluation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Evaluation help?'
        },
        {
          globalDay: 314,
          week: 45,
          title: 'Iteration',
          priority: 'HIGH',
          tasks: [
            { label: 'Iteration: Core concepts', estMinutes: 90 },
            { label: 'Iteration: Implementation', estMinutes: 75 },
            { label: 'Iteration: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Iteration help?'
        },
        {
          globalDay: 315,
          week: 45,
          title: 'Testing',
          priority: 'HIGH',
          tasks: [
            { label: 'Testing: Core concepts', estMinutes: 90 },
            { label: 'Testing: Implementation', estMinutes: 75 },
            { label: 'Testing: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Testing help?'
        },
        {
          globalDay: 316,
          week: 45,
          title: 'Week 45 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 317,
          week: 46,
          title: 'Planning',
          priority: 'HIGH',
          tasks: [
            { label: 'Planning: Core concepts', estMinutes: 90 },
            { label: 'Planning: Implementation', estMinutes: 75 },
            { label: 'Planning: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Planning help?'
        },
        {
          globalDay: 318,
          week: 46,
          title: 'Data collection',
          priority: 'HIGH',
          tasks: [
            { label: 'Data collection: Core concepts', estMinutes: 90 },
            { label: 'Data collection: Implementation', estMinutes: 75 },
            { label: 'Data collection: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data collection help?'
        },
        {
          globalDay: 319,
          week: 46,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        },
        {
          globalDay: 320,
          week: 46,
          title: 'Evaluation',
          priority: 'HIGH',
          tasks: [
            { label: 'Evaluation: Core concepts', estMinutes: 90 },
            { label: 'Evaluation: Implementation', estMinutes: 75 },
            { label: 'Evaluation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Evaluation help?'
        },
        {
          globalDay: 321,
          week: 46,
          title: 'Iteration',
          priority: 'HIGH',
          tasks: [
            { label: 'Iteration: Core concepts', estMinutes: 90 },
            { label: 'Iteration: Implementation', estMinutes: 75 },
            { label: 'Iteration: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Iteration help?'
        },
        {
          globalDay: 322,
          week: 46,
          title: 'Testing',
          priority: 'HIGH',
          tasks: [
            { label: 'Testing: Core concepts', estMinutes: 90 },
            { label: 'Testing: Implementation', estMinutes: 75 },
            { label: 'Testing: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Testing help?'
        },
        {
          globalDay: 323,
          week: 46,
          title: 'Week 46 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 324,
          week: 47,
          title: 'Planning',
          priority: 'HIGH',
          tasks: [
            { label: 'Planning: Core concepts', estMinutes: 90 },
            { label: 'Planning: Implementation', estMinutes: 75 },
            { label: 'Planning: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Planning help?'
        },
        {
          globalDay: 325,
          week: 47,
          title: 'Data collection',
          priority: 'HIGH',
          tasks: [
            { label: 'Data collection: Core concepts', estMinutes: 90 },
            { label: 'Data collection: Implementation', estMinutes: 75 },
            { label: 'Data collection: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data collection help?'
        },
        {
          globalDay: 326,
          week: 47,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        },
        {
          globalDay: 327,
          week: 47,
          title: 'Evaluation',
          priority: 'HIGH',
          tasks: [
            { label: 'Evaluation: Core concepts', estMinutes: 90 },
            { label: 'Evaluation: Implementation', estMinutes: 75 },
            { label: 'Evaluation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Evaluation help?'
        },
        {
          globalDay: 328,
          week: 47,
          title: 'Iteration',
          priority: 'HIGH',
          tasks: [
            { label: 'Iteration: Core concepts', estMinutes: 90 },
            { label: 'Iteration: Implementation', estMinutes: 75 },
            { label: 'Iteration: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Iteration help?'
        },
        {
          globalDay: 329,
          week: 47,
          title: 'Testing',
          priority: 'HIGH',
          tasks: [
            { label: 'Testing: Core concepts', estMinutes: 90 },
            { label: 'Testing: Implementation', estMinutes: 75 },
            { label: 'Testing: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Testing help?'
        },
        {
          globalDay: 330,
          week: 47,
          title: 'Week 47 Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review week materials', estMinutes: 75 },
            { label: 'Practice exercises', estMinutes: 60 },
            { label: 'Write weekly log', estMinutes: 30 }
          ],
          reflectionPrompt: 'What progress this week?'
        },
        {
          globalDay: 331,
          week: 48,
          title: 'Planning',
          priority: 'HIGH',
          tasks: [
            { label: 'Planning: Core concepts', estMinutes: 90 },
            { label: 'Planning: Implementation', estMinutes: 75 },
            { label: 'Planning: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Planning help?'
        },
        {
          globalDay: 332,
          week: 48,
          title: 'Data collection',
          priority: 'HIGH',
          tasks: [
            { label: 'Data collection: Core concepts', estMinutes: 90 },
            { label: 'Data collection: Implementation', estMinutes: 75 },
            { label: 'Data collection: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Data collection help?'
        },
        {
          globalDay: 333,
          week: 48,
          title: 'Model training',
          priority: 'HIGH',
          tasks: [
            { label: 'Model training: Core concepts', estMinutes: 90 },
            { label: 'Model training: Implementation', estMinutes: 75 },
            { label: 'Model training: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Model training help?'
        },
        {
          globalDay: 334,
          week: 48,
          title: 'Evaluation',
          priority: 'HIGH',
          tasks: [
            { label: 'Evaluation: Core concepts', estMinutes: 90 },
            { label: 'Evaluation: Implementation', estMinutes: 75 },
            { label: 'Evaluation: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Evaluation help?'
        },
        {
          globalDay: 335,
          week: 48,
          title: 'Iteration',
          priority: 'HIGH',
          tasks: [
            { label: 'Iteration: Core concepts', estMinutes: 90 },
            { label: 'Iteration: Implementation', estMinutes: 75 },
            { label: 'Iteration: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Iteration help?'
        },
        {
          globalDay: 336,
          week: 48,
          title: 'Testing',
          priority: 'HIGH',
          tasks: [
            { label: 'Testing: Core concepts', estMinutes: 90 },
            { label: 'Testing: Implementation', estMinutes: 75 },
            { label: 'Testing: Practice', estMinutes: 60 }
          ],
          reflectionPrompt: 'How does Testing help?'
        }
      ]
    },
    {
      id: 'portfolio',
      title: 'Phase 14: Portfolio & Final Polish',
      description: 'Bilingual blogs, architecture diagrams, documentation, and public demos.',
      duration: '28 days (Weeks 49-52)',
      weeks: [49, 50, 51, 52],
      days: [
        {
          globalDay: 337,
          week: 49,
          title: 'Portfolio Website Design',
          priority: 'HIGH',
          tasks: [
            { label: 'Design portfolio website structure', estMinutes: 90 },
            { label: 'Choose static site generator (Hugo/Jekyll/plain HTML)', estMinutes: 45 },
            { label: 'Create wireframes for main pages', estMinutes: 60 },
            { label: 'Set up GitHub Pages or custom domain', estMinutes: 60 },
            { label: 'Write docs/portfolio/design.md', estMinutes: 30 }
          ],
          reflectionPrompt: 'What story do you want your portfolio to tell?'
        },
        {
          globalDay: 338,
          week: 49,
          title: 'First Blog Post: Scaling Lessons (English)',
          priority: 'HIGH',
          tasks: [
            { label: 'Outline blog post structure', estMinutes: 30 },
            { label: 'Write first draft (800+ words)', estMinutes: 120 },
            { label: 'Create 3 visualizations/diagrams', estMinutes: 90 },
            { label: 'Edit and polish', estMinutes: 60 },
            { label: 'Publish and share on Twitter/HF Discord', estMinutes: 20 }
          ],
          reflectionPrompt: 'What surprised you most about scaling laws?'
        },
        {
          globalDay: 339,
          week: 49,
          title: 'First Blog Post: Scaling Lessons (Hebrew)',
          priority: 'HIGH',
          tasks: [
            { label: 'Translate English blog post to Hebrew', estMinutes: 90 },
            { label: 'Adapt cultural references for Israeli audience', estMinutes: 45 },
            { label: 'Review with native speaker if possible', estMinutes: 30 },
            { label: 'Publish Hebrew version', estMinutes: 20 },
            { label: 'Share in Israeli tech communities', estMinutes: 20 }
          ],
          reflectionPrompt: 'How does writing in Hebrew change your technical communication?'
        },
        {
          globalDay: 340,
          week: 49,
          title: 'Architecture Diagram: End-to-End System',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Sketch system architecture on paper', estMinutes: 45 },
            { label: 'Create diagram with draw.io or similar', estMinutes: 90 },
            { label: 'Add component descriptions and data flows', estMinutes: 60 },
            { label: 'Export high-quality PNG/SVG', estMinutes: 15 },
            { label: 'Add to portfolio and relevant repos', estMinutes: 30 }
          ],
          reflectionPrompt: 'Does your architecture diagram tell a clear story?'
        },
        {
          globalDay: 341,
          week: 49,
          title: 'Repository Documentation Audit',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review all repo READMEs', estMinutes: 60 },
            { label: 'Add missing setup instructions', estMinutes: 45 },
            { label: 'Create CONTRIBUTING.md for main repos', estMinutes: 45 },
            { label: 'Add badges (build status, coverage, etc.)', estMinutes: 30 },
            { label: 'Verify all links work', estMinutes: 30 }
          ],
          reflectionPrompt: 'Can a stranger clone and run your projects?'
        },
        {
          globalDay: 342,
          week: 49,
          title: 'Second Blog Post: BPE Tokenizer Pitfalls (English)',
          priority: 'HIGH',
          tasks: [
            { label: 'Outline blog post on tokenizer challenges', estMinutes: 30 },
            { label: 'Write first draft (800+ words)', estMinutes: 120 },
            { label: 'Include code examples and gotchas', estMinutes: 75 },
            { label: 'Edit and polish', estMinutes: 60 },
            { label: 'Publish and share', estMinutes: 20 }
          ],
          reflectionPrompt: 'What tokenizer mistake cost you the most time?'
        },
        {
          globalDay: 343,
          week: 49,
          title: 'Second Blog Post: BPE Tokenizer Pitfalls (Hebrew)',
          priority: 'HIGH',
          tasks: [
            { label: 'Translate BPE blog post to Hebrew', estMinutes: 90 },
            { label: 'Adapt technical terminology', estMinutes: 45 },
            { label: 'Review and polish Hebrew version', estMinutes: 30 },
            { label: 'Publish and share in Israeli communities', estMinutes: 20 }
          ],
          reflectionPrompt: 'What Hebrew technical terms felt awkward? Why?'
        },
        {
          globalDay: 344,
          week: 50,
          title: 'Demo Video Recording',
          priority: 'HIGH',
          tasks: [
            { label: 'Write demo script (3-5 minutes)', estMinutes: 45 },
            { label: 'Set up recording environment (OBS/Zoom)', estMinutes: 30 },
            { label: 'Record demo video (multiple takes)', estMinutes: 90 },
            { label: 'Edit video (cuts, captions)', estMinutes: 75 },
            { label: 'Upload to YouTube with description', estMinutes: 30 }
          ],
          reflectionPrompt: 'Does your demo showcase your best work?'
        },
        {
          globalDay: 345,
          week: 50,
          title: 'Third Blog Post: MVP Shipping Process (English)',
          priority: 'HIGH',
          tasks: [
            { label: 'Outline blog post on deployment journey', estMinutes: 30 },
            { label: 'Write first draft (800+ words)', estMinutes: 120 },
            { label: 'Include deployment architecture diagram', estMinutes: 60 },
            { label: 'Edit and polish', estMinutes: 60 },
            { label: 'Publish and share', estMinutes: 20 }
          ],
          reflectionPrompt: 'What surprised you most about production deployment?'
        },
        {
          globalDay: 346,
          week: 50,
          title: 'Third Blog Post: MVP Shipping Process (Hebrew)',
          priority: 'HIGH',
          tasks: [
            { label: 'Translate MVP blog post to Hebrew', estMinutes: 90 },
            { label: 'Adapt for Israeli tech context', estMinutes: 45 },
            { label: 'Review and polish', estMinutes: 30 },
            { label: 'Publish and share', estMinutes: 20 }
          ],
          reflectionPrompt: 'How do Israeli deployment practices differ from global standards?'
        },
        {
          globalDay: 347,
          week: 50,
          title: 'LinkedIn Profile Update',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Update LinkedIn headline and summary', estMinutes: 45 },
            { label: 'Add all projects with descriptions', estMinutes: 60 },
            { label: 'Upload project images/demos', estMinutes: 30 },
            { label: 'Add skills and endorsements', estMinutes: 30 },
            { label: 'Connect with mentors and peers', estMinutes: 30 }
          ],
          reflectionPrompt: 'Does your LinkedIn profile reflect your growth?'
        },
        {
          globalDay: 348,
          week: 50,
          title: 'GitHub Profile README',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Create github.com/DovJNash/DovJNash repo', estMinutes: 15 },
            { label: 'Write profile README with bio and projects', estMinutes: 90 },
            { label: 'Add GitHub stats widgets', estMinutes: 30 },
            { label: 'Pin best repositories', estMinutes: 15 },
            { label: 'Add contact information and social links', estMinutes: 15 }
          ],
          reflectionPrompt: 'What first impression does your GitHub profile make?'
        },
        {
          globalDay: 349,
          week: 50,
          title: 'Code Quality Final Pass',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Run black on all repositories', estMinutes: 30 },
            { label: 'Fix any linting errors', estMinutes: 60 },
            { label: 'Update all requirements.txt files', estMinutes: 30 },
            { label: 'Verify all tests pass', estMinutes: 45 },
            { label: 'Tag stable releases (v1.0)', estMinutes: 30 }
          ],
          reflectionPrompt: 'Is your code ready for others to read and use?'
        },
        {
          globalDay: 350,
          week: 50,
          title: 'Documentation Final Review',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review all markdown files for typos', estMinutes: 60 },
            { label: 'Ensure consistent formatting', estMinutes: 45 },
            { label: 'Add table of contents where needed', estMinutes: 30 },
            { label: 'Verify all code examples work', estMinutes: 60 },
            { label: 'Add "last updated" dates', estMinutes: 15 }
          ],
          reflectionPrompt: 'Would a beginner understand your documentation?'
        },
        {
          globalDay: 351,
          week: 51,
          title: 'Resume/CV Update for IDF & University',
          priority: 'HIGH',
          tasks: [
            { label: 'Update technical skills section', estMinutes: 45 },
            { label: 'Add all projects with quantified impact', estMinutes: 75 },
            { label: 'Tailor one version for IDF (security focus)', estMinutes: 60 },
            { label: 'Tailor one version for university (research focus)', estMinutes: 60 },
            { label: 'Get feedback from mentor', estMinutes: 30 }
          ],
          reflectionPrompt: 'How do you quantify your learning achievements?'
        },
        {
          globalDay: 352,
          week: 51,
          title: 'Portfolio Presentation Prep',
          priority: 'HIGH',
          tasks: [
            { label: 'Create slide deck (15-20 slides)', estMinutes: 120 },
            { label: 'Include problem, solution, results for each project', estMinutes: 90 },
            { label: 'Practice 10-minute presentation', estMinutes: 60 },
            { label: 'Record practice run for self-review', estMinutes: 30 },
            { label: 'Refine based on feedback', estMinutes: 45 }
          ],
          reflectionPrompt: 'Can you explain your work to a non-technical audience?'
        },
        {
          globalDay: 353,
          week: 51,
          title: 'Community Presentation: PyData TLV Meetup Prep',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Research PyData TLV previous talks', estMinutes: 45 },
            { label: 'Propose talk topic (email organizers)', estMinutes: 30 },
            { label: 'Prepare lightning talk (5 min) on scaling experiments', estMinutes: 90 },
            { label: 'Create simple slides with key results', estMinutes: 60 },
            { label: 'Practice delivery', estMinutes: 45 }
          ],
          reflectionPrompt: 'What aspect of your work would interest the PyData community?'
        },
        {
          globalDay: 354,
          week: 51,
          title: 'Master Summary Document',
          priority: 'HIGH',
          tasks: [
            { label: 'Create docs/master_summary.txt', estMinutes: 30 },
            { label: 'Summarize each phase (2-3 paragraphs each)', estMinutes: 150 },
            { label: 'List key achievements and metrics', estMinutes: 60 },
            { label: 'Document lessons learned', estMinutes: 75 },
            { label: 'Add future directions section', estMinutes: 30 }
          ],
          reflectionPrompt: 'What would you do differently if starting over?'
        },
        {
          globalDay: 355,
          week: 51,
          title: 'Future Learning Plan (Months 13-24)',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Reflect on current skill gaps', estMinutes: 45 },
            { label: 'Research advanced topics (RL, multimodal, agents)', estMinutes: 75 },
            { label: 'Draft next 6-month learning plan', estMinutes: 90 },
            { label: 'Identify potential mentors for advanced topics', estMinutes: 30 },
            { label: 'Write docs/future_plan.md', estMinutes: 45 }
          ],
          reflectionPrompt: 'What excites you most about continuing this journey?'
        },
        {
          globalDay: 356,
          week: 51,
          title: 'Thank You Notes & Relationship Building',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Write thank you emails to mentors', estMinutes: 45 },
            { label: 'Share final portfolio with community supporters', estMinutes: 30 },
            { label: 'Schedule follow-up calls with key connections', estMinutes: 30 },
            { label: 'Join advanced Discord channels or Slack groups', estMinutes: 30 },
            { label: 'Offer to help other learners', estMinutes: 45 }
          ],
          reflectionPrompt: 'Who had the biggest impact on your journey? Have you thanked them?'
        },
        {
          globalDay: 357,
          week: 52,
          title: 'Final Portfolio Polish Day 1',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Review portfolio website on mobile devices', estMinutes: 45 },
            { label: 'Optimize images for web (compress, lazy load)', estMinutes: 60 },
            { label: 'Test all links and forms', estMinutes: 30 },
            { label: 'Add analytics (Google Analytics or Plausible)', estMinutes: 30 },
            { label: 'Set up custom domain if not already done', estMinutes: 45 }
          ],
          reflectionPrompt: 'Does your portfolio load fast and look professional?'
        },
        {
          globalDay: 358,
          week: 52,
          title: 'Final Portfolio Polish Day 2',
          priority: 'MEDIUM',
          tasks: [
            { label: 'Add testimonials or endorsements if available', estMinutes: 45 },
            { label: 'Create project showcase GIFs/videos', estMinutes: 90 },
            { label: 'Write compelling project descriptions', estMinutes: 75 },
            { label: 'Add "What I Learned" section to each project', estMinutes: 60 },
            { label: 'Final proofread of all content', estMinutes: 45 }
          ],
          reflectionPrompt: 'Would you hire yourself based on this portfolio?'
        },
        {
          globalDay: 359,
          week: 52,
          title: 'Launch Portfolio & Announce',
          priority: 'HIGH',
          tasks: [
            { label: 'Final portfolio review with mentor', estMinutes: 60 },
            { label: 'Write launch announcement post', estMinutes: 45 },
            { label: 'Share on Twitter, LinkedIn, HF Discord', estMinutes: 30 },
            { label: 'Submit to Hacker News / Reddit r/MachineLearning', estMinutes: 30 },
            { label: 'Email university programs with portfolio link', estMinutes: 30 }
          ],
          reflectionPrompt: 'What are you most proud to share with the world?'
        },
        {
          globalDay: 360,
          week: 52,
          title: 'IDF Application Preparation',
          priority: 'HIGH',
          tasks: [
            { label: 'Research 8200 and other tech units requirements', estMinutes: 60 },
            { label: 'Prepare portfolio presentation for IDF interview', estMinutes: 90 },
            { label: 'Emphasize security, systems thinking, and reliability', estMinutes: 45 },
            { label: 'Practice explaining projects in Hebrew', estMinutes: 60 },
            { label: 'Prepare questions about unit missions', estMinutes: 30 }
          ],
          reflectionPrompt: 'How does your work demonstrate IDF-relevant skills?'
        },
        {
          globalDay: 361,
          week: 52,
          title: 'University Application Preparation',
          priority: 'HIGH',
          tasks: [
            { label: 'Research HUJI Mahar and TAU Alpha programs', estMinutes: 75 },
            { label: 'Prepare university application essays', estMinutes: 120 },
            { label: 'Highlight research potential and curiosity', estMinutes: 45 },
            { label: 'Get recommendation letters from mentors', estMinutes: 60 },
            { label: 'Submit applications with portfolio link', estMinutes: 30 }
          ],
          reflectionPrompt: 'What research questions do you want to explore in university?'
        },
        {
          globalDay: 362,
          week: 52,
          title: 'Final Reflection & Goal Setting',
          priority: 'HIGH',
          tasks: [
            { label: 'Write comprehensive reflection on entire 12-month journey', estMinutes: 120 },
            { label: 'List top 10 achievements', estMinutes: 30 },
            { label: 'List top 10 lessons learned', estMinutes: 30 },
            { label: 'Set concrete goals for next 6 months', estMinutes: 60 },
            { label: 'Create accountability plan for continued growth', estMinutes: 45 }
          ],
          reflectionPrompt: 'How have you changed as a learner and builder in 12 months?'
        },
        {
          globalDay: 363,
          week: 52,
          title: 'Celebration & Gratitude',
          priority: 'LOW',
          tasks: [
            { label: 'Review all weekly logs from Week 1 to Week 52', estMinutes: 90 },
            { label: 'Create "Year in Review" infographic or video', estMinutes: 120 },
            { label: 'Share celebration post with community', estMinutes: 30 },
            { label: 'Treat yourself to something special', estMinutes: 60 },
            { label: 'Plan celebration with family/friends', estMinutes: 30 }
          ],
          reflectionPrompt: 'What are you most grateful for on this journey?'
        },
        {
          globalDay: 364,
          week: 52,
          title: 'Rest & Recharge',
          priority: 'LOW',
          tasks: [
            { label: 'Take the day off—no coding or studying', estMinutes: 0 },
            { label: 'Reflect on personal growth (journal)', estMinutes: 30 },
            { label: 'Plan next week with renewed energy', estMinutes: 30 },
            { label: 'Reach out to one person who inspired you', estMinutes: 15 },
            { label: 'Dream about what comes next', estMinutes: 0 }
          ],
          reflectionPrompt: 'You did it. What\'s next?'
        }
      ]
    }
  ]
};

// Export for use in main.js
if (typeof module !== 'undefined' && module.exports) {
  module.exports = { PLAN };
}
